[
    
    {
        "title"    : "Some personal updates",
        "category" : "Random",
        "tags"     : "",
        "content": "Just want to give a brief updates on my current status. I moved from Bay Area working for Arista Networks to Irvine@Orange County@SoCalworking for Amazon on Oct 2019. Part of the reasons why I haven’t been updating this blog is just the work is far busier compared towhat’s in Arista XD.I’ll have another post about this one and a half year journey in Amazon@Irvine, across two organizations two teams three managers.",
        "url"      : "/2021/03/09/Some-Personal-Updates/",
        "date"     : "2021-03-09 10:00:00 +0000"
    } ,
    
    {
        "title"    : "Data Encoding and Transmission- Part 2 of Designing Data-Intensive Applications",
        "category" : "Distributed System",
        "tags"     : "Books, Distributed System",
        "content": "This blog talks about various data encoding methods and their advantages/limitations,along with protocols of transmitting them.Efficiency is certainly one of the main concerns for various encoding methods. The otherthing we need to care about is compatibility. Backward compatibility means thatnewer code can read data that was written by older code. and Forward compatibilitymeans that older code can read data that was written by newer code. Backwardcompatibility is normally not hard to achieve: as author of the newer code, you knowthe format of data written by older code, and so you can explicitly handle it(if necessary by simply keeping the old code to read the old data). Forwardcompatibility can be trickier, because it requires older code to ignore additionsmade by a newer version of the code.Programming language–specific encodingsExamples java.io.Serializable for Java, pickle for Python. Problems:  The encoding is often tied to a particular programming language, and reading thedata in another language is very difficult. If you store or transmit data in such anencoding, you are committing yourself to your current programming language forpotentially a very long time, and precluding integrating your systems with those ofother organizations (which may use different languages).  In order to restore data in the same object types, the decoding process needs to beable to instantiate arbitrary classes. This is frequently a source of securityproblems: if an attacker can get your application to decode an arbitrary byte sequence,they can instantiate arbitrary classes, which in turn often allows them to do terriblethings such as remotely executing arbitrary code.  Versioning data is often an afterthought in these libraries: as they are intendedfor quick and easy encoding of data, they often neglect the inconvenient problems offorward and backward compatibility.  Efficiency (CPU time taken to encode or decode, and the size of the encoded structure)is also often an afterthought. For example, Java’s built-in serialization isnotorious for its bad performance and bloated encoding.For these reasons it’s generally a bad idea to use your language’s built-in encodingfor anything other than very transient purposeJSON, XML, and Binary VariantsProblems:  There is a lot of ambiguity around the encoding of numbers. In XML and CSV, you cannotdistinguish between a number and a string that happens to consist of digits (exceptby referring to an external schema). JSON distinguishes strings and numbers, but itdoesn’t distinguish integers and floating-point numbers, and it doesn’t specify a precision.This is a problem when dealing with large numbers; for example, integers greater than253 cannot be exactly represented in an IEEE 754 double-precision floating- pointnumber, so such numbers become inaccurate when parsed in a language that usesfloating-point numbers (such as JavaScript). An example of numbers larger than 253occurs on Twitter, which uses a 64-bit number to identify each tweet. The JSONreturned by Twitter’s API includes tweet IDs twice, once as a JSON number and onceas a decimal string, to work around the fact that the numbers are not correctlyparsed by JavaScript applications.  JSON and XML have good support for Unicode character strings (i.e., human-readabletext), but they don’t support binary strings (sequences of bytes without a characterencoding). Binary strings are a useful feature, so people get around this limitationby encoding the binary data as text using Base64. The schema is then used to indicatethat the value should be interpreted as Base64-encoded. This works, but it’s somewhathacky and increases the data size by 33%.  CSV does not have any schema, so it is up to the application to define the meaningof each row and column. If an application change adds a new row or column, you haveto handle that change manually. CSV is also a quite vague format (what happens if avalue contains a comma or a newline character?). Although its escaping rules havebeen formally specified, not all parsers implement them correctly.Despite these flaws, JSON, XML, and CSV are good enough for many purposes. It’s likelythat they will remain popular, especially as data interchange formats (i.e., forsending data from one organization to another). In these situations, as long as peopleagree on what the format is, it often doesn’t matter how pretty or efficient theformat is. The difficulty of getting different organizations to agree on anythingoutweighs most other concerns.Binary EncodingMessagePackMessagePack is a binary encoding for JSON. Here is an example:  The first byte, 0x83, indicates that what follows is an object (top four bits =0x80) with three fields (bottom four bits = 0x03). (In case you’re wondering whathappens if an object has more than 15 fields, so that the number of fields doesn’tfit in four bits, it then gets a different type indicator, and the number of fieldsis encoded in two or four bytes.)  The second byte, 0xa8, indicates that what follows is a string (top four bits =0xa0) that is eight bytes long (bottom four bits = 0x08).  The next eight bytes are the field name “userName” in ASCII. Since the length wasindicated previously, there’s no need for any marker to tell us where the stringends (or any escaping).  The next seven bytes encode the six-letter string value Martin with a prefix 0xa6,and so on.The binary encoding is 66 bytes long, which is only a little less than the 81 bytestaken by the textual JSON encoding (with whitespace removed). All the binary encodingsof JSON are similar in this regard. It’s not clear whether such a small space reduction(and perhaps a speedup in parsing) is worth the loss of human-readability.Thrift and Protocol BuffersApache Thrift and Protocol Buffers (protobuf) are binary encoding libraries that arebased on the same principle. Protocol Buffers was originally developed at Google,Thrift was originally developed at Facebook, and both were made open source in 2007–08.Both Thrift and Protocol Buffers require a schema for any data that is encoded.To encode the data above in Thrift, you would describe the schema in the Thriftinterface definition language (IDL) like this:The equivalent schema definition for Protocol Buffers looks very similar:Thrift and Protocol Buffers each come with a code generation tool that takes a schemadefinition like the ones shown here, and produces classes that implement the schemain various programming languages. Your application code can call this generated codeto encode or decode records of the schema.Confusingly, Thrift has two different binary encoding formats, called BinaryProtocoland CompactProtocol, respectively. Let’s look at BinaryProtocol first.Similarly to MessagePack, each field has a type annotation (to indicate whether it isa string, integer, list, etc.) and, where required, a length indication (length of astring, number of items in a list). The strings that appear in the data (“Martin”,“daydreaming”, “hacking”) are also encoded as ASCII (or rather, UTF-8), similar to before.The big difference compared to MessagePack is that there are no field names(userName, favoriteNumber, interests). Instead, the encoded data contains field tags,which are numbers (1, 2, and 3). Those are the numbers that appear in the schemadefinition. Field tags are like aliases for fields—they are a compact way of sayingwhat field we’re talking about, without having to spell out the field name.The Thrift CompactProtocol encoding is semantically equivalent to BinaryProtocol, butas you can see in the following, it packs the same information into only 34 bytes.It does this by packing the field type and tag number into a single byte, and by usingvariable-length integers. Rather than using a full eight bytes for the number 1337,it is encoded in two bytes, with the top bit of each byte used to indicate whetherthere are still more bytes to come. This means numbers between –64 and 63 are encodedin one byte, numbers between –8192 and 8191 are encoded in two bytes, etc.Bigger numbers use more bytes.Finally, Protocol Buffers (which has only one binary encoding format) encodes the samedata as shown in Figure 4-4. It does the bit packing slightly differently, but isotherwise very similar to Thrift’s CompactProtocol. Protocol Buffers fits the samerecord in 33 bytes.CompatibilityFor forward compatibility, You can change the name of a field in the schema, since the encodeddata never refers to field names, but you cannot change a field’s tag, since thatwould make all existing encoded data invalid. You can add new fields to the schema,provided that you give each field a new tag number. If old code (which doesn’t knowabout the new tag numbers you added) tries to read data written by new code,including a new field with a tag number it doesn’t recognize, it can simply ignorethat field. The datatype annotation allows the parser to determine how many bytes itneeds to skip. This maintains forward compatibility: old code can read records thatwere written by new code.For backward compatibility, As long as each field has aunique tag number, new code can always read old data, because the tag numbers stillhave the same meaning. The only detail is that if you add a new field, you cannotmake it required. If you were to add a field and make it required, that check wouldfail if new code read data written by old code, because the old code will not havewritten the new field that you added. Therefore, to maintain backward compatibility,every field you add after the initial deployment of the schema must be optional orhave a default value.Removing a field is just like adding a field, with backward and forward compatibilityconcerns reversed. That means you can only remove a field that is optional(a required field can never be removed), and you can never use the same tag numberagain (because you may still have data written somewhere that includes the old tagnumber, and that field must be ignored by new code).Changing datatype of a field is possible, but there’s a risk that values will loseprecision or get truncated (say if you write a 64-bit variable and decode it in 32 bits).AvroApache Avro is another binary encoding format that is interestingly different fromProtocol Buffers and Thrift. It was started in 2009 as a subproject of Hadoop,as a result of Thrift not being a good fit for Hadoop’s use cases.Avro also uses a schema to specify the structure of the data being encoded. It hastwo schema languages: one (Avro IDL) intended for human editing, and one (based onJSON) that is more easily machine-readable.Our example schema, written in Avro IDL, might look like this:The equivalent JSON representation of that schema is as follows:Here is the breakdown of the encoded byte sequence:First of all, notice that there are no tag numbers in the schema.If you examine the byte sequence, you can see that there is nothing to identifyfields or their datatypes. The encoding simply consists of values concatenated together.A string is just a length prefix followed by UTF-8 bytes, but there’s nothing in theencoded data that tells you that it is a string. It could just as well be an integer,or something else entirely. An integer is encoded using a variable-length encoding(the same as Thrift’s CompactProtocol).To parse the binary data, you go through the fields in the order that they appear inthe schema and use the schema to tell you the datatype of each field. This means thatthe binary data can only be decoded correctly if the code reading the data is usingthe exact same schema as the code that wrote the data. Any mismatch in the schemabetween the reader and the writer would mean incorrectly decoded data.CompatibilityWith Avro, when an application wants to encode some data (to write it to a file ordatabase, to send it over the network, etc.), it encodes the data using whateverversion of the schema it knows about—for example, that schema may be compiled intothe application. This is known as the writer’s schema.When an application wants to decode some data (read it from a file or database,receive it from the network, etc.), it is expecting the data to be in some schema,which is known as the reader’s schema. That is the schema the application code isrelying on -code may have been generated from that schema during the application’sbuild process.The key idea with Avro is that the writer’s schema and the reader’s schema don’t haveto be the same—they only need to be compatible. When data is decoded (read), theAvro library resolves the differences by looking at the writer’s schema and thereader’s schema side by side and translating the data from the writer’s schemainto the reader’s schema. The Avro specification defines exactly how this resolutionworks, and it is illustrated below.For example, it’s no problem if the writer’s schema and the reader’s schema havetheir fields in a different order, because the schema resolution matches up the fieldsby field name. If the code reading the data encounters a field that appears inthe writer’s schema but not in the reader’s schema, it is ignored. If the codereading the data expects some field, but the writer’s schema does not contain afield of that name, it is filled in with a default value declared in the reader’s schema.With Avro, forward compatibility means that you can have a new version of the schemaas writer and an old version of the schema as reader. Conversely, backwardcompatibility means that you can have a new version of the schema as readerand an old version as writer.To maintain compatibility, you may only add or remove a field that has a default value.(The field favoriteNumber in our Avro schema has a default value of null.) For example,say you add a field with a default value, so this new field exists in the new schemabut not the old one. When a reader using the new schema reads a record written withthe old schema, the default value is filled in for the missing field.If you were to add a field that has no default value, new readers wouldn’t be able toread data written by old writers, so you would break backward compatibility. If youwere to remove a field that has no default value, old readers wouldn’t be able toread data written by new writers, so you would break forward compatibility.Writer’s SchemaThere is an important question that we’ve glossed over so far: how does the readerknow the writer’s schema with which a particular piece of data was encoded? We can’tjust include the entire schema with every record, because the schema would likely bemuch bigger than the encoded data, making all the space savings from the binary encoding futile.The answer depends on the context in which Avro is being used. To give a few examples:      Large file with lots of records: A common use for Avro—especially in the context ofHadoop—is for storing a large file containing millions of records, all encoded withthe same schema. In this case, the writer of that file can just include the writer’sschema once at the beginning of the file. Avro specifies a file format(object container files) to do this.        Database with individually written records: In a database, different records may bewritten at different points in time using different writer’s schemas—you cannotassume that all the records will have the same schema. The simplest solution is toinclude a version number at the beginning of every encoded record, and to keep a listof schema versions in your database. A reader can fetch a record, extract the versionnumber, and then fetch the writer’s schema for that version number from the database.Using that writer’s schema, it can decode the rest of the record. (Espresso works this way, for example.)        Sending records over a network connection: When two processes are communicating overa bidirectional network connec‐ tion, they can negotiate the schema version onconnection setup and then use that schema for the lifetime of the connection.The Avro RPC protocol works like this.  Dynamically generated SchemasOne advantage of Avro’s approach, compared to Protocol Buffers and Thrift, is that theschema doesn’t contain any tag numbers. But why is this important? What’s the problemwith keeping a couple of numbers in the schema?The difference is that Avro is friendlier to dynamically generated schemas. Forexample, say you have a relational database whose contents you want to dump to a file,and you want to use a binary format to avoid the aforementioned problems with textualformats (JSON, CSV, SQL). If you use Avro, you can fairly easily generate an Avroschema (in the JSON representation we saw earlier) from the relational schema andencode the database contents using that schema, dumping it all to an Avro objectcontainer file. You generate a record schema for each database table, and each columnbecomes a field in that record. The column name in the database maps to the field name in Avro.Now, if the database schema changes (for example, a table has one column added and onecolumn removed), you can just generate a new Avro schema from the updated databaseschema and export data in the new Avro schema. The data export process does not needto pay any attention to the schema change—it can simply do the schema conversionevery time it runs. Anyone who reads the new data files will see that the fields ofthe record have changed, but since the fields are identified by name, the updatedwriter’s schema can still be matched up with the old reader’s schema.By contrast, if you were using Thrift or Protocol Buffers for this purpose, the fieldtags would likely have to be assigned by hand: every time the database schema changes,an administrator would have to manually update the mapping from data‐ base column namesto field tags. (It might be possible to automate this, but the schema generator wouldhave to be very careful to not assign previously used field tags.) This kind ofdynamically generated schema simply wasn’t a design goal of Thrift or Protocol Buffers,whereas it was for Avro.Code Generation and Dynamically Typed LanguagesThrift and Protocol Buffers rely on code generation: after a schema has been defined,you can generate code that implements this schema in a programming language of yourchoice. This is useful in statically typed languages such as Java, C++, or C#, becauseit allows efficient in-memory structures to be used for decoded data, and it allowstype checking and autocompletion in IDEs when writing programs that access the data structures.In dynamically typed programming languages such as JavaScript, Ruby, or Python, thereis not much point in generating code, since there is no compile-time type checker tosatisfy. Code generation is often frowned upon in these languages, since theyotherwise avoid an explicit compilation step. Moreover, in the case of a dynamicallygenerated schema (such as an Avro schema generated from a database table), codegeneration is an unnecessarily obstacle to getting to the data.Avro provides optional code generation for statically typed programming languages, butit can be used just as well without any code generation. If you have an objectcontainer file (which embeds the writer’s schema), you can simply open it using theAvro library and look at the data in the same way as you could look at a JSON file.The file is self-describing since it includes all the necessary metadata.This property is especially useful in conjunction with dynamically typed data processinglanguages like Apache Pig [26]. In Pig, you can just open some Avro files, startanalyzing them, and write derived datasets to output files in Avro format withouteven thinking about schemas.Models of DataflowDataflow Through DatabasesIn a database, the process that writes to the database encodes the data, and theprocess that reads from the database decodes it. There may just be a single processaccessing the database, in which case the reader is simply a later version of the sameprocess—in that case you can think of storing something in the database as sendinga message to your future self.A couple issues need to be taken care of:A database generally allows any value to be updated at any time. This means thatwithin a single database you may have some values that were written five millisecondsago, and some values that were written five years ago.When you deploy a new version of your application (of a server-side application,at least), you may entirely replace the old version with the new version within a fewminutes. The same is not true of database contents: the five-year-old data will stillbe there, in the original encoding, unless you have explicitly rewritten it since then.This observation is sometimes summed up as data outlives code.Rewriting (migrating) data into a new schema is certainly possible, but it’s anexpensive thing to do on a large dataset, so most databases avoid it if possible.Most relational databases allow simple schema changes, such as adding a new columnwith a null default value, without rewriting existing data.v When an old row is read,the database fills in nulls for any columns that are missing from the encoded data ondisk. LinkedIn’s document database Espresso uses Avro for storage, allowing it to useAvro’s schema evolution rules.Schema evolution thus allows the entire database to appear as if it was encoded witha single schema, even though the underlying storage may contain records encoded withvarious historical versions of the schema.Dataflow Through Services: REST and RPCWe all know about REST and RPC so we’ll just talk about the relationships betweenthem and data encoding, i.e. compatibility.For evolvability, it is important that RPC clients and servers can be changed anddeployed independently. Compared to data flowing through databases (as described inthe last section), we can make a simplifying assumption in the case of dataflowthrough services: it is reasonable to assume that all the servers will be updatedfirst, and all the clients second. Thus, you only need backward compatibility onrequests, and forward compatibility on responses. And bhe backward and forwardcompatibility properties of an RPC scheme are inherited from whatever encoding it uses.Message-Passing DataflowBasically a streaming or pub/sub system like RabbitMQ and Kafka.",
        "url"      : "/2019/08/15/Data-Encoding-and-Transmission/",
        "date"     : "2019-08-15 15:00:00 +0000"
    } ,
    
    {
        "title"    : "Data Models, Querys, and Storage - Part 1 of Designing Data-Intensive Applications",
        "category" : "Distributed System",
        "tags"     : "Books, Distributed System",
        "content": "  Layout of the Whole Book  Data Models and Query Languages  Storage and Retrieval          SSTables and LSM-Trees      B-Trees      Comparing B-Trees and LSM-Trees      OLTP and OLAP      Data Warehousing and Column-Oriented-Storage        SummaryAnthoer book summary/review!Layout of the Whole BookThe book is arranged into these parts:  Fundamental ideas that underpin the design of data-intensive applications.          What we’re actually trying to achieve: reliability,  scalability, and maintainability; how we need to think about them; and how we can  achieve them.      Comparisons of different data models and query languages      Storage engines how databases arrange data on disk so that we can find it again  efficiently.      Encoding and evolution of schemas over time.        Distributed Data          Replication, partitioning/sharding      Transactions      Consistency and consensus      Batching      Streaming      This blog post only talks about part 1.Data Models and Query LanguagesThe book talks about various query languages other than SQL which can be referred fromthe book so we are not going to talk about them here. Essentially there are a fewgeneral-purpose data models for data storage and querying including relational model(SQL), the document model like JSON and XML (NoSQL), and graph-based model. The ideais that relational model is good at modeling relations (called tables in SQL),where each relation is an unordered collection of tuples (rows in SQL); the documentmodel is good at modeling documents or “objects” in programming; when it comes tomany-to-one and many-to-many relationships, graph-based model does the best job.Storage and RetrievalSSTables and LSM-TreesSSTable stands for Sorted String Table. The idea is that we keep an in-memory tablewhere every key is mapped to a byte offset in the data file. Whenever you append a newkey-value pair to the file, you also update the table to reflect the offset of thedata you just wrote (this works both for inserting new keys and for updating existingkeys). When you want to look up a value, use the table to find the offset in thedata file, seek to that location, and read the value. It’s basically an append-onlylog. The key idea is that the sequence of key-value pairs is sorted by key.How do you get your data to be sorted by key in the first place? Use a balanced BST!We can now make our storage engine work as follows:  When a write comes in, add it to an in-memory balanced tree data structure (forexample, a red-black tree). This in-memory tree is sometimes called a memtable.  When the memtable gets bigger than some threshold—typically a few megabytes—writeit out to disk as an SSTable file. This can be done efficiently because the treealready maintains the key-value pairs sorted by key. The new SSTable file becomes themost recent segment of the database. While the SSTable is being written out to disk,writes can continue to a new memtable instance.  In order to serve a read request, first try to find the key in the memtable, thenin the most recent on-disk segment, then in the next-older segment, etc.  From time to time, run a merging and compaction process in the background tocombine segment files and to discard overwritten or deleted values.How do we avoid eventually running out of disk space? A good solution is to break thelog into segments of a certain size by closing a segment file when it reaches acertain size, and making subsequent writes to a new segment file. We can thenperform compaction on these segments. Compaction meansthrowing away duplicate keys in the log, and keeping only the most recent updatefor each key. Moreover, since compaction often makes segments much smaller (assumingthat a key is overwritten several times on average within one segment), we can alsomerge several segments together at the same time as performing the compaction.Segments are never modified after they have been written, so the merged segment iswritten to a new file. The merging and compaction of frozen segments can be done ina background thread, and while it is going on, we can still continue to serve readand write requests as normal, using the old segment files. After the merging processis complete, we switch read requests to using the new merged segment instead of theold segments—and then the old segment files can simply be deleted.If you want to delete a key and its associated value, you have to append a specialdeletion record to the data file (sometimes called a tombstone). When log segmentsare merged, the tombstone tells the merging process to discard any previous valuesfor the deleted key.if the database crashes, the most recent writes (which are in the memtable but notyet written out to disk) are lost. In order to avoid that problem, we can keep aseparate log on disk to which every write is immediately appended.That log is not in sorted order, but that doesn’t matter, because its only purposeis to restore the memtable after a crash. Every time the memtable is written out to anSSTable, the corresponding log can be discarded.Append-only log with SSTable has several advantages:  Appending and segment merging are sequential write operations, which are generallymuch faster than random writes, especially on magnetic spinning-disk hard drives.To some extent sequential writes are also preferable on flash-based solid state drives (SSDs)  Concurrency and crash recovery are much simpler if segment files are append-only orimmutable. For example, you don’t have to worry about the case where a crash happenedwhile a value was being overwritten, leaving you with a file containing part of theold and part of the new value spliced together.  Merging segments is simple and efficient, even if the files are bigger than theavailable memory. The approach is like the one used in the mergesort algorithm.You start reading the input files side by side, look at the first key in each file,copy the lowest key (according to the sort order) to the output file, and repeat.This produces a new merged segment file, also sorted by key. What if the same keyappears in several input segments? Remember that each segment contains all the valueswritten to the database during some period of time. This means that all the values inone input segment must be more recent than all the values in the other segment(assuming that we always merge adjacent segments). When multiple segments contain thesame key, we can keep the value from the most recent segment and discard the valuesin older segments.  Merging old segments avoids the problem of data files getting fragmented over time.  Lookup is fast as it’s basically a binary searchB-TreesLike SSTables, B-trees keep key-value pairs sorted by key, which allows efficientkey-value lookups and range queries. But that’s where the similarity ends: B-treeshave a very different design philosophy. The log-structured indexes we saw earlierbreak the database down into variable-size segments, typically several megabytes ormore in size, and always write a segment sequentially. By contrast, B-trees break thedatabase down into fixed-size blocks or pages, traditionally 4 KB in size(sometimes bigger), and read or write one page at a time. This design correspondsmore closely to the underlying hardware, as disks are also arranged in fixed-sizeblocks. Most databases can fit into a B-tree that is three or four levels deep, soyou don’t need to follow many page references to find the page you are looking for.(A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.)The basic underlying write operation of a B-tree is to overwrite a page on disk withnew data. It is assumed that the overwrite does not change the location of the page;i.e., all references to that page remain intact when the page is overwritten.This is in stark contrast to log-structured indexes such as LSM-trees, which onlyappend to files (and eventually delete obsolete files) but never modify files in place.In order to make the database resilient to crashes, it is common for B-treeimplementations to include an additional data structure on disk: a write-ahead log(WAL, also known as a redo log). This is an append-only file to which every B-treemodification must be written before it can be applied to the pages of the tree itself.When the data‐ base comes back up after a crash, this log is used to restore theB-tree back to a consistent stateComparing B-Trees and LSM-TreesAs a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees arethought to be faster for reads. Reads are typically slower on LSM-trees because theyhave to check several different data structures and SSTables at different stages of compaction.OLTP and OLAPOLTP stands for online transaction processing and OLAP stands for online analytic processing.We can compare characteristics of the two as following            Property      OLTP      OLAP              Main read pattern      small number of records per query, fetched by key      Aggregate over large number of records              Main write pattern      Random-access, low-latency writes from user input      Bulk import (ETL) or event stream              Primarily used by      End user/customer, via web application      Internal analyst, for decision support              What data represents      Latest state of data (current point in time)      History of events that happened over time              Dataset size      Gigabytes to terabytes      Terabytes to petabytes      Data Warehousing and Column-Oriented-StorageThe example schema following shows a data warehouse that might be found at a groceryretailer. At the center of the schema is a so-called fact table (in this example,it is called fact_sales). Each row of the fact table represents an event that occurredat a particular time (here, each row represents a customer’s purchase of a product).If we were analyzing website traffic rather than retail sales, each row mightrepresent a page view or a click by a user.In a typical data warehouse, tables are often very wide: fact tables often have over100 columns, sometimes several hundred. Dimension tables can also be very wide, asthey include all the metadata that may be relevant for analysis—for example, thedim_store table may include details of which services are offered at each store,whether it has an in-store bakery, the square footage, the date when the store wasfirst opened, when it was last remodeled, how far it is from the nearest highway, etc.Although fact tables are often over 100 columns wide, a typical data warehouse queryonly accesses 4 or 5 of them at one time (“SELECT *” queries are rarely needed for analytics).But in most OLTP databases, storage is laid out in a row-oriented fashion: all the valuesfrom one row of a table are stored next to each other. Document databases aresimilar: an entire document is typically stored as one contiguous sequence of bytes.So we need another layout of our storage engine, which is column-oriented storage.The idea behind column-oriented storage is simple: don’t store all the values from onerow together, but store all the values from each column together instead. If eachcolumn is stored in a separate file, a query only needs to read and parse thosecolumns that are used in that query, which can save a lot of work.This principle is illustrated below.Besides only loading those columns from disk that are required for a query, we canfurther reduce the demands on disk throughput by compressing data. Fortunately,column-oriented storage often lends itself very well to compression: columns oftenlook quite repetitive (for example, a retailer may have billions of sales transactions,but only 100,000 distinct products), so we can use a bitmap to encode them.If n is very small (for example, a country column may have approximately 200 distinctvalues), those bitmaps can be stored with one bit per row. But if n is bigger, therewill be a lot of zeros in most of the bitmaps (we say that they are sparse). In thatcase, the bitmaps can additionally be run-length encoded, as shown below.Note: Cassandra and HBase have a concept of column families, which they inherited fromBigtable. However, it is very misleading to call them column-oriented: within eachcolumn family, they store all columns from a row together, along with a row key, andthey do not use column compression. Thus, the Bigtable model is still mostly row-oriented.SummaryOn a high level, we saw that storage engines fall into two broad categories: thoseoptimized for transaction processing (OLTP), and those optimized for analytics (OLAP).There are big differences between the access patterns in those use cases:  OLTP systems are typically user-facing, which means that they may see a huge volumeof requests. In order to handle the load, applications usually only touch a smallnumber of records in each query. The application requests records using some kind ofkey, and the storage engine uses an index to find the data for the requested key.Disk seek time is often the bottleneck here.  Data warehouses and similar analytic systems are less well known, because they areprimarily used by business analysts, not by end users. They handle a much lowervolume of queries than OLTP systems, but each query is typically very demanding,requiring many millions of records to be scanned in a short time. Disk bandwidth(not seek time) is often the bottleneck here, and column- oriented storage is anincreasingly popular solution for this kind of workload.On the OLTP side, we saw storage engines from two main schools of thought:  The log-structured school, which only permits appending to files and deletingobsolete files, but never updates a file that has been written. Bitcask, SSTables,LSM-trees, LevelDB, Cassandra, HBase, Lucene, and others belong to this group.  The update-in-place school, which treats the disk as a set of fixed-size pages thatcan be overwritten. B-trees are the biggest example of this philosophy, being usedin all major relational databases and also many nonrelational ones.We then took a detour from the internals of storage engines to look at the high-levelarchitecture of a typical data warehouse. This background illustrated why analyticworkloads are so different from OLTP: when your queries require sequentially scanningacross a large number of rows, indexes are much less relevant. Instead it becomesimportant to encode data very compactly, to minimize the amount of data that thequery needs to read from disk. We discussed how column-oriented storage helps achievethis goal.",
        "url"      : "/2019/08/15/Data-Models-Querys-and-Storage/",
        "date"     : "2019-08-15 10:00:00 +0000"
    } ,
    
    {
        "title"    : "Streaming Systems",
        "category" : "Distributed System",
        "tags"     : "Books, Distributed System",
        "content": "  Layout of the book  Theory of Stream and Table Relativity          stream → stream: Nongrouping (element-wise) operations      stream → table: Grouping operations      table → stream: Ungrouping (triggering) operations      table → table: (none)        ThoughtsI recently read through the book Streaming Systems so think it would be a goodidea to write up a summary/thoughts about it. The book is recommended by评:Streaming System(简直炸裂,强势安利) onzhihu.Layout of the bookThe book consists of 10 chapters. The book starts of by stating that traditionallybatching systems, streaming systems, databases are three distinct concepts.Batching systems are systems that process finite amount of data (bounded data)producing accurate (strong consistent) (exactly-once) results which typically havehigher throughput and higher latency. Streaming systems deal with unbounded dataand are typically not as accurate (consistent) as batching systems, which could beimplemented by repeated batching. Databases are just persistent data storage thatone can do CRUD operations and queries on. The first half of the book summarizes varioustechniques that modern streaming systems use to achieve certain goals like dealingwith out-of-order updates based on event time and end-to-end exactly-once processing.The argument the author wants to make for the first half of the book is that,given the recent improvements of various streaming systems, strong consistency can beachieve in streaming systems (examples are MillWheel, Spark Streaming, and Flink).As a result, streaming systems can have parity with batch.In the second half of the book, the author presents a view or way of thinking, wherestreaming systems and databases are just two sides of a coin, where they are justprocessors of two different forms of data, called “Theory of Stream and Table Relativity”,and show that how those different techniques used in streaming systems like trigger,watermark, and windowing play a role in the new unified world. The author also talksabout how to extend the current SQL syntax to support the new unified theory (whichunfortunately I personally am not interested in so I just skimmed through that part).I think the core of this book, or I personally find the most value out of this book,is the point of view that tables (databases) and streams are basically talking aboutthe same thing. This is a conclusion the author draws after many years of working indistributed systems and examining different techniques used by modern streamingsystems, this post will talk in the reverse order of the book, such that the theorywill first be presented and explained, and after that various techniques will bepresented to see how they fit into the big picture. I won’t be going into detailsof how those techniques work though.Theory of Stream and Table RelativitySuppose there isn’t any streams or tables, there is just data, and processors of data.A processor is just a function that takes a dataset, do some calculation to it, andoutput a dataset. For some operations, we need to take a look at every single datain the dataset (like sum, average, and count), while for other a partial view of thedataset or even a view of a single data is enough (like filter, and +1). We can thenthink of streams are just “flowing” data over time, while a table is just a view ofthe data at one specific point of time, i.e. static data. So data is either staticor dynamic, and we need transformations/operations of the two. The specific techniquesdescribed below is available in the book and there is also a nice example of howMapReduce fits into the theory. This post is just a summary mainly comes from Chapter 6of the book.stream → stream: Nongrouping (element-wise) operationsApplying nongrouping operations to a stream alters the data in the stream whileleaving them in motion, yielding a new stream with possibly different cardinality.stream → table: Grouping operationsGrouping data within a stream brings those data to rest, yielding a table that evolves over time.      Windowing incorporates the dimension of event time into such groupings.        Merging windows dynamically combine over time, allowing them to reshape themselves inresponse to the data observed and dictating that key remain the unit ofatomicity/parallelization, with window being a child component of grouping within that key.  table → stream: Ungrouping (triggering) operationsTriggering data within a table ungroups them into motion, yielding a stream thatcaptures a view of the table’s evolution over time.      Watermarks provide a notion of input completeness relative to event time, which isa useful reference point when triggering event-timestamped data, particularly datagrouped into event-time windows from unbounded streams.        The accumulation mode for the trigger determines the nature of the stream,dictating whether it contains deltas or values, and whether retractions for previousdeltas/values are provided.  table → table: (none)There are no operations that consume a table and yield a table, because it’s notpossible for data to go from rest and back to rest without being put into motion.As a result, all modifications to a table are via conversion to a stream and back again.ThoughtsTBH I don’t really have the same strong feeling as the author of the recommendationin 评:Streaming System(简直炸裂,强势安利).I think the main reason is that I don’t have much experience working on those“traditional” systems as the author does and to me this new kind of way to think aboutmodern distributed system doesn’t “change” the way I think about these systems - it’s“just” a way to think about them. It doesn’t mean that the book and the view it presentsisn’t great; it is wonderful in a way that it provides a way to view things in a higherdimension and when we take a close look a specific problem, we’ll have a good understandingof how it plays a role in the bigger picture, which is think is the most value of this book.In short, it’s a wonderful book that presents a complete new way of thinking aboutdistributed systems based on years of experience of dealing with big data in Google.",
        "url"      : "/2019/07/24/Streaming-Systems/",
        "date"     : "2019-07-24 10:25:00 +0000"
    } ,
    
    {
        "title"    : "Cost Semantics for Parallelism",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Simple Example of Product Types  Deterministic Parallelism  Cost Semantics  Brent’s Principle  Machine with States          Local Transitions      Global Transitions      Scheduling      Cost semantics is to discuss: How long do programs run (abstractly)?The idea of cost semantics for parallelism is that we have the concrete abilityto compute simultaneously.Simple Example of Product TypesFor sequential computation we have:\\[\\frac{e_1\\mapsto_{seq}e_1'}{(e_1,e_2)\\mapsto_{seq}(e_1',e_2)}\\]\\[\\frac{e_1\\ val;e_2\\mapsto_{seq}e_2'}{(e_1,e_2)\\mapsto_{seq}(e_1,e_2')}\\]For parallel computation we have:\\[\\frac{e_1\\mapsto_{par}e_1';e_2\\mapsto_{par}e_2'}{(e_1,e_2)\\mapsto_{par}(e_1',e_2')}\\]Deterministic Parallelism\\[e\\mapsto_{seq}^*v\\ iff\\ e\\Downarrow v\\ iff\\ e\\mapsto_{par}^*v\\]It means that we are getting the same answer, just (potentially) faster.Given a closed program $e$, we can count the number of $\\mapsto_{seq}$(or $\\mapsto^{w}$ “work”) and the number of $\\mapsto_{par}$ (or $\\mapsto^{s}$ “span”)Cost SemanticsWe annotate $e\\Downarrow^{w,s}v$ to keep tract of work and span.\\[\\frac{e_1\\Downarrow^{w_1,s_1}v_1;e_2\\Downarrow^{w_2,s_2}v_2}{e_1,e_2)\\Downarrow^{w_1+w_2,max(s_1,s_2)}(v_1,v_2)}\\]\\[\\frac{e_1\\Downarrow^{w_1,s_1}(v_1,v_2);[v_1/x][v_2/y]e_2\\Downarrow^{w_2,s_2}v}{let(x,y)=e_1\\ in\\ e_2\\Downarrow^{w_1+w_2+1,s_1+s_2+1}v}\\]\\[If\\ e\\Downarrow^{w,s}v\\ then\\ e\\mapsto^w_{seq}v\\ and\\ e\\mapsto^s_{par}v\\]\\[If\\ e\\mapsto_{seq}^w v\\ then \\exists s\\ e\\Downarrow^{w,s}v\\]\\[If\\ e\\mapsto_{par}^s v\\ then \\exists w\\ e\\Downarrow^{w,s}v\\]\\[If\\ e\\Downarrow^{w,s}v\\ and\\ e\\Downarrow^{w',s'}v\\ then\\ w=w',s=s'\\]Brent’s PrincipleIn general, it is a principle about how work and span predict evaluation insome machine.For example, for a machine that has $p$ processors:\\[If\\ e\\Downarrow^{w,s}v\\ then\\ e\\ can\\ be\\ run\\ to\\ v\\ in\\ time\\ O(max(\\frac{w}{p},s))\\]Machine with StatesLocal Transitions\\[\\gamma\\Sigma_{a_1,...,a_n}\\{a_1\\hookrightarrow s_1\\otimes...a_n\\hookrightarrow s_n\\}\\]$a$’s are names for the tasks, and\\[s:=e|join[a](x.e)|join[a_1,a_2](x,y.e)\\]Where \\(join[a](x.e)\\) means to “wait for task $a$ to complete,and then plug it’s value in for $x$”, and \\(join[a_1,a_2](x,y.e)\\) means to wait fortwo tasks.Suppose one of $e_1,e_2$ is not $val$, we have the following, which is also called fork:\\[\\gamma a\\{a\\hookrightarrow(e_1,e_2)\\}\\mapsto\\gamma a,a_1,a_2\\{a_1\\hookrightarrow e_1,a_2\\hookrightarrow e_2,a\\hookrightarrowjoin[a_1,a_2](x,y.(x,y))\\}\\]And we have join:\\[\\gamma a_1,a_2,a\\{a_1\\hookrightarrow v_1,a_2\\hookrightarrow v_2,a\\hookrightarrowjoin[a_1,a_2](x_1,x_2.e)\\}\\mapsto\\gamma a\\{a\\hookrightarrow[v_1/x_2][v_2/x_2]e\\}\\]Similarly for let:\\[\\gamma a\\{a\\hookrightarrow let(x,y)=e_1\\ in\\ e_2\\}\\mapsto\\gamma a_1,a\\{a_1\\hookrightarrow e_1,a\\hookrightarrow join[a_1](z.let(x,y)\\ in\\ e_2)\\}\\]Global Transitions  Select $1\\leq k\\leq p$ tasks to make local transitions  Step locally  Each creates or garbage collectos processes (global synchronization by $\\alpha-renaming$)SchedulingHow to we “Select $1\\leq k\\leq p$ tasks to make local transitions” e.g DFS, BFS",
        "url"      : "/2019/01/22/Cost-Semantics/",
        "date"     : "2019-01-22 20:00:00 +0000"
    } ,
    
    {
        "title"    : "Imperative Programming - Algol",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Modernized Algol (MA) - revised by Robert Harper          Statics      Dynamics      Some issues with type safety      MA - Scoped Assignables                  References                    MA - (Scope) Free Assignables        IssuesHaskell is a dialect Algol!Modernized Algol (MA) - revised by Robert HarperMA = PCF with a $modality$ - distinguishes expressions from commands\\[\\tau=things\\ in\\ PCF|cmd(\\tau) \\\\expressions\\ e=things\\ in\\ PCF|cmd(m)\\\\commands\\ m=ret(e)|bnd(e,x.m)|dcl(e,a.m)\\ (dcl\\ a:=e\\ in\\ m)|set[a](e)\\ (a:=e)|get[a]\\ (get\\ a)\\]$a$’s are assignables not variables! $x$’s are variables! Assignables are not a form of an expression of it’s type. Assignables is a location in memory whose contentshas a type, where we write $a_1\\sim\\tau_1$ (not $a_1:\\tau_1$). Assignables are reallyindices to a family of $get$, $set$ operations, they are not values, arguments,or evaluated. They are just indices, and $get$’s and $set$’s are just capabilitiesto get and set $a$. We can define references, i.e. &amp;a in a real programming language,as a pair $&lt;get_a,set_a&gt;$, which just a thing that gives you access to the capabilitiesof getting and setting $a$.Types and expressions are “pure” - don’t depend on memory, whereas commands are “impure”.Statics\\[\\Gamma\\vdash_\\Sigma e:\\tau\\]where $\\Sigma$ is tye types of assignables, i.e. $a_1\\sim\\tau_1,…,a_n\\sim\\tau_n$.\\[\\Gamma\\vdash_\\Sigma m\\sim:\\tau\\]It means a well-formed command whose return values has type $\\tau$\\[\\frac{\\Gamma\\vdash_\\Sigma m\\sim:\\tau}{\\Gamma\\vdash_\\Sigma cmd(m):cmd(\\tau)}\\]The above is the Introduction rule for $cmd$\\[\\frac{\\Gamma\\vdash_\\Sigma e:cmd(\\tau);\\Gamma,x:\\tau\\vdash_\\Sigma m'\\sim:\\tau'}{\\Gamma\\vdash_\\Sigma bnd(e,x.m')\\sim:\\tau'}\\]The above is the Elimination rule for $cmd$\\[\\frac{\\Gamma\\vdash_\\Sigma e:\\tau}{\\Gamma\\vdash_\\Sigma ret(e)\\sim:\\tau}\\]\\[\\frac{\\Gamma\\vdash_\\Sigma e:\\tau; \\Gamma\\vdash_{\\Sigma,a\\sim\\tau}m'\\sim:\\tau';\\tau\\ mobile;\\tau'\\ mobile}{\\Gamma\\vdash_\\Sigma dcl(e,a.m')\\sim:\\tau'}\\]It means I am declaring an assignable: I declare $a$, initialize it to $e$, andrun the command $m’$. A type is $mobile$ if the value of the type can be pulled outfrom the scope of the assignable. Example of mobile types: eager natural numbers,pairs/sums of mobile types. Example of not mobile types: functions (becausethe body of the function can use assignables even if the ultimate return value is $nat$), commands. This will be explained in later sections when we talk about the dynamics.\\[\\frac{}{\\Gamma\\vdash_{\\Sigma,a\\sim\\tau}get[a]\\sim:\\tau}\\]\\[\\frac{\\Gamma\\vdash_{\\Sigma,a\\sim\\tau}e:\\tau}{\\Gamma\\vdash_{\\Sigma,a\\sim\\tau}set[a](e)\\sim:\\tau}\\]Exercise: We have the following [Pre-]monad defined:\\[T(a):type\\\\r:a\\rightarrow T(a)\\\\b:T(a)\\rightarrow(a\\rightarrow T(b))\\rightarrow T(b)\\]Show that you can define $r$ and $b$ for $T(a)=cmd(a)$.The important fact is that you start with the modality, then they can be formedinto a pre-monad.Dynamics\\[e\\ val_\\Sigma\\]\\[e\\mapsto_\\Sigma e'\\]\\(\\mu||m\\)means a command $m$ in memory \\(\\mu\\). The notation is designed to connecto with concurrency.The idea is that we have a concurrent composition of a main program $m$ running simultaneouslywith threads that govern the contents of each of the location.\\[\\mu||m\\ final_\\Sigma\\]\\[\\mu||m\\mapsto_\\Sigma \\mu'||m'\\]\\[\\frac{}{cmd(m)\\ val_\\Sigma}\\]\\[frac{e\\mapsto_\\Sigma e'}{\\mu||ret(e)\\mapsto_\\Sigma\\mu||ret(e')}\\]\\[\\frac{e\\ val_\\Sigma}{\\mu||ret(e)\\ final_\\Sigma}\\]\\[\\frac{e\\mapsto_\\Sigma e'}{\\mu||bnd(e,x.m_1)\\mapsto_\\Sigma\\mu||bnd(e',x.m_1)}\\]\\[\\frac{\\mu||m\\mapsto_\\Sigma\\mu'||m'}{\\mu||bnd(cmd(m),x.m_1)\\mapsto_\\Sigma\\mu||bnd(cmd(m'),x.m_1)}\\]\\[\\frac{e\\ val_\\Sigma}{\\mu||bnd(cmd(ret(e)),x.m_1)\\mapsto\\mu||[e/x]m_1}\\]\\[\\frac{}{\\mu\\otimes a\\hookrightarrow e||get[a]\\mapsto_{\\Sigma,a\\sim\\tau}\\mu\\otimes a\\hookrightarrow e||ret(e)}\\]Exercise: define $set$We have something called Stack Discipline invented by Dijkstra.The idea is that the assignables in Algol are stack alocated.When I do a $dcl(e,a.m’)$, I declare an assignable in $m’$, I can get it and set it in $m’$.When $m’$ is finished it’s deallocated.\\[\\frac{e\\ val_\\Sigma;\\mu\\otimes a\\hookrightarrow e||m\\mapsto_\\Sigma\\mu'\\otimes a\\hookrightarrow e'||m'}{\\mu||dcl(e,a.m)\\mapsto_\\Sigma\\mu'||dcl(e',a.m')}\\]To rephrase the above in English: Start from lower left: I have a value $e$ which is the initializer of theassignable $a$ and I want to execute $m$ in the presence of that assignable. What canI do? I go above the line and do: let’s extend the memory \\(\\mu\\)with $a$ having the content $e$, and I execute $m$, and I will, in the process ofdoing that, maybe modify some outer assignables (turn \\(\\mu\\) to \\(\\mu'\\)), maybemodify some inner assignables (make $a$ have the content $e’$ instead of $e$ originally)and get a new command $m’$. Then I update the memory (from \\(\\mu\\) to \\(\\mu'\\)), and reset the world (from $m$ to $m’$). In other words, I take the starting state whereI declare $a$ being initialized to $e$ and execute $m$, once you take a step of executionof $m$ in the situation of \\(\\mu\\otimes a\\hookrightarrow e\\), you might have done a $set$in $a$ and updated that to $e’$! The resulting state is: I restart your program in thesituation in which the initializer is not what it was ($e$) but what it becomes as resultof the execution step ($e’$) and then proceed from there.\\[\\frac{e,e'\\ val_\\Sigma}{\\mu||dcl(e,a.ret(e'))\\mapsto_\\Sigma\\mu||ret(e')}\\]To rephrase the above in English: Start from lower left: I declare an assignable$a$ and assign it to $e$, and I am returning a value $ret(e’)$, what do I do next?The idea of the stack discipline is: when you finish executing the body of $dcl$then you get out of it!Some issues with type safetyIn the above formula, in the lower left part, if $e$ has type $\\tau$,then $e’$ has type $\\tau’$ in the context of $\\Sigma,a\\sim\\tau$, but in the lowerright part, $e’$ should also have type $\\tau’$, but with only $\\Sigma$ alone. Intraditional Algol, we can only return $nat$, which means if a numeric value $val$type checks with an assignable present ($\\Sigma,a\\sim\\tau$), it will type checkswith the assignable absence (only $\\Sigma$), only under some conditions!So here is the question: Under what condition is the following statement true?If $e$ is a value of type $nat$ and type checks with the assignable $a$, it will alsotype check in type $nat$ without $a$.Answer: Only if the successors are valued eagerly! if the successors are lazy,then the arguments of the successors are unevaluated expressions (they are no longervalues $val$)! Algol only makes sense if the constructors of the successors are eager!Here is an (clever) example of a successor of \\(\\mathbb{N}\\) doesn’t type check if lazy. I want asuccessor of something, which is an expression, that uses an assignable $a$. The goalis it will not type check outside of scope of $a$:\\[S((\\lambda x:nat.z)(cmd(get([a]))))\\]Explanation: We have a constant function that takes in a $nat$ and return 0 $z$.And to type check the argument $cmd(get([a]))$ we have to eventually type check $a$,even though it doesn’t matter the ultimate return value of the function because it’sconstant!This is a perfect example of the interactions between language features. We might thinkthat whatever I do with the PCF level whatever we don’t care lazy or eager and thecommand level is separated. This is wrong! The hard part of a language design is to makeeverything fit in a coherent way. This is way language design is hard!So traditional Algol they make things work by restricting the return value only $nat$,and make $nat$ eager. A better idea is that we can demand the result type of $dcl$ $mobile$!And we also need to restrict that we cannot assign values that aren’t $mobile$, becausewe can assign the value from a return value. This is the explanation of theabove “This will be explained in later sections”.MA - Scoped AssignablesReferencesWe can define $ref(\\tau)$ (immobile) as we mentioned briefly before in the following ways:Concretely:\\[ref(\\tau)\\triangleq cmd(\\tau)\\times(\\tau\\rightarrow cmd(\\tau))\\]Where $cmd(\\tau)$ is just the getter and $\\tau\\rightarrow cmd(\\tau)$ is the setter.Then we can have:\\[getref(&lt;g,s&gt;)\\mapsto g\\\\setref(&lt;g,s&gt;)\\mapsto s\\]The problem is, if you look at the type $ref(\\tau)$, it only says it has getter andsetters, but it doesn’t say the getter and setter are for the same assignable! Soyou can have a getter for $a$ and a setter for $b$ and you won’t know the difference.So then we can define it in another way:Abstractly:The type is still $ref(\\tau)$, but we have the following elim rules:\\[getref(\\&amp;a)\\mapsto get[a]\\\\setref(\\&amp;a,e)\\mapsto set[a](e)\\]MA - (Scope) Free AssignablesAll types are mobile. Previously because the stack discipline is causing ustrouble about mobility, and we go back to to add some mobility rules in the statics.But there are other ways to fix this: let’s change the statics, make every type mobile,and we’ll make our dynamics fit that. Here are the new dynamics. (Also calledscope-free dynamics, or in other words, assignables are heap allocated.)\\[\\gamma\\Sigma\\{\\mu||m\\}\\mapsto\\gamma\\Sigma'\\{\\mu'||m'\\}\\]Note the above transition is unlabelled.\\[\\frac{e\\ val_\\Sigma}{\\gamma\\Sigma\\{\\mu||dcl[\\tau](e,a.m)\\}\\mapsto\\gamma\\Sigma,a\\sim\\tau\\{\\mu\\otimes a\\hookrightarrow e||m\\}}\\]\\[\\frac{e\\ val}{\\gamma\\Sigma\\{\\mu||ret(e)\\}\\ final}\\]PCF (FPC with recursive types $rec$) + commands above with free assignables$\\approx$ Haskell :)IssuesIn Algol, we have a clear distinction between expressions and commands; they arecompletely separated. There are many benefits of doing that. But in the “real world”,in doing that, we lose a lot of benign effects. For example efficiency, we lose:  laziness/memoization  splay trees - self-adjusting data structures  …While we are condemning benign effects, we do rely on them in real life.",
        "url"      : "/2019/01/21/imperative-programming/",
        "date"     : "2019-01-21 19:58:00 +0000"
    } ,
    
    {
        "title"    : "Recursive Types",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Recap for Product/Sum Types          Product Types                  Introductions          Eliminations                    Sum Types                  Introduction          Eliminations                    Unit        Recursive Types in a Partial Language          Examples                  Natural Numbers          Lists                    Construction      Introductions      Eliminations      Dynamics      Examples using Recursive Types        Bridging Recursive Types and Programs          Self Types                  Introductions          Eliminations          Dynamics                    Construct Recursive Programs                  From Recursive Computation to Self Types          From Self Types to Recursive Types                    Recap for Product/Sum TypesBrief recap for product and sum to have a consensus on notations.Product Types\\[\\tau_1\\times\\tau_2\\]Introductions\\[\\frac{\\Gamma\\vdash e_1:\\tau_2,\\Gamma\\vdash e_2:\\tau_2}{\\Gamma\\vdash &lt;e_1,e_2&gt;:\\tau_1\\times\\tau_2}\\]Eliminations\\[\\frac{\\Gamma\\vdash e:\\tau_1\\times\\tau_2}{\\Gamma\\vdash fst(e):\\tau_1}\\]\\[\\frac{\\Gamma\\vdash e:\\tau_1\\times\\tau_2}{\\Gamma\\vdash snd(e):\\tau_2}\\]Sum Types\\[\\tau_1+\\tau_2\\]Introduction\\[\\frac{\\Gamma\\vdash e_1:\\tau_1}{\\Gamma\\vdash inl_{\\tau_1,\\tau_2}(e_1):\\tau_1+\\tau_2}\\]\\[\\frac{\\Gamma\\vdash e_2:\\tau_2}{\\Gamma\\vdash inr_{\\tau_1,\\tau_2}(e_2):\\tau_1+\\tau_2}\\]Eliminations\\[\\frac{\\Gamma\\vdash e:\\tau_1+\\tau_2;\\Gamma,x_1:\\tau_2\\vdash e_1:\\sigma;\\Gamma,x_2:\\tau_2\\vdash e_2:\\sigma}{\\Gamma\\vdash case(e)of\\{inl(x_1)\\hookrightarrow e_1,inr(x_2)\\hookrightarrow e_2\\}:\\sigma}\\]Unit\\[\\frac{}{\\Gamma\\vdash &lt;&gt;:unit}\\]Recursive Types in a Partial LanguageExamplesNatural NumbersWe can write\\[\\mathbb{N}\\cong unit+\\mathbb{N}\\]where $\\cong$ means “type isomorphism”which means to write functions back and forth that compose to the identity.From left to right:\\[\\lambda x:\\mathbb{N}:=rec\\{0\\hookrightarrow inl(&lt;&gt;),s(y)\\hookrightarrow inr(y)\\}\\]From right to left:\\[\\lambda x:unit+\\mathbb{N}:=case(z)of\\{inl(\\_)\\hookrightarrow 0,inr(y)\\hookrightarrow s(y)\\}\\]It works because anything is observationally equivalent to $&lt;&gt;$ at type $unit$:\\[\\_\\sim_{unit}&lt;&gt;\\]ListsAny list is either the empty list or a cons with the type and another list.\\[\\tau list\\cong unit+(\\tau\\times\\tau list)\\]From left to right:\\[\\lambda l:\\tau list:=case(l)of\\{[]\\hookrightarrow inl(&lt;&gt;),x::xs\\hookrightarrow inr(x,xs)\\}\\]ConstructionWhat we’ve got here is that: certain types can be characterize as solutions to equationsof the form:\\[t\\cong\\sigma(t)\\]$\\mathbb{N}$ solves $t=unit+t$$\\tau list$ solves $t=unit+(\\tau\\times t)$\\[\\frac{\\Delta,t\\ type\\vdash\\tau\\ type}{\\Delta\\vdash rec(t.\\tau)\\ type}\\]which is a/the solution to $t\\cong \\tau(t)$. i.e.\\[rec(t.\\tau)\\cong [rec(t.\\tau)/t]\\tau\\]Introductions\\[\\frac{\\Gamma\\vdash e:[rec(t.\\tau)/t]\\tau}{\\Gamma\\vdash fold(e):rec(t.\\tau)}\\]Eliminations\\[\\frac{\\Gamma\\vdash e:rec(t.\\tau)}{\\Gamma\\vdash unfold(e):[rec(t.\\tau)/t]\\tau}\\]Dynamics\\[unfold(fold(e))\\mapsto e\\]Examples using Recursive Types\\[\\mathbb{N}:=rec(t.unit+t)\\]\\[z:\\mathbb{N}:=fold(inl(&lt;&gt;))\\]\\[s(e):\\mathbb{N}:=fold(inr(e))\\]Bridging Recursive Types and ProgramsIn this language, even though there is no loops, no recursive functions in the terms,we can still get general recursion just from self-referencial types. Meaning thatself-referential types will give us self-referential codes. How to do?  Define a type of self-referential programs, and get $fix(x.e)$ from it  Define the type of self-referential programs from $rec(t.\\tau)$Self TypesThe following thing is only used as a bridge between $fix$ and $rec$ such thatwe can define both $fix$ and $rec$ from it. It’s only to help the constructions.In the following rules, $x$ could be interpreted as this in a normal programming language.$\\tau self$ represents a self-referential computation of a $\\tau$Introductions\\[\\frac{\\Gamma,x:\\tau self\\vdash e:\\tau}{\\Gamma\\vdash self(x.e):\\tau self}\\]Eliminations\\[\\frac{\\Gamma\\vdash e:\\tau self}{\\Gamma\\vdash unroll(e):\\tau}\\]Dynamics\\[\\frac{}{self(x.e)\\ value}\\]\\[\\frac{e\\mapsto e'}{unroll(e)\\mapsto unroll(e')}\\]\\[unroll(self(x.e))\\mapsto[self(x.e)/x]e\\]Construct Recursive ProgramsNow we have $\\tau self$, the above two steps can be rephrased as:  We want to get a recursive computation of any type, i.e. $fix(x.e):\\tau$, from self types $\\tau self$  We want to get self types $\\tau self$ from recursive types $rec(x.e)$From Recursive Computation to Self TypesWe have $\\tau self$, and we want:\\[\\frac{x:\\tau\\vdash e:\\tau}{fix(x.e):\\tau}\\]\\[\\frac{}{fix(x.e)\\mapsto[fix(x.e)/x]e}\\]Solution:\\[fix(x.e):\\tau:=unroll(self(y:\\tau self.[unroll(y)/x]e))\\]From Self Types to Recursive TypesWe want to solve:\\[\\tau self\\cong \\tau self\\rightarrow\\tau\\]Solution:\\[\\tau self:=rec(t.(t\\rightarrow\\tau))\\]\\[self(x.e):=fold(\\lambda (x:\\tau self).(e:\\tau))\\]\\[unroll(e):=(unfold(e):\\tau self\\rightarrow \\tau)(e)\\]",
        "url"      : "/2019/01/20/Recursive-Types/",
        "date"     : "2019-01-20 20:25:00 +0000"
    } ,
    
    {
        "title"    : "Jekyll Search Solutions",
        "category" : "Jekyll",
        "tags"     : "Jekyll, Blog",
        "content": "  Updates  问题和需求  解决方案（暂时）  总结吐槽Updates终于我找到了这个。但是这个其实充满了各种bug，比如我提交的这个。里面还有很多的bug以及我不满意的地方（不知道为什么能活到现在……），反正所以我现在自己写了我自己的版本。现在的左侧搜索框应该能显示搜索出来的节选高亮了。问题终于解决了（大概Agolia的我还是先留着吧，毕竟我自己写的也没有特别满意，而且他们也帮我卖广告了所以就都保留着想用哪个用哪个吧。。问题和需求最近心血来潮想自己在博客上加上搜索功能，因为我用的这个框架的程序员一直不更新就一气之下想着自己写一个算了。问题在于我就完全没碰过前端，工作也跟写网页一点关系都没有，于是我就google呗，看看有啥好的在jekyll创建的静态网页有好用的搜索。问题在于，无论google用中文搜索还是英文都没有一个完美的方案来对应我的需求，见到有别人的博客能实现我的需求但是好像并没有开源……好吧说了这么多我先列个需求：  方便：最好是一个小插件或者一段js就好，我懒得换主题而且换了的新主题就算搜索功能很好也不一定别的功能很好  能搜索全文  搜索出来的东西能高亮：这个是最重要的feature，也是我到处怎么找开源也找不到的feature。意思就是你搜索正文中的东西能把match上的文字以及前后文的一小段都显示出来而且高亮match。我看到的基本要么只显示title，要么只能显示正文节选。  免费开源：这样我自己也能改样式  API好用好吧基本上2+3就能排除掉一大箱的google上的选项了，下面说说我的解决方案吧……解决方案（暂时）我用了两套搜索工具，也没说一定要用两套只不过刚好写了两套就都放着呗……首先是这个。使用很简单方便github page上也有教程，API也挺好用的，满足了1245，但是3这个点挺麻烦的，要修改它本身js的search的function，which is doable但是其实跟我自己重新写一个插件没什么区别了……然后我专门写了一个单独的page用的algolia链接里面是详细的教程，使用倒是也挺方便的，最主要是能高亮啊啊啊！唯一的问题就是它不是开源的，有免费的plan倒是如果你的博客很小倒没所谓，但是不开源的我自己用起来也不舒服，毕竟没什么控制权……以上两种解决方案在我github上都能看到，分别是_includes/algolia.html和_includes/sidebar-search.htmlcss和js什么的自己看着搜索就能看到了……代码会跟上面的攻略不一样毕竟这是我自己的博客……总结吐槽anyway反正就这么先用着了，有更好的方案再说……不得不吐槽网上那些程序员自己写的博客前端真是一个比一个坑……博客文章雷同不说…写关于Jekyll博客搜索功能的攻略然而自己博客的搜索功能蠢的一B……有些前端的排版还完全不对…其它什么页面404啊搜索只能搜索title啊就不提了真是坑爹……还有些写了搜索攻略的但是博客本身就没搜索功能的……好了发泄了一通，不管怎么说用这两套方案kind of满足我的需求了……最好还是有个开源的插件吧algolia替换掉啦…嘛以上",
        "url"      : "/2019/01/20/Jekyll-Search-Solutions/",
        "date"     : "2019-01-20 15:13:00 +0000"
    } ,
    
    {
        "title"    : "Recursive Programs",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Partial Functions  Recursive Programs in Plotkin PCF          Why do we have partial functions      Statics      Dynamics      Equivalence                  Main theorm          Fixed point induction                    Partial FunctionsSystem $T$, $F$ are total languages.PCF (Programming Language for Computable Functions) (By Gordon Plotkin) - E. col. of partial languages.Idea: Extending the theory of computability to HIGHER TYPE. Standard computabilitycourses only talk about computation over $\\mathbb{N}$, but nothing beyond that.e.g. $gcd$ is defined by the following (recursive) equations:\\[gcd(m,n) = m\\ if\\ m=n\\\\gcd(m,n) = gcd(m-n,n)\\ if\\ m&gt;n\\\\gcd(m,n) = gcd(m,n-m) ow\\]We can transcribe the above in a real programming language (ML, Haskell) in an evident way.We call it “Recursive function” in a typical course - it is using recursion.Then the next typical topic would be about “stack”.Recursion has nothing to do with the stack. (One obvious example is flip-flop with two $NOR$ gates.)Better idea (correct idea): simultaneous equations in the variable gcd. Solve for gcd!We want: \\(gcd\\ s.t.\\ gcd=G(gcd)\\) where $G$ is the function of definition my $gcd$.We are looking for a FIXED POINT.The equations only make sense (solution exists) with computable partial functions.Recursive Programs in Plotkin PCF($\\rightharpoonup$ means partial functions)\\[\\tau:=nat|\\tau_1\\rightharpoonup\\tau_2\\]\\[e:=x|z|s(e)|ifz(e,e_2,x.e_1)|\\lambda x:\\tau.e|e_1(e_2)|fix\\{\\tau\\}(x.e)(written\\ as\\ fix\\ x\\ is\\ e)\\]\\(fix\\) is called general recursion.Examples of equations:\\[gcd(m,n)=gcd(m,n)\\]\\[gcd(m,n)=1+gcd(m,n)\\]Both equations have solutions (only in the context of partial function, 2nd won’t workfor total setting becuase it’s like $x=x+1$): totally undefined function!\\(fix\\ x\\ is\\ e\\) is the solution to $x=e_x$\\(\\perp_{\\tau}=fix\\{\\tau\\}(x.x)\\) ($\\perp$ is called bottom (the least element ina certain pre-order)) (This is undefined, which mean it has no value)Then we can define \\(gcd \\triangleq fix\\{nat\\rightarrow nat\\}(x.G(x))\\) (Note: Thereis not occurrence of $gcd$ on the right hand side! It is solving a fixed point equation!)Why do we have partial functions  Challenge: Try to define $gcd$ in $Godel’s T$ (using products, sums, subtractions, equality/inequality). This is hard! Because you must bake the termination proof into the code(not your head).  Blum Size Theorm: Fix an expansion factor, say $2^{2^n}$, there is a function\\(f:\\mathbb{N}\\rightarrow\\mathbb{N}\\) whose shortest program in $T$ (or any total language)is $2^{2^n}$ longer than it’s code in $PCF$.Statics\\[\\frac{\\Gamma,x:\\tau\\vdash e:\\tau}{\\Gamma\\vdash fix\\{\\tau\\}(x.e):\\tau}\\](Note: All $\\tau$’’s are the same! The crucial thing: you assume what you are trying prove!)In other words, you assume what you are trying to prove, and you are showing that assumptionis tenable (show that nothing contradicts that fact), then it just is the case! I don’tshow anything absolutely outright. I just assume what I am trying to prove and considerthat sufficient to be true because I don’t care if thing diverges like in partial maps.Dynamics\\[\\frac{}{fix\\{\\tau\\}(x.e)\\mapsto[fix\\{\\tau\\}(x.e)/x]e}\\](It’s just “$F\\mapsto[F/x]e$”) It’s called “unrolling recursion”.There is no stack in the dynamics! :)EquivalenceMain theorm\\[If\\ e:\\tau\\ then\\ e\\sim_{\\tau}e\\]Fixed point inductionFor admissible relations only:To show:\\[fix(x.e)\\sim_\\tau fix(x.e')\\]It’s suffice to show:\\[\\forall n\\ge 0\\ fix^{(n)}(x.e)\\sim_\\tau fix^{(n)}(x.e')\\](\\(fix^{(n)}\\) means the n-fold unrolling of the fix)",
        "url"      : "/2019/01/18/Recursive-Programs/",
        "date"     : "2019-01-18 22:25:00 +0000"
    } ,
    
    {
        "title"    : "Parametricity - Logical Equivalence for Polymorphic Types",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Hereditary Termination and Logical Equivalence Recap  Extension to Polymorphic Types          System T - Extend to F        Logical Equivalence of Polymorphic Types          “Admissible Relation”      Formal Definition        Main Theorm  Existential TypeHereditary Termination and Logical Equivalence RecapHereditary Termination:  $HT_{\\tau}(e)$ hereditary termination at type $\\tau$  $HT_{nat}(e)$ $iff $$e\\mapsto^*z$ or $e\\mapsto^*s(e’)$ such that $HT_{nat}(e’)$ (inductively defined)  $HT_{\\tau_1\\rightarrow\\tau_2}(e)$ $iff $$if\\ HT_{\\tau_1}(e_1)\\ then\\ HT_{\\tau_2}(e(e_1))$ (implication)Logical Equivalence:  $e\\sim_{nat}e’$ $iff $either $e\\mapsto^*n^*\\leftarrow e’$ or $e\\mapsto^*s(e_1),e’\\mapsto^*s(e_1’),e_1\\sim_{nat}e_1’$  $e\\sim_{\\tau_1\\rightarrow\\tau_2}e’$ $iff $$if\\ e_1\\sim_{\\tau_1}e_1’\\ then\\ e(e_1)\\sim_{\\tau_2}e’(e_1’)$Some theorem:  $e:\\tau\\ implies\\ e\\sim_{nat} e$  $e\\sim_{\\tau}e’$ $iff$ $e\\simeq_{\\tau}e’$  $e\\sim_{\\tau}e\\ iff\\ HT_{\\tau}(e)$Extension to Polymorphic TypesSystem T - Extend to F\\[\\tau ::= nat|\\tau_1\\rightarrow\\tau_2|\\forall t.\\tau|t(variable\\ type)\\\\e ::=x...|\\Lambda t.e(type\\ abstraction)|e[\\tau](type\\ application)\\]Now that we can define Existential Type in system F (client is polymorphic):\\[\\exists t.\\tau:=\\forall u(\\forall t.\\tau\\rightarrow u)\\rightarrow u\\]Logical Equivalence of Polymorphic TypesWe can do the following:\\[e\\sim_{\\forall t.\\tau}e'\\ iff\\ \\forall\\sigma(small)type,e[\\sigma]\\sim_{[\\sigma/t]\\tau}e'[\\sigma]\\]But this is saying type variables range over type expression, which is not what we want.We want to say type variables range over all conceivable types (not sure the onesyou can write down)Idea: types are certain relations!“Admissible Relation”  domain type $\\tau_1$  range type $\\tau_2$  binary relations between exp’s of those types $R:\\tau_1\\leftrightarrow\\tau_2$  Exact definition varies with application language  Demand respect observational equality:\\[e_1Re_2,e_1\\cong e_1',e_2\\cong e_2'\\ iff\\ e_1'Re_2'\\]Getting back to equiv. of polymorphic types using our admissible relations. The ideais approximately to say:\\[e\\sim_{\\forall t.\\tau}e'\\ iff\\ \"\\forall R\\ admissible\\ e\\sim_{\\tau}e'\\ modulo\\ t=R\"\\]In order to make this precise we have to define a more general relation which isheterogeneous.Formal DefinitionDefine:\\[e\\sim_\\tau e'[\\eta:\\delta\\leftrightarrow\\delta']\\]idea:\\[\\delta:t_1\\mapsto\\sigma_1,...,t_n\\mapsto\\sigma_n \\\\\\delta:t_1\\mapsto\\sigma_1',...,t_n\\mapsto\\sigma_n' \\\\\\eta:t_1\\mapsto R_1\\sigma_1\\leftrightarrow\\sigma_1',...,t_n\\mapsto R_n\\sigma_n\\leftrightarrow\\sigma_n' (R\\ is\\ admissible\\ relation)\\\\e:\\hat{\\delta}(\\tau),e':\\hat{\\delta'}(\\tau) (e\\ and\\ e'\\ are\\ disparate\\ types)\\\\\\]  $e\\sim_t e’[\\eta:\\delta\\leftrightarrow\\delta’]\\ iff\\ e\\eta(t)e’ (\\sigma(t)\\leftrightarrow\\sigma’(t))$  $e\\sim_{nat} e’[\\eta:\\delta\\leftrightarrow\\delta’]\\ iff\\ (e\\mapsto^* z\\ and\\ e’\\mapsto^* z)or(e\\mapsto^* s(e_1),e’\\mapsto^* s(e_1’),e_1\\sim_{nat}e_1’[\\eta:\\delta\\leftrightarrow\\delta’])$  $e\\sim_{\\tau_1\\rightarrow\\tau_2}e’[\\eta:\\delta\\leftrightarrow\\delta’]\\ iff\\ e_1\\sim_{\\tau_1}e_1’[\\eta]\\supset e(e_1)\\sim_{\\tau1}e’(e_1’)[\\eta]$  $e\\sim_{\\forall t.\\tau}e’[\\eta]\\ iff\\ \\forall\\sigma,\\sigma’\\forall R:\\sigma\\leftrightarrow\\sigma’\\ admissible,e[\\sigma]\\sim_\\tau e[\\sigma’] [\\eta[t\\mapsto R]:\\delta[t\\mapsto\\sigma]\\leftrightarrow\\delta’[t\\mapsto\\sigma’]]$ (Note that $\\tau$ in $\\sim_\\tau$ has free $t$’s in it and $\\tau$ is a piece of $\\forall t.\\tau$!)Main TheormIn system F:\\[If\\ e:\\tau,then\\ e\\sim_\\tau e'[\\emptyset]\\]And there is Identity Extension:“If all type variables are interpreted as obs. equiv., then logically related thingsare obs. equiv.”Existential TypeWe have existential type of a counter defined as\\[\\exists t:&lt;inc:t\\rightarrow t,dec:t\\rightarrow t,val:t\\rightarrow nat, zero:t&gt;\\]and we have the client of that as type:\\[\\forall t((t\\rightarrow t)\\rightarrow(t\\rightarrow t)\\rightarrow (t\\rightarrow nat)\\rightarrow t \\rightarrow \\rho)\\]We have two implementation of our counter: $I$ and $II$, and their types are related by $R$$\\tau_I R \\tau_{II}$ where $R$ means “they represent the same number”.Because the client has that such polymorphic type, it is “uniform accross all possible t’s” bythe main theorm:\\[if\\\\inc_I\\sim_{t\\rightarrow t}inc_{II}[t\\mapsto R]\\\\dec_I\\sim_{t\\rightarrow t}dec_{II}[..]\\\\val_I\\sim_{t\\rightarrow nat}val_{II}[..]\\\\zero_I\\sim_t zero_{II}\\\\then\\\\client[\\tau_I] (inc_I)(dec_I)(val_I)(zero_I)\\\\\\cong_{obs}\\\\client[\\tau_{II}] (inc_{II})(dec_{II})(val_{II})(zero_{II})\\\\\\]",
        "url"      : "/2018/09/30/Logical-Equivalence-for-Polymorphic-Types/",
        "date"     : "2018-09-30 17:25:00 +0000"
    } ,
    
    {
        "title"    : "Polymorphic Functional Programming",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  An example to start off  System F (Girard|Reynolds)  Polymorphism          Parametric Polymorphism                  Some examples                      Existential Type          A Example      An example to start offFirst we have the following two examples of recursive functionsfunc toStrings(L: int list) string list =    case l of      [] =&gt; []      | x::xs =&gt;         IntToString(x)::toStrings(xs)func add(l: int list, a: int) int list =    case l of      [] =&gt; []      | x::xs =&gt; (a+x)::add(xs,a)We can have a more general function as the followingmap: for all a and b, (a-&gt;b)-&gt;a list -&gt; b listfun map f l =    case l of      [] =&gt; []      | x::xs =&gt; f(x)::map[a][b](xs)It is called polymorphism generality.To achieve these, we need variables in type: we have the following judgement\\(\\Delta\\vdash\\tau\\ type\\)where $\\Delta$ means $t_1\\ type,…,t_n\\ type$Take the map function as an example:\\[a\\ type,b\\ type\\vdash(a\\rightarrow b)\\rightarrow a\\ list\\rightarrow b\\ list\\ type\\]It reads as: $(a\\rightarrow b)\\rightarrow a\\ list\\rightarrow b\\ list$ is a typeassuming that $a$ is a type and $b$ is a type.The inference rule for that\\[\\frac{\\Delta\\vdash\\tau\\ type;\\Delta\\vdash\\sigma\\ type}{\\Delta\\vdash\\tau\\rightarrow\\sigma\\ type}\\]\\[\\frac{\\Delta,t\\ type\\vdash\\tau\\ type}{\\Delta\\vdash\\forall\\ t.\\tau\\ type}\\]System F (Girard|Reynolds)The idea is: allow $\\lambda$ and application for type variables\\[\\frac{\\Delta,t\\ type;\\Gamma\\vdash e:\\tau}{\\Delta,\\Gamma\\Lambda t.e:\\forall t.\\tau}\\](With function $\\rightarrow$)Application rule:\\[\\frac{\\Delta,\\Gamma\\vdash e:\\forall t.\\tau;\\Delta\\vdash\\sigma\\ type}{\\Delta,\\Gamma\\vdash e[\\sigma]:[\\sigma/t]\\tau}\\](We use capital lambda $\\Lambda$ as oppose to $\\lambda$ just to distinguish the factthat this is a type variable but not a term variable)Then map function would become\\[map=\\Lambda a\\Lambda b.\\lambda f:(a\\rightarrow b).\\lambda x:a\\ list\\]The Dynamics:\\[\\frac{e\\mapsto e'}{e[\\sigma]\\mapsto e'[\\sigma]}\\]\\[\\frac{}{(\\Lambda.e)[\\sigma]\\mapsto[\\sigma/t]e}\\]Substitution rules:\\[If\\ \\Delta,t\\ type\\vdash\\tau\\ type\\\\and\\ \\Delta,t\\ type;\\Gamma\\vdash e:\\tau\\\\and\\ \\Delta\\vdash\\sigma\\ type\\\\then\\ \\Delta,\\Gamma\\vdash[\\sigma/t]e:[\\sigma/t]\\tau\\]PolymorphismThere are actually two kinds of polymorphism: Intensional, and ParametricIntensional: Different code at different types (e.g. some case switch in the code says:if type is int do this, if type is double do that, …) (more programs)Parametric: Same code at many types (e.g. map function above) (more theorems)Parametric PolymorphismWe can infer more just from the types if something is parametric polymorphic.Take some examples to see what can we say about any function of the typeSome examples$\\forall a\\forall b, (a\\rightarrow b)\\rightarrow a\\ list \\rightarrow b\\ list$Every element of the result list must be $f(x)$ for some $x:a$) in the input list!!!$r:\\forall a\\forall b, a\\ list\\rightarrow a\\ list$All a’s in output must come from input!!!Note: if we are writing a specific function of specific type $int\\ list\\rightarrow int\\ list$,it could include something in the output that’s not in the input.From the above $r$ function with it’s type, and the map function definition above (not just the type!),for any $f:\\tau\\rightarrow\\sigma$, and any $l:\\tau\\ list$:\\[map\\ f(r[\\tau]l)=r[\\sigma](map\\ fl)\\]Existential TypeThis adds nothing to System F, though it can actually be derived from System F\\[\\frac{\\Delta,t\\ type\\vdash\\tau\\ type}{\\Delta\\vdash\\exists t.\\tau\\ type}\\]\\[\\frac{\\Delta\\vdash\\sigma\\ type;\\Delta,\\Gamma\\vdash e:[\\sigma/t]\\tau}{\\Delta,\\Gamma\\vdash pack[\\sigma]e:\\exists t:\\tau}\\]Application:\\[\\frac{\\Delta,\\Gamma\\vdash\\sigma\\ type;\\Delta,\\Gamma\\vdash e_1:\\exists t.\\tau;\\Delta,t\\ type,\\Gamma,x:\\tau\\vdash e:\\sigma}{\\Delta,\\Gamma\\vdash open(t,x)=e_1\\ in\\ e:\\sigma}\\]It corresponds to modules/abstract/private/hidden types. We want to draw boundariesbetween the implementation and the clients that use it, via some abstract types.i.e. interface!A Example$\\underline{Counter}$:\\[\\exists t:&lt;zero \\hookrightarrow t,Incr\\hookrightarrow t\\rightarrow t,value\\hookrightarrow t\\rightarrow nat&gt;\\]We can have two implementations of this:(1) Take $t$ to be $nat$ (unary)\\[zero=0\\\\Incr(x)=s(x)\\\\value(x)=x\\]in which case\\[pack[nat]\\\\&lt;zero\\hookrightarrow 0,Incr\\hookrightarrow\\lambda x.s(x),value\\hookrightarrow\\lambda x.x&gt;\\](2) Take $t$ to be $list(bit)$ (binary)\\[pack[list(bit)]\\\\&lt;zero\\hookrightarrow[0],\\\\Incr\\hookrightarrow func\\ incr\\ []=[1]|incr(0::bs)=1::bs|incr(1::bs)=0::incr(bs)\\\\value\\hookrightarrow\\lambda bs.\\Sigma 2^i*nth\\ bs\\ i&gt;\\]Now we have two kinds of implementation of the same interface!A client of $\\underline{Counter}$ looks like:$open(c,&lt;zero,incr,value&gt;)=$some impl (unary/binary) in some code that uses $zero:c,incr:c\\rightarrow c,value:c\\rightarrow\\ nat$Note: the abstract type doesn’t escape the $open$Note: the unary counter(UC) impl should be observational equivalent to the binary counter(BC),which means no client can distinguish between them, if  client only uses exposed operations  implementation are obs eqv. (i.e. they behave the same)Key idea: choose a “simulation” relation between UCs and BCs that is preserved by the operations  UC zero is related to BC zero  UC incr is related to BC inc  UC value is related to BC value",
        "url"      : "/2018/09/09/Polymorphic-Functional-Programming/",
        "date"     : "2018-09-09 17:25:00 +0000"
    } ,
    
    {
        "title"    : "In Searching Counter Examples for Ramsey Number",
        "category" : "Cloud Computing",
        "tags"     : "Math",
        "content": "  What is Ramsey Number?  Some Basic Ideas  What Doesn’t Work          Simulated Annealing/Stochastic Tunneling      Clique Counts Approximation        Our Design and Implementation          Clique Counting      Tabu Search      Bloom Filter      The Main Trick      Server-Client Protocol        Summary and Possible ImprovementsThis is a project I did in my undergrad, so I might forget some of the details.Also some of the design choices might seen stupid for now, but I am writing themdown anyway just for note-taking. The codes on Github.And here is the presentation we have.I assume the readers of this post have basically knowledge of computer science,like graphs, but not necessarily mathematics (I am really bad at mathematics in fact).What is Ramsey Number?According to Wolfram Research,it is to Find the minimum number of guests that must be invited so that at least mwill know each other or at least n will not know each other. The solutions areknown as Ramsey numbers. To phrase it in Computer Science word, for $R(n,m)$,it is to find a complete graph with minimum vertices such that if we color edgesin two colors, there is no monochromatic n-cliques and m-cliques in any edge labelling.A counter example of $R(n,m)$ is basically a graph with a a labeling that containsmonochromatic clique(s).How hard is this? Well we know that $R(3,3)=6$, $R(4,4)=18$, and we don’t know anythingabove $R(4,4)$. For $R(10,10)$, the current bounds are $798$ to $23556$. In the project,we are to design an algorithm/system that make use of basically endless computingpower from both super-computers (XSEDE, CondorHTC), and cloud platforms (AWS) tofind the largest counter example of $R(10,10)$.Some Basic IdeasWe didn’t do edge coloring in our implementation. Instead, we split the graph intotwo graphs by the colors of the edges, and count the cliques in both graphs.There are various reasons we make this decision, which we’ll talk about later. Fornow just remember the setting that we are dealing with two graphs now.The basic flow of the algorithm is the following:  Start from a random very small graph (like 20 vertices), split it into two, andwork on this first.          Count the cliques of two graphs. If the sum of the cliques of two graphs is 0, we are done with this graph.      If not, we need to “flip” edge(s), namly remove edge(s) from one graph and add it to the other graph. Then count the cliques again. Repeat until the sum is 0.        After we done with a graph, record the number, and add a vertex/vertices to thatgraph to form a new graph, for reason being that the old graph doesn’t have any cliques,so adding vertices to that graph is likely to not produce many cliques. And then repeatthe process.So there are a few things we need to design/implement, which we’ll talk about later:  How do we count cliques?          Well it is a NPC problem so we might just do the dummy method, but infact using a smarter method would help.        How do we choose which edge to flip?          We can do some greedy search like: try one edge, if good (sum of the cliques goes down) then keep the change, if not good then discard the change and try another one, and keep going until the count gets to 0. This won’t get us very far as we will reach some “local minimum”, in which case we’ll need some ways to get out of it.        How do we add vertices to old graph?          We can blindlessly add vertices, or we might want to have some smartness.        How do we make maximum use of those compute power?          This might sound stupid, but because we have many different platforms/computers located all around the world, we’ll need some kind of protocol, and even build a distributed system to make use of them.      What Doesn’t WorkThis section will cover some methods to try to solve some of the above questionsand doesn’t work quite well for us. It doesn’t mean that it won’t work well in general,but just us.Simulated Annealing/Stochastic TunnelingSimulated Annealing(SA), quoted from Wikipedia,is a probabilistic technique forapproximating the global optimum of a given function. Here is the pseudocode:- Let s = s0- For k = 0 through kmax (exclusive):   - T ← temperature(k ∕ kmax)   - Pick a random neighbour, snew ← neighbour(s)   - If P(E(s), E(snew), T) ≥ random(0, 1):      - s ← snew- Output: the final state sAnd stochastic tunneling (STUN), also quoted from WIkipediais an approach to global optimization based on theMonte Carlo method-sampling of the function to be objective minimized in which thefunction is nonlinearly transformed to allow for easier tunneling among regionscontaining function minima.Both of the above methods are randomized simulation methods. Here is the majordrawbacks, due to which we discard them in our final implementation: Setting Parameters.The most vaguely described aspect of various simulation/ML algorithmsin my opinion is how to set the proper parameters. Most of them say somethinglike “we pick $k=2,t_0=5,s=100$ for our experiment because they work the best; chooseyour own set of parameter in production”, which equivalently says “choose whateverparameters you feel like”. It is quite hard in our case to determine what parameterworks best for us as the problem size is so large that even parameters themselvesneed to be adjusted during different phases of simulation. There is also somethingcalled Detrended Fluctuation Analysis (DFA),which is a way to detect if the simulation reaches a local minimum and thus adjustthe parameter to work better. But the problem is, the algorithm for adjusting parametersalso needs some parameters to work well…So basically you have to set at least oneparameter in order to get things working. And to get things working faster, especiallyin such a large problem space, you really have to set parameters well. In our case,due to lack of experience, we can’t get things run fast enough, and thereforewe basically give up for any kinds of randomized simulation algorithm.Clique Counts ApproximationWe were also considering to first get an approximation of clique count of 0, after thatthen use the exact clique counting algorithm. The reasoning is that the approximationis much much faster than the exact one, which is a NPC problem. But it was just abrainstorm and it didn’t actually work well for us, as we found a even smarteralgorithm.Our Design and ImplementationThis section will cover all the design/implementation details in our final product.It is splitted into several sections for clarity.Clique CountingWe use the algorithm described here, whichcounts cliques using MapReduce. The idea is basically to split the problem of countinga $n-clique$ into a few $(n-1)-clique$ problems and suming them up. The sub-problemscan be splitted accordingly depending on how many cores/machines we have to do thecomputation. This is much faster than the dummy clique-counting algorithm, as itcheats by parallelizing the problem.Tabu SearchTabu searchis basically a greedy algorithm for flipping edges, but with an improvementof having a history, called tabu list, usually implemented by a linked list, so that it won’tsearch on the same thing twice. Having a history to recordwhat its moves are, and when it hits a local minimum, becasue it doesn’t take the stepsthat is used before, it’ll just “back-up” from the local minimum, i.e. acceptinga move that is worse because it has not choice, and start searching from there.Note that the tabu list might have some maximum length, due to 1. memory issue, and 2.we might want to revisite the same spot twice in the future.The original form of tabu search doesn’t work quite well in our case:  Our graph is large. To represent a 300 vertices graph needs at least $300x300$ bits matrix, and we might need to store hundreds of them. Memory is an big issue for usif we need a large tabu list.  Our algorithm will need to be distributed to make the maximum use of the compute powerwe have. So should we have a local tabu list for each worker, in which case the workerswill not collaborate well, or should we have a global tabu list, in which case memorybecomes a even larger issue for us. We could in fact use a database for global storage,but access data from a disk is too slow.We solve (we think) the above two issues by using another data structure for storinghistory, described below.Bloom FilterAs you all know, bloom filter is a super space efficient data structure with somefalse positive probability but no false negative, as it only stores bits insteadof the original data.  In other words in our case,  If the bloom filter says the step we take is not taken before, then we know thatfor sure (with probability of 1)  If it says the step is taken before, we know that it is the case for some probability $p$The important point here is that, the false positive rate $p$, along with the desirednumber of graphs we want to store, can be configured to meet our needs best. For example,in our case, we make the maximum number of graphs it can store be 40,000,000, and $p$be $1.0E^{-10}$, then we have the memory cost of $228.53MB$. Note that the expectednumber of false positive instnaces is 0.004, which means that even if we use upall the spaces of this bloom filter, we are extremely unlikely to have even oneinstance of false positive. Even in the case we have false positive, the assumptionhere is that the problem space is so large that it is ok if we miss a couple steps;there are plenty of other steps we can take to make it work.The Main TrickNot all men are equal, and not all edges are equally likely to be flipped. And moreover,we don’t actually need to count $10-cliques$ every time we flip an edge. Here is thetrick:  When move to larger problem size          Count 10 cliques once      Maintain a Map: {Edges} -&gt; {clique counts}        After the first and only 10-clique count check, we start to flip edges          During flipping, we also maintain the “complement” of the graph $G$, because:                  To check $R(m,n)$ is the same to check:                          $m-clique$ on $G$              $n-clique$ on $G^c$                                          When we test if flipping one edge works, we need to update the corresponding {Edges} -&gt; {clique counts} map. We get a list of intersected nodes of that edge and search for (n-2)-clique      This is super hard to describe in words for me, so here are the relavent exerted codes.void updateEdgeToClique(Edge edge) {    edgeToClique.remove(edge);    Edge flip = flip(edge);    Map&lt;Edge, Long&gt; minus = new HashMap&lt;&gt;();    Map&lt;Edge, Long&gt; plus = new HashMap&lt;&gt;();    Map&lt;Integer, Long&gt; nodesMinus = new HashMap&lt;&gt;();    Map&lt;Integer, Long&gt; nodesPlus = new HashMap&lt;&gt;();    long oldCount = countCliquesSub(edge, true, minus);    long newCount = countCliquesSub(flip, true, plus);    for(Map.Entry&lt;Edge, Long&gt; entry : minus.entrySet()) {        changeEdgeToClique(entry.getKey(), -entry.getValue());        nodesMinus.putIfAbsent(entry.getKey().node1, new Long(0));        nodesMinus.putIfAbsent(entry.getKey().node2, new Long(0));        nodesMinus.put(entry.getKey().node1, nodesMinus.get(entry.getKey().node1) + entry.getValue());        nodesMinus.put(entry.getKey().node2, nodesMinus.get(entry.getKey().node2) + entry.getValue());    }    for(Map.Entry&lt;Edge, Long&gt; entry : plus.entrySet()) {        changeEdgeToClique(entry.getKey(), entry.getValue());        nodesPlus.putIfAbsent(entry.getKey().node1, new Long(0));        nodesPlus.putIfAbsent(entry.getKey().node2, new Long(0));        nodesPlus.put(entry.getKey().node1, nodesPlus.get(entry.getKey().node1) + entry.getValue());        nodesPlus.put(entry.getKey().node2, nodesPlus.get(entry.getKey().node2) + entry.getValue());    }    for(Map.Entry&lt;Integer, Long&gt; entry : nodesMinus.entrySet()) {        Edge temp1 = new Edge(Math.min(edge.node1, entry.getKey()),                              Math.max(edge.node1, entry.getKey()));        Edge temp2 = new Edge(Math.min(edge.node2, entry.getKey()),                              Math.max(edge.node2, entry.getKey()));        changeEdgeToClique(temp1, -entry.getValue() / 7);        changeEdgeToClique(temp2, -entry.getValue() / 7);    }    for(Map.Entry&lt;Integer, Long&gt; entry : nodesPlus.entrySet()) {        Edge temp1 = new Edge(Math.min(flip.node1, entry.getKey()),                              Math.max(flip.node1, entry.getKey()));        Edge temp2 = new Edge(Math.min(flip.node2, entry.getKey()),                              Math.max(flip.node2, entry.getKey()));        changeEdgeToClique(temp1, entry.getValue() / 7);        changeEdgeToClique(temp2, entry.getValue() / 7);    }    if(newCount != 0) {        edgeToClique.put(flip, new AtomicLong(newCount));    }    current.set(current.get() + newCount - oldCount);}private long countCliquesSub(Edge edge, boolean change, Map&lt;Edge, Long&gt; edgeToClique) {    int currentSize = client.getCurrentSize();    List&lt;Integer&gt; intersect = getIntersectNodes(edge);    AdjListGraph g = new AdjListGraph();    g.edgeToClique = edgeToClique;    Collections.sort(intersect);    for(int i = 0; i &lt; intersect.size(); i++) {        for(int j = i + 1; j &lt; intersect.size(); j++) {            int node1 = intersect.get(i);            int node2 = intersect.get(j);            if(hasEdge(new Edge(node1, node2))) {                g.addEdge(Integer.toString(node1), Integer.toString(node2));            }        }    }    return g.countCliquesOfSize(8, change);}List&lt;Integer&gt; getIntersectNodes(Edge edge) {    List&lt;Integer&gt; intersect = new ArrayList&lt;Integer&gt;();    if(edge.node1 &gt;= currentSize) {        for(int i = currentSize; i &lt; currentSize * 2; i++) {            if(i == edge.node1 || i == edge.node2) {                continue;            }            Edge test1 = new Edge(Math.min(edge.node1, i), Math.max(edge.node1, i));            Edge test2 = new Edge(Math.min(edge.node2, i), Math.max(edge.node2, i));            if(hasEdge(test1) &amp;&amp; hasEdge(test2)) {                intersect.add(i);            }        }    } else {        for(int i = 0; i &lt; currentSize; i++) {            if(i == edge.node1 || i == edge.node2) {                continue;            }            Edge test1 = new Edge(Math.min(edge.node1, i), Math.max(edge.node1, i));            Edge test2 = new Edge(Math.min(edge.node2, i), Math.max(edge.node2, i));            if(hasEdge(test1) &amp;&amp; hasEdge(test2)) {                intersect.add(i);            }        }    }    return intersect;}void changeEdgeToClique(Edge edge, long delta) {    if(delta &lt;= 0) {        edgeToClique.get(edge).addAndGet(delta);        if(edgeToClique.get(edge).get() == 0) {            edgeToClique.remove(edge);        }    } else {        edgeToClique.putIfAbsent(edge, new AtomicLong());        edgeToClique.get(edge).addAndGet(delta);    }    }And here is a picture illustration:Again all codes are on Github and here is thepresentation.Server-Client ProtocolIn our implementation, this part is nothing really special but a bunch of TCP connections.The information exchanged includes:  Current graph size  Current graph  Current clique size  The {Edges} -&gt; {clique counts} map  Best clique so far.Basically each worker does thei own search until one of them finds a solution to currentgraph size, and then all of them move on to the next graph size. They are “always” doing searchesin different graphes because of the global bloom filter we have. The main servermight be down so we also have a backup server to take care of this.All in all, the client-server implementation is pretty basic in the sense that theyare just a bunch of TCP connections, no HTTP, no Raft consensus, no monitoring.Summary and Possible Improvements  It is better to use more up-to-date technologies :)          Opensource solution for server cluster with consensus by Raft/Paxos      Opensource solution for monitoring and visualized search progress        Some other tricks like we can skip to next prime graph size, which I heard is better.",
        "url"      : "/2018/07/06/In-Searching-Counter-Examples-for-Ramsey-Number/",
        "date"     : "2018-07-06 15:00:00 +0000"
    } ,
    
    {
        "title"    : "Python Global Interpreter Lock (GIL)",
        "category" : "Interpreter",
        "tags"     : "Interpreter, GIL",
        "content": "  Introduction  A Performance Experiment  About Python Threads  GIL Behavior          Singal Handling        GIL Implementation  Back to the Performance Experiment  Multicore GIL ContentionIntroductionIn CPython, the global interpreter lock, or GIL is a mutex that prevents multiple threads from executing Python bytecodes at once. The lock is necessary mainly because CPython’s memory management is not thread-safe.A Performance Experiment  0 import time  1 import threading  2 import multiprocessing  3  4 NUM = 10000000  5  6 def count(n):  7    while n &gt; 0:  8    ¦  n -= 1  9 10 t1 = threading.Thread(target=count, args=(NUM,)) 11 t2 = threading.Thread(target=count, args=(NUM,)) 12 start = time.time() 13 t1.start() 14 t2.start() 15 t1.join() 16 t2.join() 17 print \"multithread:\" 18 print time.time() - start 19 20 start = time.time() 21 count(NUM) 22 count(NUM) 23 print \"single thread:\" 24 print time.time() - start 25 26 p1 = multiprocessing.Process(target=count, args=(NUM,)) 27 p2 = multiprocessing.Process(target=count, args=(NUM,)) 28 start = time.time() 29 p1.start() 30 p2.start() 31 p1.join() 32 p2.join() 33 print \"multi process:\" 34 print time.time() - startHere’s the output:multithread:1.70929884911single thread:1.03298616409multi process:0.507339954376Why do I get those performance results?About Python ThreadsPython threads are real system threads (pthreads or Windows threads). They are fully managed by host OS including all scheduling/thread switching. Here’s what happens on thread creation  Python creates a small data structure containing some interpreter state  A new thread (pthread) is launched  The thread calls PyEval_CallObject, which is justa C function call that runs whatever Python callable was specifiedEach thread has its own interpreter specific data structure (PyThreadState):  Current stack frame (for python code)  Current recursion depth  Thread ID  Some per-thread exception information  Optional tracing/profiling/debugging hooks  It’s just a small C structThe interpreter has a global variable that simply points to the ThreadState struct of the currently running threadPyThreadState *_PyThreadState_Current = NULL;So that operations in the interpreter implicitly depend this variable to know what thread they’re currently working withGIL BehaviorThreads hold the GIL when running; however they release it when blocking for I/OBasically any time a thread is forced to wait other “ready” threads get their chance to run.But for CPU-bound threads that never perform any I/O, the interpreter periodically performs a “check”. By default, every 100 interpreter “ticks” it does a check.The check interval is a global counter that is completely independent of thread scheduling. See sys.setcheckinterval() and sys.getcheckintervalWhat happens during the periodic check?  In the main thread only, signal handlers will execute if there are any pending signals  Release and reacquire the GIL so that multiple CPU-bound threads get to run by briefly releasing the GIL, and other threads get a chance to run.A Tick loosely map to interpreter instruction(s)Noted that:  Ticks are not time-based  In fact long operations can block everything  ticks are uninterruptibleSingal HandlingA very common problem encountered with Python thread programming is that threaded programs can no longer be killed with the keyboard interrupt so you have to use kill -9 in a separate window. Why???If a signal arrives, the interpreter runs the “check” after every tick until the main thread runs. Since signal handlers can only run in the main thread, the interpreter quickly acquires/releases the GIL after every tick until it gets scheduled. Because Python has no control over scheduling so it just attempts to thread swithc as fast as possible with the hope that main will run.The reason Ctrl-C doesn’t work with threaded programs is that the main thread is opten blocked on an uninterruptible thread-join or lock. Since it’s blocked, it never gets scheduled to run any kind of signal handler for it. And as an extra bonus, the interpreter is left in a state where it tries to thread-switch after every tick, so not only can you not interrupt the program, it runs slow as hell!!!GIL ImplementationIt is either a POSIX semaphore or a pthreads condition variable. So all interpreter locking is based on signaling.Back to the Performance ExperimentAs we saw earlier CPU-bound threads have terrible performances, and two threads is even worse than single thread. The question is: what is the source of that overhead?Answer: Signaling: After every 100 ticks, the interpreter locks a mutex, signals on a conditional variable/semaphore where another thread is always waiting. And because another thread is waiting, extra pthreads processing and system calls get triggered to deliver the signal.What’s happening above is a battle between two competing and incompatible goals:  Python - only wants to run single-threaded but doesn’t want anything to do with thread scheduling (up to OS)  OS - “Oooh. Multiple cores.” Freely schedules processes/threads to take advantage of as many cores as possibleMulticore GIL ContentionEven 1 CPU-bound thread causes problems: it degrades response time of I/O-bound threadsThis scenario is a bizarre sort of “priority inversion” problem: A CPU-bound thread (low priority) is blocking the execution of an I/O-bound thread (high priority). It occurs because the I/O thread can’t wake up fast enough to acquire the GIL before the CPU-bound thread reacquires it. ANd it only happens on multicore…",
        "url"      : "/2018/06/28/Python-Global-Interpreter-Lock/",
        "date"     : "2018-06-28 14:30:00 +0000"
    } ,
    
    {
        "title"    : "Introduction to Category Theory",
        "category" : "Category Theory",
        "tags"     : "Category Theory, Math",
        "content": "  What is a Category ($C$)?  Structured Sets as Categories  Categories of Structured Sets  Categories of Types and Terms in Type Theory  Categories of Categories  Size of Categories  Representable FunctorWe can think of category theory as a generalized set theory, where in set theorywe have sets and $\\in$, but in category theory we have objects and arrows, wherearrows are just any kinds of mappings.So we have a kind of composition structure, where ther order of composition doesn’tmatter, but the configuration matters.And rather than reasoning structurally like PL does, it reasons “behaviorally”.What is a Category ($C$)?  Data          Object collections $C_0$, \\(A:C:=A\\in C_0\\)      Morphisms: Arrow collection \\(C_1\\) or $hom(C)$, \\(f::C:=f\\in C_1\\)                  We write \\(f:a\\rightarrow b\\) to say $f$ is a morphism from $a$ to $b$.          We write $hom_C(a,b)$ to denote all morphisms from $a$ to $b$, which is also called the hom-class of all morphisms from $a$ to $b$.          Note that $hom(C)$ is a collection of all $homs$ expanded.          Boundary maps. domain: \\(\\delta^-\\), codomain: \\(\\delta^+\\), \\(C(A\\rightarrow B):=\\{f::C|\\delta^-(f)=A\\wedge\\delta^+(f)=B\\}\\)          Identity morphism: \\(id(A):C(A\\rightarrow A)\\), which is also called loop endomorphism.          Composition: \\(for\\ f:A\\rightarrow B,g:B\\rightarrow C,we\\ have\\ f\\cdot g=g\\circ f:A\\rightarrow C\\)                      Composition laws          Unit laws: \\(for\\ f:A\\rightarrow B,we\\ have\\ id(A)\\cdot f=f=f\\cdot id(B)\\)      Associativity law: \\(for\\ f:A\\rightarrow B,g:B\\rightarrow C,h:C\\rightarrow D,we\\ have\\  (f\\cdot g)\\cdot h=f\\cdot(g\\cdot h)\\). From $A$ to $B$ to $C$ to $D$ we call it a path.      If \\(\\delta^-(f)=\\delta^-(g)\\&amp;\\&amp;\\delta^+(f)=\\delta^+(g)\\), we say the $f$ and $g$ arrows are parallel.        Diagram: If we treat objects as vertices and arrows as directed edges, we have a directed graph or diagram.          A comutative diagram is one such that all directed paths with the same starting and end points lead to the same result.      Whiskering: If we have a diagram that commutes, and we add one more arrow into it, we still have commuting diagram.      Pasting: If we two diagrams, both commute, and they have a common path, then we can “stick” those two diagrams along that path, and the resulting diagram still commutes.      Structured Sets as CategoriesWe can construct a category from a set.  Empty Category: $0$, which has no obejcts at all.  Singleton Category: $1$, which has only one object, and one morphism, which is the $id$for the object itself.  Discrete Category: For a set $S’$, we construct a category $S$, where the objects arejust the elements of the set, $S_0:=S’$, and the mophisms are only the $id$ for each object,\\(S_1:=\\{id(x)|x\\in S'\\}\\).      Preorder Category: For a preordered set ($P’,\\leq$) (a set with areflexive and transitive binary relation on it), a category $P$ with                  objects: $P_0:=P’$                    arrows:\\(P(x\\rightarrow y):=\\left\\{ \\begin{array}{rcl}\\{\"x\\leq y\"\\} &amp; \\mbox{if} &amp; x\\leq y \\\\ \\emptyset  &amp; \\mbox{otherwise} \\\\\\end{array}\\right.\\)                  identities: \\(id(x):=x\\leq x\\)          composition: \\(x\\leq y\\cdot y\\leq z:=x\\leq z\\)                            So the simplest category satisfying above requirements is call Interval Category ($I$),where there are only two objects, two $id$ rules, and one arrow from one to the other.              Monoid Category: For a monoid (\\(M',*,\\varepsilon\\))(a set $M’$, an associative binary operation $*$,and a unit for the operation $\\varepsilon$), the category $M$ with          objects: \\(M_0:=\\{\\star\\}\\)      arrows: \\(M(\\star\\rightarrow\\star):=M'\\)                  identity: \\(id(\\star):=\\varepsilon\\)          composition: \\(x\\cdot y:=x*y\\)                    An example monoid is (\\(\\mathbb{N},+,0\\))      Categories of Structured SetsSome math background: According to Russell’s Paradox,we cannot have a set of all sets, but we can have category of all sets. A classis a collection of sets (or other mathematical objects) that can be unambiguouslydefined by a property that all its members share. A class that is not a set iscalled a proper class, and a class that is a set is sometimes called a small class.  Category of sets $SET$, where objects are just sets and arrows are functions  Category of preordered sets $PREORD$, where objects are the preordered sets,and arrows are monotone maps (functions that preserve the order).  Category of monoids $MON$          objects: monoids      arrows: monoid homomorphisms (structure preserving maps of monoids)      \\[f:MON((M,*,\\varepsilon)\\rightarrow(N,*',\\varepsilon'))\\\\is\\\\f:SET(M\\rightarrow N)\\\\such\\ that\\\\f(x*y)=f(x)*'f(y),f(\\varepsilon)=\\varepsilon'\\]Categories of Types and Terms in Type Theory  objects: interpretations of types or typing context, \\([\\![A]\\!]\\) or \\([\\![\\Gamma]\\!]\\)  arrows: \\([\\![\\Gamma\\vdash M:A]\\!]:C([\\![\\Gamma]\\!]\\rightarrow[\\![A]\\!])\\)          identity: \\([\\![x:A\\vdash x:A]\\!]=id([\\![A]\\!])\\)      compositionn: \\([\\![x:A\\vdash [y/M]N:C]\\!]=[\\![x:A\\vdash M:B]\\!]\\cdot[\\![y:B\\vdash N:C]\\!]\\)        “baby tyep theory” simple example:\\[\\begin{align}          [\\![x:A\\vdash [x/x]M:C]\\!]          &amp;=[\\![x:A\\vdash x:A]\\!]\\cdot[\\![x:A\\vdash M:B]\\!]\\\\          &amp;=id([\\![A]\\!])\\cdot[\\![x:A\\vdash M:B]\\!]\\\\          &amp;=[\\![x:A\\vdash M:B]\\!]        \\end{align}\\]Categories of Categories  What is the morphism of categories? We define functor: For categories $C,D$,functor $F$ from $C$ to $D$ is:          a map \\(F_0:C_0\\rightarrow D_0\\)      a map \\(F_1:C_1\\rightarrow D_1\\) such that it                  respects boundaries: \\(for\\ f:C(A\\rightarrow B),we\\ have\\ F_1(f):D(F_0(A)\\rightarrow F_0(B))\\)          preserve identity morphisms: \\(F(id(x))=id(F(x))\\forall x\\in C_0\\)          preserve composition morphisms: \\(F(f\\cdot g)=F(f)\\cdot F(g)\\forall f:X\\rightarrow Y,g:Y\\rightarrow Z\\in C_1\\)          identity functors and functor composition are just as expected                    Size of CategoriesSome definitions:  a collection is either a $proper\\ class$, which is $large$, or a $set$, which is $small$.  $C\\ a\\ small\\ category\\ if\\ C_1\\ is\\ small$, meaning $C_1\\ is\\ a\\ set$(which implies that $C_0\\ is\\ small$ because of the identity rule: $C_1$ is atleast the same size of $C_0$)  $CAT$: Category of small categories, note: $CAT\\notin CAT_0$  A category is $locally\\ small$ means all homs are $small$: \\(\\forall a,b\\in C_0,hom(a,b)\\ is\\ small\\)Representable FunctorA representable functor is a functor of a special form that map a locally small categoryinto the category of sets, namely $SET$.For a category $C$, if we fix an object in category $C$, $X:C$, we can define afunctor denoted $F$ or $hom(X\\rightarrow -)$:\\[F:C\\rightarrow SET\\\\such\\ that\\\\F_0:=(A:C_0)\\rightarrow hom_C(X\\rightarrow A)\\\\F_1:=(f:C(A\\rightarrow B))\\rightarrow(C(X\\rightarrow f):hom_C(X\\rightarrow A)\\rightarrow hom_C(X\\rightarrow B))\\\\where\\\\C(X\\rightarrow f):=\\lambda a.a\\cdot f\\]$X$ is known as the representitive of the representable functor $F$.To proof $F$ is a functor, we need to proof:\\[C(X\\rightarrow id(A))=id(C(x\\rightarrow A))\\\\C(X\\rightarrow f\\cdot g)=C(X\\rightarrow f)\\cdot C(X\\rightarrow g)\\]Proofs are skipped ;)",
        "url"      : "/2018/02/24/Introduction-to-Category-Theory/",
        "date"     : "2018-02-24 21:45:00 +0000"
    } ,
    
    {
        "title"    : "Equational Reasoning",
        "category" : "Programming Language Theory",
        "tags"     : "Programming Languages",
        "content": "  Observational Equavalence  Logical Equivalence          Foundamental Theorm of Logical Relations      Closure under Converse Evalutaion      Relationship to Hereditary Termination      The main question is, how do we define two programs are equal, and how do we prove it.So for setup we have type $int$, and $k$ which is a $int$, and $e_1+e_2$.Observational EquavalenceWe say that 2 programs are equal $iff$ you can’t tell them apart, which means:  For 2 closed programs of type $int$, $e\\equiv e’\\ iff\\ \\exists k$ such that \\(e\\mapsto^*k,e'\\mapsto^*k\\).This is called Kleeve equivalence.  For \\(e,e':\\tau,e=^{obs}_\\tau e'\\ iff\\ \\forall o:\\tau\\vdash P:int,P[e]\\equiv P[e']\\),were $P$ is a program context, defined as an expression: \\(o:\\tau\\vdash P:int\\), where$o$ can said as a “hole”, and $P[e]$ means $[e/o]P$. For example, in $2+(o+1)$, $o$ is type $int$;in $2+(o7+1)$, $o$ is type $int\\rightarrow int$. To rephrase the rule, it means thattwo programs are equal if we plug them in a larger program $P$, we get the same result.This is called observational equivalence.Observational equivalence is characterized by a universal property:  Observatinoal equivalence is the coarsest consistent congruence.Consistent: \\(e\\sim_\\tau e'\\) is consistent $iff\\ e\\sim_{int}e’$ implies \\(e\\equiv e'\\).In other words, it means it implies $\\equiv$ at $int$.Congruence: if \\(e\\sim_{\\tau_1} e'\\), then \\(C[e]\\sim_{\\tau_2}C[e']\\) for anycontext, where context is \\(o:\\tau_1\\vdash C:\\tau_2\\).Coarsest: if \\(e\\sim_\\tau e'\\) by any consistent congruence, then \\(e=^{obs}_\\tau e'\\)Proof:  ConsistentWe want to show \\(if\\ e=^{obs}_{int}e',then\\ e\\equiv e'\\), which is obvious by the definitionof $=^{obs}$, just take $P$ to be $o$ itself. (identity)  CongruenceWe want to show \\(if\\ e=^{obs}_{\\tau_1} e'then\\ C[e]=^{obs}_{\\tau_2}C[e']\\).\\(e=^{obs}_{\\tau_1} e'\\) means \\(\\forall P_{o:\\tau_1},P[e]\\equiv P[e']\\).\\(C[e]=^{obs}_{\\tau_2}C[e']\\) means \\(\\forall P'_{o:\\tau_2},P'[C[e]]\\equiv P'[C[e']]\\).We take $P$ to be $P’[C/o]$, so $P[e]$ is just $P’[C[e]]$ (composition)  CoarsestBy congruence, we have \\(P[e]\\sim P[e']\\).By consistency, we have \\(P[e]\\equiv P[e']\\). So we are done.So we proved the properties of observational equivalence, by how do we proveobservational equivalence itself in the first place? Afterall, we need toshow the rule holds for all contexts. The idea is to cutdown contexts thatyou need to consider using types. We need to introduce a new notion below \\(=^{log}\\)to do that, so that we have a rule:\\[e=^{obs}_\\tau e'\\ iff\\ e^{log}_\\tau e'\\]Logical EquivalenceIt is equivalence defined by logical relations.  \\[e=^{log}_{int}e'\\ iff\\ e\\equiv e'\\]    \\[e=^{log}_{\\tau_1\\rightarrow \\tau_2}e'\\ iff\\ \\forall e_1,e_1':\\tau_1,if\\ e_1=^{log}_{\\tau_1}e_1',then\\ ee_1=^{log}_{\\tau_2}e'e_1'\\]  Thm:\\[e:\\tau\\ implies\\ e=^{obs}_{int}e\\]Proof \\(e=^{obs}_\\tau e'\\ iff\\ e^{log}_\\tau e'\\):From left to right should be easier because we are coming from $\\forall$ to $int$and $\\tau_1\\rightarrow \\tau_2$, so we are not gonna do it here.From right to left:Because observational equivalence is the coarsest consistent congruence, so weonly need to show logical equivalence is a consistent congruence. Consistencycomes for free, so we only need congruence.We want to proof: \\(\\forall o:\\tau_1\\vdash C:\\tau_2,if\\ e=^{log}_{\\tau_1}e',then\\ C[e]=^{log}_{\\tau_2}C[e']\\). We need a generalized version of $Thm(v3)$ fromhere:Foundamental Theorm of Logical Relations\\[If\\ \\Gamma\\vdash e:\\tau,and\\ \\gamma=^{log}_\\Gamma\\gamma',then\\ \\hat{\\gamma}(e)=^{log}_\\tau\\hat{\\gamma'}(e)\\]Closure under Converse EvalutaionLemma:\\[If\\ e\\mapsto e_0,e_0=^{log}_\\tau e',then\\ e=^{log}_\\tau e'\\]where \\(\\mapsto\\) is the transition in dynamics fromhere.Then we can proof it by the above therom and lemma, we will just skip it here ;)Relationship to Hereditary Termination\\[e=^{obs}_\\tau e\\ iff\\ HT_\\tau(e)\\]",
        "url"      : "/2018/02/24/Equational-Reasoning/",
        "date"     : "2018-02-24 17:25:00 +0000"
    } ,
    
    {
        "title"    : "Basic Computational Type Theory",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Family of Types          Some Notations      Functionality / Respect for Equality      $\\Pi$ and $\\Sigma$ Types        “Axiom” (Theorm) of Choice  Natural Numbers \\(\\mathbb{N}\\)  Identity Type          Therom[Martin-Lof]        Function Extensionality / Principle of Extensionality          Extensional Type Theory (ETT) / (Homotopy) Set Theory      Observational Type Theory      Family of TypesFamily of types is a generalization of the concept of a predicate/relation.Formally, given a type $A:U$ in a universe of types $U$, one may havea family of types$B:A\\rightarrow U$, which assigns to each term $a:A$ a type $B(a):U$. We say that thetype $B(a)$ varies with $a$.e.g.the following proposition\\[x:\\mathbb{N}\\vdash even(x)\\]is a predicate / propositional function / a family of types (indexed by \\(\\mathbb{N}\\)) / fibration.We can rewrite it as\\[\\{even(x)\\ prop/type\\}_{x:\\mathbb{N}}\\]The idea is that we are exhibiting a family of types/proofs in that if you give me anyparticular choice of number, it’s going to give me back a proposition, which may or maynotbe inhabitied, but they are all types.For example, \\(even(3)\\) will be uninhabited, and \\(even(2)\\) will be inhabitited.Some Notations\\[\\Gamma\\ ctx\\]means $\\Gamma$ being a context.\\[\\Gamma\\equiv\\Delta\\]means definitionally equivalent context.\\[\\Gamma\\vdash A\\ type\\]means we have a family of types named $A$ indexed by elements of $\\Gamma$\\[x:A\\vdash B_x\\ type\\]means we have a family of types named $B$ indexed by $x$, which are elements of type $A$(sometimes subscript $x$ might be omitted)\\[\\Gamma\\vdash A\\equiv B\\ type\\]means definitionally equivalent families of types.\\[\\Gamma\\vdash M:A\\]means we have an element of the type\\[\\Gamma\\vdash M\\equiv N:A\\]means definitionally equivalent elements of that typeFunctionality / Respect for EqualityHere is the basic idea:\\[x:A\\vdash B_x\\ type\\]\\[\\frac{M:A}{[M/x]B_x\\ type}\\]Some important points:\\[\\frac{\\Gamma\\vdash M:A,\\Gamma\\vdash A\\equiv B\\ type}{\\Gamma\\vdash M:B}\\]Functionality condition says more:\\[\\frac{x:A\\vdash B\\ type,M\\equiv N:A}{[M/x]B\\equiv[N/x]B\\ type}\\]In other words, if I give you definitionally equal instances, they they are going to bedefinitionally equal types.$\\Pi$ and $\\Sigma$ Types$\\Pi$ and $\\Sigma$ types are both families of types.$\\Pi$ types are a list of types, where each type is a function mapping from one type to another type.Formation of $\\Pi$ ($\\Pi-F$):\\[\\frac{\\Gamma\\vdash A\\ type\\ \\ \\ \\Gamma x:A\\vdash B\\ type}{\\Gamma\\vdash\\Pi_xA.B\\ type}\\]Introduction rule ($\\Pi-I$):\\[\\frac{\\Gamma x:A\\vdash M_x:B_x}{\\Gamma\\vdash (\\lambda_{x:A}.M_x):\\Pi_xA.B}\\]Elimination rule ($\\Pi-E$):\\[\\frac{\\Gamma\\vdash M:\\Pi_xA.B\\ \\ \\ \\Gamma\\vdash N:A}{\\Gamma\\vdash MN:[N/x]B}\\]$\\Pi$ Computation / Equivalence rule ($\\Pi-C$):\\[\\frac{\\Gamma\\vdash A\\ type\\ \\ \\ \\Gamma x:A\\vdash M_x:B\\ \\ \\ \\Gamma\\vdash N:A}{\\Gamma\\vdash(\\lambda_{x:A}.M_x)N\\equiv[N/x]M:[N/x]B}\\]$\\Sigma$ types are a list of indexed pair, where the first term is any type,and the second term is a function of the first term.Formation of $\\Sigma$ ($\\Sigma-F$):\\[\\frac{\\Gamma\\vdash A\\ type\\ \\ \\ \\Gamma x:A\\vdash B_x\\ type}{\\Gamma\\vdash\\Sigma_xA.B\\ type}\\]Introduction rule ($\\Sigma-I$):\\[\\frac{\\Gamma\\vdash M:A\\ \\ \\ \\Gamma\\vdash N:[M/x]B}{\\Gamma\\vdash&lt;M,N&gt;:\\Sigma_xA.B}\\]Elimination rule ($\\Sigma-E$):\\[\\frac{\\Gamma\\vdash M:\\Sigma_xA.B}{\\Gamma\\vdash\\Pi_1(M):A}\\]\\[\\frac{\\Gamma\\vdash M:\\Sigma_xA.B}{\\Gamma\\vdash\\Pi_2(M):[\\Pi_1(M)/x]B}\\]Equivalence rule:\\[\\Pi_1&lt;M,N&gt;\\equiv M\\]\\[\\Pi_2&lt;M,N&gt;\\equiv N\\]“Axiom” (Theorm) of ChoiceEvery total binary relation contains a funtion, where a total relation means:\\[\\forall x:A,\\exists y:B,s.t.\\ R(x,y)\\]So the axiom of choice in set theory, where $f$ is the choice function:\\[(\\forall x:A,\\exists y:B,R(x,y))\\supset\\exists f:A\\rightarrow B\\forall x:A,R(x,fx)\\]In type theory, it is actually a therom:\\[(\\Pi_xA.(\\Sigma_yB.R(x,y)))\\longrightarrow(\\Sigma_fA\\rightarrow B.(\\Pi_xA.R(x,fx)))\\]We can write the proof:\\[\\lambda(t:(\\Pi_xA.(\\Sigma_yB.R(x,y)))).&lt;\\lambda(x:A).\\Pi_1(tx),\\lambda(x:A).\\Pi_2(tx)&gt;\\]Explanation: first it’s clear that what we want to proof is a map from left to right.Let’s explain what’s the types left and right and what should be the instance of those types.Left: the outer most is a $\\Pi$ type of $A$ and something else, and we have $t$ the instance of left type.So $t$ must be a function mapping from $A$ to something. So we go one level deeper, it’s a $\\Sigma$ type.An instance of it must be a tuple, where the first term is type $B$, the second termis a mapping from $B$ to $R$. So $t$ is a mapping from $A$ to a tuple.Right: the outer most is a $\\Sigma$ type, so an instance of it is a tuple.The first term is a mapping from $A$ to $B$, the second term is an instance $\\Pi$ type.So it should be a function mapping from $A$ to $R$.Therefore, our outer most $\\lambda$ should return a tuple, and each term of it shouldbe a function mapping from $A$ to something. If we apply $t$ to $x$, we get a tuple, oran instance of $\\Sigma$ type. We can now apply elimination rules to it. $\\Pi_1$ wouldgive us type $B$, and $\\Pi_1$ would give us $[\\Pi_1(tx)/y]R(x,y)$, which is what we want.Natural Numbers \\(\\mathbb{N}\\)Introduction rules:(\\(\\mathbb{N}-I_0\\)):\\[\\frac{}{\\Gamma\\vdash 0:\\mathbb{N}}\\](\\(\\mathbb{N}-I_s\\)):\\[\\frac{\\Gamma\\vdash M:\\mathbb{N}}{\\Gamma\\vdash s(M):\\mathbb{N}}\\]Elimination rules:  Computational/non-dependent “Godel’s T”\\[\\frac{\\Gamma\\vdash M:\\mathbb{N},\\Gamma\\vdash C\\ type,\\Gamma\\vdash N_0:C,\\Gamma x:C\\vdash N_s:C}{\\Gamma\\vdash rec(M;N_0;x.N_s):C}\\]where $rec$ has the following definition:\\[rec(0;N_0;x.N_s)\\equiv N_0\\\\rec(s(M);N_0;x.N_s)\\equiv[rec(M;N_0;x.N_s)/x]N_s\\]  Proof by induction / dependent\\[\\frac{\\Gamma\\vdash M:\\mathbb{N},\\Gamma x:\\mathbb{N}\\vdash C\\ type,\\Gamma\\vdash N_0:[0/x]C,\\Gamma x:\\mathbb{N}\\ y:C\\vdash N_s:[s(x)/x]C}{\\Gamma\\vdash rec(M;N_0;x,y.N_s):[M/x]C}\\]or\\[\\frac{\\Gamma x:\\mathbb{N}\\vdash C\\ type,\\Gamma\\vdash N_0:[0/x]C,\\Gamma x:\\mathbb{N}\\ y:C\\vdash N_s:[s(x)/x]C}{\\Gamma\\ u:\\mathbb{N}\\vdash rec(u;N_0;x,y.N_s):[u/x]C}\\]where $rec$ has the following definition:\\[rec(0;N_0;x,y.N_s)\\equiv N_0\\\\rec(s(M);N_0;x,y.N_s)\\equiv[M,rec(M;N_0;x,y.N_s)/x,y]N_s\\]Identity TypeIdentity formation ($Id_A-F$):\\[\\frac{\\Gamma\\vdash A\\ type,\\Gamma\\vdash M:A,\\Gamma\\vdash N:A}{\\Gamma\\vdash Id_A(M,N)\\ type}\\]It is a type of proofs that $M$ and $N$ are equal(?) elements of type $A$.($Id_A-I$):\\[\\frac{\\Gamma\\vdash M:A}{\\Gamma\\vdash refl_A(M):Id_A(M,M)}\\]It is the least reflexive relation, meaning that that’s the only rule we have for identiy,$refl$ is the only thing we can have eventually.Elimination rule:\\[\\frac{\\Gamma\\vdash P:Id_A(M,N),\\Gamma x:A\\ y:A\\ z:Id_A(x,y)\\vdash C_{x,y,z}\\ type,\\Gamma x:A\\vdash Q:[x,x,refl(x)/x,y,z]C}{\\Gamma\\vdash J_{x,y,z.C}(P;x.Q):[M,N,P/x,y,z]C}\\]Side Note: \\(\\Gamma x:A,y:A,z:Id_A(x,y)\\vdash C_{x,y,z}\\ type\\) and any others that“construct” the “$C\\ type$” like things is also called motive.Computation rule:\\[J(refl(M);x.Q)\\equiv[M/x]Q\\]  Equality is Symmetricdefine $sym$ such that ($sym(x)$ can also be written as $x^{-1}$):\\[x:Id_A(M,N)\\vdash sym(x):Id_A(N,M)\\]Proof:We use the motive: \\(u,v,\\_,Id_A(v,u)\\) for $J$, corresponding to $x,y,z,C$ in the above elimination rule motive:\\[y:A\\vdash refl_A(y):Id_A(y,y)\\\\sym(x):=J(x;y.refl_A(y)):[M,N/u,v]Id_A(v,u)=Id_A(N,M)\\]  Equality is Transitivefind \\(trans(x,y)\\) such that \\(x:Id_A(M,N),y:Id_A(N,P)\\vdash trans(x,y):Id_A(M,P)\\)where $trans(x,y)$ is just like composition $y\\cdot x$.Proof:We use the motive: \\(u,v,\\_,Id_A(v,P)\\rightarrow Id_A(u,P)\\) for $J$,corresponding to $x,y,z,C$ in the above elimination rule motive:\\[z:A\\vdash Q:Id_A(v,P)\\rightarrow Id_A(u,P)\\\\J(x;Q):[M,N/u,v]Id_A(v,P)\\rightarrow Id_A(u,P)=Id_A(N,P)\\rightarrow Id_A(M,P)\\\\then\\\\J(x;Q)(y):Id_A(M,P)\\]  Substitutivity/Functionality/Transport\\[\\frac{\\Gamma x:A\\vdash B\\ type,\\Gamma\\vdash P:Id_A(M,N),\\Gamma\\vdash Q:[M/x]B}{\\Gamma\\vdash subst(P,Q):[N/x]B}\\]Proof:We use the motive: \\(u,v,\\_,[u/x]B\\rightarrow[v/x]B\\) for $J$,corresponding to $x,y,z,C$ in the above elimination rule motive:  Respect\\[\\frac{\\Gamma x:A\\vdash Q:B,\\Gamma\\vdash P:Id_A(M,N)}{\\Gamma\\vdash resp(Q;P):Id_B([M/x]Q,[N/x]Q)}\\]Therom[Martin-Lof]If $P:Id_A(M,N)$ (closed), then $M\\equiv N:A$ (definitionally). Which means $Id_A$internalizes definitional equality.Fact(1): There is a $P$ such that \\(x:\\mathbb{N},y:\\mathbb{N}\\vdash P:Id_{\\mathbb{N}}(x+y,y+x)\\)But! The type \\(Id_{\\mathbb{N}\\rightarrow\\mathbb{N}\\rightarrow\\mathbb{N}}(\\lambda_x\\lambda_y.x+y,\\lambda_x\\lambda_y.y+x)\\)has no proof (not inhabited).Function Extensionality / Principle of Extensionality\\[\\frac{x:A\\vdash P_x:Id_{B_x}(Mx,Nx)}{\\vdash ext(P):Id_{\\Pi_xA.B}(M,N)}\\]It is not derivable in Intentional Type Theory (ITT). But it is inter-derivable from:$\\eta$:\\[\\frac{\\Gamma\\vdash M:\\Pi_xA.B}{\\Gamma\\vdash\\eta:Id_{\\Pi_xA.B}(M,\\lambda_x.A:M_x)}\\]$\\xi$ (weak extensionality):\\[\\frac{\\Gamma x:A\\vdash P:Id_B(M,N)}{\\Gamma\\vdash\\xi(P):Id_{\\Pi_xA.B}(\\lambda_x.M,\\lambda_x.N)}\\]So it is not derivable in the type theory we developed before, so what do we do about it?There are a few options. One is to justify that there is another form of type theory,so that we can derive the above rule from the new form of type theory.One is called Extensional Type Theory (ETT) or (Homotopy) Set Theory.Extensional Type Theory (ETT) / (Homotopy) Set TheoryWe replace $Id-E$ with the following rules:reflection:\\[\\frac{\\Gamma\\vdash P:Id_A(M,N)}{\\Gamma\\vdash M=N:A}\\]Note: it is judgemental equlity, not definitional equality.uniqueness of identity proofs (UIP) / Discreteness:\\[\\frac{\\Gamma\\vdash P:Id_A(M,N)}{\\Gamma\\vdash P=refl(M):Id_A(M,N)}\\]In ETT, Fact(1) means that: $x,y:\\mathbb{N}\\vdash x+y=y+x:\\mathbb{N}$, and so it follows as a corollary that:$\\lambda_x\\lambda_y.x+y=\\lambda_x\\lambda_y.y+x:\\mathbb{N}\\rightarrow\\mathbb{N}\\rightarrow\\mathbb{N}$,then we will have a proof $refl:Id_{\\mathbb{N}\\rightarrow\\mathbb{N}\\rightarrow\\mathbb{N}}(…,…)$In ETT, if we think of type $A$ as a space, it is discrete in the sense that theonly paths of elements of $A$ are the paths between elements and themselves. There isno path between two different points. We say that as a Homotopically discrete space, whichis called a set, which is totally different from mathematic’s set theory.Besides ETT, we can justify another form of type theory, called Observational Type Theory (OTT).Observational Type TheoryIt is another attempt at dealing with the idea that types are homotopically discrete.Define: \\(Id_A\\) by induction on the structure of $A$:\\[Id_{\\Pi_xA.B}(M,N):=\\Pi_{x:A}Id_B(Mx,Nx)\\\\refl_{\\Pi_xA.B}(M):=\\lambda_{x:A}.refl_B(Mx)\\]So that the function extensionality can be proof here in OTT",
        "url"      : "/2018/02/20/Basic-Computational-Type-Theory/",
        "date"     : "2018-02-20 20:00:00 +0000"
    } ,
    
    {
        "title"    : "Type Theory Foundations",
        "category" : "Programming Language Theory, Type Theory",
        "tags"     : "Programming Languages, Type Theory",
        "content": "  Intuitionistic Logic/Constructive Logic          Sythetic Judgement vs Analytic Judgement      Equivalence of Proofs      Negation (\\(\\urcorner\\)) in Heyting Algebra      We can think of Type Theory as being a catalog of a variety of notions of computation.The type structure determines the “programming language features”. For example,whether you have higher order functions amounts to saying “do you have exponential types”;whether you have structs or tuples amounts to saying “do you have Cartesian product types”;whether you have choices or multiple classes of data corresponds to“whether you have sum types”. A programming language is really just a collection of types.So type theory is just a theory of construction. From that point of view, logic isjust an application of type theory, because there are particular constructions whichcorrespond to proofs. Other constructions like natural numbers or geometric objectsdon’t correspond to proofs of particular propositions. They are just mathematicalconstructions. What type theory is intersted in is the general concept of what isa mathematical construction. That’s why intuitionistic logic is also called constructivelogic. From this point of view, we can say that logic is just a corner of mathematics,and mathematics is just a corner of computer science ;)Intuitionistic Logic/Constructive Logic“Logic as if people matters”. We are talking about communication of knowledge.We treat proofs as mathematical objects, or programs. We claimthat $A$ is true, we actually mean that we have a proof of $A$. $M:A$ means that$M$ is a proof of $A$, or $M$ is of type $A$, they are the same thing. There aremany strong connections among proof theory, type theory, and category theory.Some connections between Intuitionisitc Propositional logic:proof theory ($\\vdash$) and Heyting Algebra:category theory(Cartesian closed pre-order, which follows Reflexivity and Transitivity) ($\\leq$)            \\(A\\ true\\vdash T\\ true\\)      \\(A\\leq T\\)              \\(A\\wedge B\\ true\\vdash A\\ true\\),\\(A\\wedge B\\ true\\vdash B\\ true\\)      \\(A\\wedge B\\leq A\\),\\(A\\wedge B\\leq B\\)              \\(\\frac{C\\ true\\vdash A\\ true,C\\ true\\vdash B\\ true}{C\\ true\\vdash A\\wedge B\\ true}\\)      \\(\\frac{C\\leq A,C\\leq B}{C\\leq A\\wedge B}\\)              \\(\\perp\\ true\\vdash A\\ true\\)      \\(I\\leq A\\)              \\(A\\ true\\vdash A\\vee B\\ true\\),\\(A\\ true\\vdash A\\vee B\\ true\\)      \\(A\\leq A\\vee B\\),\\(B\\leq A\\vee B\\)              \\(\\frac{A\\ true\\vdash C\\ true,B\\ true\\vdash C\\ true}{A\\vee B\\ true\\vdash C\\ true}\\)      \\(\\frac{A\\leq C,B\\leq C}{A\\vee B\\leq C}\\)              Introduction rule: \\(A\\ true,A\\supset B\\ true\\vdash B\\ true\\) Elimination rule: \\(\\frac{C\\ true,A\\ true\\vdash B\\ true}{C\\ true\\vdash A\\supset B\\ true}\\)      \\(A\\wedge B\\leq C\\ iff\\ A\\leq(B\\supset C)\\) or \\(A\\wedge B\\leq C\\ iff\\ A\\leq C^B\\)      Sythetic Judgement vs Analytic JudgementFor synthetic judgement, we have something like “$A$ is true”, which requires a proof.It means that we need to do some proof searching.For analytic judgement, we have “$M:A$”, which gives me a proof of $A$ which is $M$.All we need to do is to check whether $M$ is actually a proof of $A$, which is mucheasier than searching a proof. It is also called self-evident.Equivalence of Proofs      Definitional Equality (analytic judgement): equality of sense\\(\\Gamma\\vdash M\\equiv N:A\\)        Denotational Equality (synthetic judgement): equality of reference\\(\\Gamma\\vdash M=N:A\\)        (Homotopy) Equivalence (synthetic judgement)\\(\\Gamma\\vdash\\alpha:M\\cong N:A\\), where $\\alpha$ is an evidence of equivalence.  Example to distinguish 1 and 2:Define addition as follows:\\[a+0\\equiv a\\\\a+succ(b)\\equiv succ(a+b)\\]Then we have the following equalities:\\[true:2+2\\equiv4\\\\true:x\\in\\mathbb{N}\\vdash x+0\\equiv x\\\\false:x\\in\\mathbb{N}\\vdash 0+x\\equiv x\\\\true:x\\in\\mathbb{N}\\vdash 0+x=x\\\\true:x\\in\\mathbb{N}\\vdash succ(x)\\equiv x+1\\\\false:x\\in\\mathbb{N}\\vdash succ(x)\\equiv 1+x\\\\true:x\\in\\mathbb{N}\\vdash succ(x)=1+x\\\\true:x,y\\in\\mathbb{N}\\vdash x+y=y+x\\\\\\]For Denotational Equality, we are suppressing the trivial evidence which is alwaysreflexivity/identity. But for Equivalence, $\\alpha$ can be reflexivity but can also besomething else.Negation (\\(\\urcorner\\)) in Heyting AlgebraIntroduction rule:\\[\\frac{C\\vee A\\leq\\perp}{C\\leq\\urcorner A}\\]Elimination rule:\\[A\\vee\\urcorner A\\leq\\perp\\]Note: negation in Heyting algebra is not complement!!!Namely, we don’t have the following:\\[T\\leq A\\vee\\urcorner A\\]or in logic, we don’t expect\\[A\\vee\\urcorner A\\ true\\]Boolean Algebra is Heyting Algebra with negation equals complement, which is also called law of excluded middle.We can define in Heyting Algebra:\\[A\\ decidable\\ iff\\ (A\\vee\\urcorner A)\\ true\\]It just means that there are propositions that we neither have a proof nor refutation. Example: $P=NP$.Note: Failing to affirm the decidability of every proposition is not the same as refuting the decidability of every proposition.We have the following theorm in intuitionistic logic:\\[\\urcorner(\\urcorner(A\\vee\\urcorner A))\\]To put it in human language, it says: intuitionistic logic does not refute the law of excluded middle. It does not affirm it, but it does not refute it also.The importance is: we can always heuristically assume a proposition is decidable even if we don’t have a proof. The apparent limitations of intuitionistic logic are the very source of its strength/expressiveness.So far we have some elementry constructions:\\[0,1,A\\times B,A+B,A\\rightarrow B\\]or to write it in logic\\[\\perp,T,A\\wedge B,A\\vee B, A\\supset B\\]",
        "url"      : "/2018/02/19/Type-Theory-Foundations/",
        "date"     : "2018-02-19 23:00:00 +0000"
    } ,
    
    {
        "title"    : "Sequent Calculus",
        "category" : "Programming Language Theory, Proof Theory",
        "tags"     : "Programming Languages, Proof Theory, Logic, Philosophy",
        "content": "  Verifications          Conjunctions      Implications      Disjunctions        Sequents          Conjunctions      Implications      Disjunctions      Falsehood      Verifications\\(A\\uparrow\\)means $A$ has a verification.\\(A\\downarrow\\)means $A$ ay be usedA conversion rule ($\\downarrow\\uparrow$):\\[\\frac{P\\downarrow}{P\\uparrow}\\]where $P$ is atomicConjunctionsIntroduction rule ($I$):\\[\\frac{A\\uparrow,B\\uparrow}{A\\wedge B\\uparrow}\\]Elimination rules ($E$):\\[\\frac{A\\wedge B\\downarrow}{A\\downarrow}\\]\\[\\frac{A\\wedge B\\downarrow}{B\\downarrow}\\]ImplicationsIntroduction rule ($I^x$):\\[\\frac{\\overline{A\\downarrow}^x\\ B\\uparrow}{A\\supset B\\uparrow}\\]Elimination rules ($E$):\\[\\frac{A\\supset B\\downarrow A\\uparrow}{B\\downarrow}\\]DisjunctionsIntroduction rule ($I$):\\[\\frac{A\\uparrow}{A\\vee B\\uparrow}\\]\\[\\frac{B\\uparrow}{A\\vee B\\uparrow}\\]Elimination rules ($E$):\\[\\frac{A\\vee B\\downarrow,\\overline{A\\downarrow}^x\\ C\\uparrow,\\overline{B\\downarrow}^y\\ C\\uparrow}{C\\uparrow}\\]Note: We can’t use elimination right after introduction, becasue the arrows don’t matchSequentsSo for natural deduction, we have elimination rules and introduction rules.For sequent calculus, we have left rules and right rules, where left rules arejust the inverse of the elimination rules, and right rules are just the same as introduction rules.The reasoning behind it is that elimination rules always produce down arrows, and introduction rulesalways produce up arrows. If we use natural deduction, we will kind of need to work on twodirections, and when they meet, we stop and use the conversion rule ($\\downarrow\\uparrow$).To simplify things, we reverse the elimination rules so that it is also pointing upward,so that we will only need to work in one direction. And we stop at something called identity rule.Basically everything with $uparrow$ should be on the right of $\\Rightarrow$, and vice versa.We use the following notation:\\[B_1,\\ldots,B_n\\Rightarrow A\\]Identity rule:\\[\\frac{}{P,P\\Rightarrow P}\\]ConjunctionsRight rules:\\[\\frac{\\Gamma\\Rightarrow A,\\Gamma\\Rightarrow b}{\\Gamma\\Rightarrow A\\wedge B}\\]Left rules:\\[\\frac{\\Gamma,A\\wedge B,A\\Rightarrow C}{\\Gamma,A\\wedge B\\Rightarrow C}\\]\\[\\frac{\\Gamma,A\\wedge B,C\\Rightarrow C}{\\Gamma,A\\wedge B\\Rightarrow C}\\]ImplicationsRight rules:\\[\\frac{\\Gamma,A\\Rightarrow B}{\\Gamma\\Rightarrow A\\supset B}\\]Left rules:\\[\\frac{\\Gamma,A\\supset B\\Rightarrow A\\ \\ \\ \\Gamma,A\\supset B,B\\Rightarrow C}{\\Gamma,A\\supset B\\Rightarrow C}\\]DisjunctionsRight rules:\\[\\frac{\\Gamma\\Rightarrow A}{\\Gamma\\Rightarrow A\\vee B}\\]\\[\\frac{\\Gamma\\Rightarrow B}{\\Gamma\\Rightarrow A\\vee B}\\]Left rules:\\[\\frac{\\Gamma,A\\vee B,A\\Rightarrow C\\ \\ \\ \\Gamma,A\\vee B,B\\Rightarrow C}{\\Gamma,A\\vee B\\Rightarrow C}\\]FalsehoodRight rules:\\[no\\ \\perp R\\]Left rules:\\[\\frac{}{\\Gamma,\\perp\\Rightarrow C}\\]",
        "url"      : "/2018/02/18/Sequent-Calculus/",
        "date"     : "2018-02-18 19:30:00 +0000"
    } ,
    
    {
        "title"    : "Computational Interpretations",
        "category" : "Programming Language Theory, Proof Theory",
        "tags"     : "Programming Languages, Proof Theory, Logic, Philosophy",
        "content": "  Lax and Monand  Let’s Write Some Proofs/ProgramsHere we will talk about computational interpretations by the example of lax logic.Hope from the example we can have sense of how logic and PL are connected.Lax and MonandSome notations:\\(\\Gamma\\vdash E:A\\ lax\\)means $E$ as a computational evidence might not terminate. It might not give me $A$ at the end. Think of it like a “possibility” that $E$ will give me $A$.\\(\\{E\\}\\)a suspended computation of $E$.\\(&lt;E/x&gt;F\\)is a kind of substitution of $E$ for $x$ in $F$. It is an operration on proof/computation. Will talk about it later.So we have the following structural rules or judgemental rules about $lax$:1.\\[\\frac{\\Gamma\\vdash M:A\\ true}{\\Gamma\\vdash M:A\\ lax}\\]2.\\[\\frac{\\Gamma\\vdash E:A\\ lax,\\Gamma x:A\\ true\\vdash F:C\\ lax}{\\Gamma\\vdash&lt;E/x&gt;F:C\\ lax}\\]We now introduce a new proposition $\\bigcirc A$ Monad:Introduction rule ($I$):\\[\\frac{\\Gamma\\vdash E:A\\ lax}{\\Gamma\\vdash \\{E\\}:\\bigcirc A\\ true}\\]Elimination rules ($E$):\\[\\frac{\\Gamma\\vdash M:\\bigcirc A\\ true,\\Gamma x:A\\ true\\vdash F:C\\ lax}{\\Gamma\\vdash \\underline{let} \\{x\\}=M\\underline{in} F:C\\ lax}\\]Summarize local reduction and local expansion:\\[\\underline{let} \\{x\\}=\\{E\\}\\underline{in} F\\Longrightarrow_R&lt;E/x&gt;F\\]\\[M:\\bigcirc A\\Longrightarrow_E\\{\\underline{let}\\{x\\} =M\\underline{in} x\\}\\]So because $E$ is type of $A\\ lax$, according to the second structural rules about $lax$:\\(\\frac{\\Gamma\\vdash E:A\\ lax,\\Gamma x:A\\ true\\vdash F:C\\ lax}{\\Gamma\\vdash&lt;E/x&gt;F:C\\ lax}\\),it could be comming from two places:      The first structural rules about $lax$:\\(\\frac{\\Gamma\\vdash M:A\\ true}{\\Gamma\\vdash M:A\\ lax}\\).So $E$ is $M$.        The elimination rules of $\\bigcirc A$:\\(\\frac{\\Gamma\\vdash M:\\bigcirc A\\ true,\\Gamma x:A\\ true\\vdash F:C\\ lax}{\\Gamma\\vdash \\underline{let} \\{x\\}=M\\underline{in} F:C\\ lax}\\). (Note: Here $F$ and $C$ could be anything).So $E$ is \\(\\underline{let} \\{x\\}=M\\underline{in} F\\)  So we write:\\(E=M|\\underline{let} \\{x\\}=M\\underline{in} F\\)For the elimination rule case, we can see $\\Gamma\\vdash M:\\bigcirc A\\ true$.For something have type $\\bigcirc A$, according to the Introduction rule of $\\bigcirc A$,it could be that $M={E}$Therefore, $M$ is everything as before plus one possibility:We write:\\(M=\\ldots\\{E\\}\\)Then we can define $&lt;E/x&gt;F$:\\[&lt;E/x&gt;F=\\\\&lt;M/x&gt;F=[M/x]F\\\\or\\\\&lt;\\underline{let} \\{y\\}=M\\underline{in} E'/x&gt;F=\\underline{let} \\{y\\}=M\\underline{in} &lt;E'/x&gt;F\\]Let’s Write Some Proofs/ProgramsFor\\((A\\supset(B\\supset C))\\supset((A\\wedge B)\\supset C)\\)which is uncurrying, we have the proof:\\(\\lambda f.\\lambda p.f(first\\ p)(second\\ p)\\)For\\(((A\\wedge B)\\supset C)\\supset (A\\supset (B\\supset C))\\)which is currying, we have the proof:\\(\\lambda g.\\lambda x.\\lambda y.g&lt;x,y&gt;\\)For monad in functional programming we have two requirements to satisfy:\\[return:A\\supset\\bigcirc A\\\\bind:\\bigcirc A\\supset(A\\supset \\bigcirc B)\\supset \\bigcirc B\\]such that\\[bind(return\\ z)f=fz\\]To proof $return$:\\[A\\supset \\bigcirc A\\]we have:\\[\\lambda x.\\{x\\}\\]To proof $bind$:\\[\\bigcirc A\\supset (A\\supset \\bigcirc B)\\supset \\bigcirc B\\]we have:\\[\\lambda x.\\lambda f.\\{\\underline{let}\\{x'\\}=x\\underline{in}\\ \\underline{let}\\{y\\}=fx'\\underline{in}\\ y\\}\\]To proof\\[bind(return\\ x)f=fx\\]we have:\\[\\begin{align}(\\lambda x.\\lambda f.\\{\\underline{let}\\{x'\\}=x\\ \\underline{in}\\ \\underline{let}\\{y\\}=fx'\\ \\underline{in}\\ y\\})((\\lambda x.\\{x\\})z)f\\\\&amp; = \\{\\underline{let}\\{x'\\}=(\\lambda x.\\{x\\})z\\ \\underline{in}\\ \\underline{let}\\{y\\}=fx'\\ \\underline{in}\\ y\\}\\\\&amp; = \\{\\underline{let}\\{x'\\}=\\{z\\}\\ \\underline{in}\\ \\underline{let}\\{y\\}=fx'\\ \\underline{in}\\ y\\}\\\\&amp; = \\{\\underline{let}\\{y\\}=fz\\ \\underline{in}\\ y\\}\\\\&amp; = fz\\end{align}\\]",
        "url"      : "/2018/02/18/Computational-Interpretation/",
        "date"     : "2018-02-18 13:30:00 +0000"
    } ,
    
    {
        "title"    : "Judgements and Propositions",
        "category" : "Programming Language Theory, Proof Theory",
        "tags"     : "Programming Languages, Proof Theory, Logic, Philosophy",
        "content": "  Defining a Judgement          Examples                  To define $A\\wedge B\\ true$:          To define implication($\\supset$):                    We state “A is true”, then “A” is a proposition, and “A is true” as a whole is a judgement.Some examples of application of logic branches in programming languagues:            K knows A      epistemic logic      distributed computing              A true at time t      temporal logic      partitial evaluation              A is a resource      linear logic      concurrent computing              A is possible      lax logic      monad              A is valid      modal logic      runtime code generation      Defining a JudgementTo define a judgement, we must have Introduction rules ($I$), and Elimination rules ($E$).They must satisfy Local Soundness (checks elimination rules are not too strong) such thatfor every way to apply the elimination rules, we can reduce it to one that already existed.The process of doing that is called local reduction. ($\\beta$ rule)They should also satisfy Local Completeness (checks elimination rules are not too weak) such thatthere is someway to apply the elimination rules so that from the pieces we canre-introduce what the original proposition is. The process of doing that is called local expansion. ($\\eta$ rule)Note: $MN$ means $M$ applies to $N$Some notations:\\(\\Gamma\\vdash M:A\\)means $M$ is a proof of $A$ is $true$, or $M$ is a program of type $A$, where\\(\\Gamma:=\\cdot|\\Gamma'x:A\\)\\([N/x]M\\)means substitude $N$ for $x$ based on the structure of $M$ (plug in $N$ for $x$)ExamplesTo define $A\\wedge B\\ true$:Introduction rule ($I$):\\[\\frac{A\\ true,B\\ true}{A\\wedge B\\ true}\\]or\\[\\frac{\\Gamma\\vdash M:A,\\Gamma\\vdash N:B}{\\Gamma\\vdash &lt;M,N&gt;:A\\wedge B}\\]Elimination rules ($E$):\\[\\frac{A\\wedge B\\ true}{A\\ true}\\]or\\[\\frac{\\Gamma\\vdash M:A\\wedge B}{\\Gamma\\vdash first\\ M:A}\\]\\[\\frac{A\\wedge B\\ true}{B\\ true}\\]or\\[\\frac{\\Gamma\\vdash M:A\\wedge B}{\\Gamma\\vdash second\\ M:B}\\]Check Local Soundness by local reduction:\\[\\frac{A\\ true,B\\ true}{\\frac{A\\wedge B\\ true}{A\\ true}\\Longrightarrow_R A\\ true}\\]or\\[first&lt;M,N&gt;\\Longrightarrow_RM\\]\\[\\frac{A\\ true,B\\ true}{\\frac{A\\wedge B\\ true}{B\\ true}\\Longrightarrow_R B\\ true}\\]or\\[second&lt;M,N&gt;\\Longrightarrow_RN\\]Check Local Completeness by local expansion:\\[A\\wedge B\\ true\\Longrightarrow_E \\frac{\\frac{A\\wedge B\\ true}{A\\ true}\\ \\frac{A\\wedge B\\ true}{B\\ true}}{A\\wedge B\\ true}\\]or\\[M:A\\wedge B\\Longrightarrow_E&lt;first\\ M,second\\ M&gt;\\]To define implication($\\supset$):Introduction rule($I^x$) (x means an assumption):\\[\\frac{\\overline{A\\ true}^x\\ B\\ true}{A\\supset B\\ true}\\]or\\[\\frac{\\Gamma x:A\\vdash M:B}{\\vdash\\lambda x.M:A\\supset B}\\]Elimination rule($E$):\\[\\frac{A\\supset B\\ true,A\\ true}{B\\ true}\\]or\\[\\frac{\\Gamma\\vdash M:A\\supset B,\\Gamma\\vdash N:A}{\\Gamma\\vdash MN:B}\\]Local reduction:\\[(\\lambda x.M)N\\Longrightarrow_R[N/x]M\\]Local expansion:\\[M:A\\supset B\\Longrightarrow_E\\lambda x.Mx\\]",
        "url"      : "/2018/02/18/Judgements-and-Propositions/",
        "date"     : "2018-02-18 09:30:00 +0000"
    } ,
    
    {
        "title"    : "Sum and Product Types",
        "category" : "Programming Language Theory",
        "tags"     : "Programming Languages",
        "content": "  Products  Sums  Some Type AlgebraProductsFor products, we have types $\\tau$:  Binary\\(\\tau_1\\times\\tau_2\\)  Nullary\\(1\\ or\\ unit\\)and expressions $e$:      Ordered pairs\\(\\langle e_1,e_2\\rangle\\)        Projections\\(e\\cdot1 \\\\e\\cdot2\\)  Statics:\\[\\frac{}{\\Gamma\\vdash\\langle\\rangle:1}\\]\\[\\frac{ \\Gamma\\vdash e_1:\\tau_1,\\Gamma\\vdash e_2:\\tau_2 }{ \\Gamma\\vdash\\langle e_1,e_2\\rangle:\\tau_1\\times\\tau_2}\\]\\[\\frac{ \\Gamma\\vdash e:\\tau_1\\times\\tau_2 }{ \\Gamma\\vdash e\\cdot1:\\tau_1,e\\cdot2:\\tau_2}\\]Dynamics (two cases, lazy or eager):lazy:\\[\\frac{}{\\langle e_1,e_2\\rangle val}\\]end lazyeager:\\[\\frac{e_1\\mapsto e_1'}{\\langle e_1,e_2\\rangle\\mapsto \\langle e_1',e_2\\rangle}\\]\\[\\frac{e_1\\ val,e_2\\mapsto e_2'}{\\langle e_1,e_2\\rangle\\mapsto \\langle e_1,e_2'\\rangle}\\]\\[\\frac{e_1\\ val,e_2\\ val}{\\langle e_1,e_2\\rangle val}\\]end eager\\[\\frac{e\\mapsto e'}{e\\cdot2\\mapsto e'\\cdot2}\\]\\[\\frac{\\langle e_1,e_2\\rangle val}{\\langle e_1,e_2\\rangle\\cdot i\\mapsto e_i}\\]SumsFor sums, we have types $\\tau$:  Binary Sum/Coproducts\\(\\tau_1+\\tau_2\\)  Nullary\\(0\\ or\\ void\\)and expressions $e$:  Binary\\(1\\cdot e\\\\2\\cdot e\\)\\[\\underline{case}_\\tau e\\{1\\cdot x\\hookrightarrow e_1 | 2\\cdot x\\hookrightarrow e_2\\}\\\\or\\\\\\underline{case} [\\tau](x.e_1;x.e_2)(e)\\]  Nullary\\[\\underline{case} \\{\\}\\\\or\\\\abort[\\tau]()\\]Note: $abort$ doesn’t mean to abort!!!!!Statics:\\[\\frac{\\Gamma\\vdash e_i:\\tau_i}{\\Gamma\\vdash i\\cdot e_i:\\tau_1+\\tau_2}\\]\\[\\frac{\\Gamma\\vdash e:\\tau_1+\\tau\\ and\\ \\Gamma,x:\\tau_1\\vdash e_1:\\tau\\ \\Gamma,x:\\tau_2\\vdash e_2:\\tau}{\\Gamma\\vdash\\underline{case}_\\tau e\\{1\\cdot x\\hookrightarrow e_1 | 2\\cdot x\\hookrightarrow e_2\\}:\\tau}\\]\\[\\frac{\\Gamma\\vdash e:0}{\\Gamma\\vdash\\underline{case} e\\{\\}:\\tau}\\]Dynamics (two cases, lazy or eager):lazy:\\[\\frac{}{i\\cdot e_i\\ val}\\]end lazyeager:\\[\\frac{e_i\\mapsto e_i'}{i\\cdot e_i\\mapsto i\\cdot e_i'}\\]\\[\\frac{e_i\\ val}{i\\cdot e_i\\ val}\\]end eager\\[\\frac{e\\mapsto e'}{\\underline{case}_\\tau e\\{...\\}\\mapsto\\underline{case} e'\\{...\\}}\\]\\[\\frac{i\\cdot e_i\\ val}{\\underline{case}_\\tau i\\cdot e_i\\{1\\cdot x\\hookrightarrow e_1' | 2\\cdot x\\hookrightarrow e_2'\\}\\mapsto[e_i/x]e_i'}\\]\\[\\frac{e\\mapsto e'}{\\underline{case} e\\{\\}\\mapsto\\underline{case} e'\\{\\}}\\]Some Type Algebra\\[\\tau\\times1\\cong\\tau\\\\\\tau_1\\times(\\tau_2\\times\\tau_3)\\cong(\\tau_1\\times\\tau_2)\\times\\tau_3\\\\\\tau_1\\times\\tau_2\\cong\\tau_2\\times\\tau_1\\\\\\tau+0\\cong\\tau\\\\\\tau_1+(\\tau_2+\\tau_3)\\cong(\\tau_1+\\tau_2)+\\tau3\\\\\\tau_1+\\tau_2\\cong\\tau_2+\\tau_1\\\\\\tau_1\\times(\\tau_2+\\tau_3)\\cong(\\tau_1\\times\\tau_2)+(\\tau_1\\times\\tau_3)\\]Algebra about functions (using the arrow notation):\\[\\tau\\rightarrow(\\rho_1\\times\\rho_2)\\cong(\\tau\\rightarrow\\rho_1)\\times(\\tau\\rightarrow\\rho_2)\\\\\\tau\\rightarrow1\\cong1\\]Functions can also be written by exponential notation:\\[(\\rho_1\\times\\rho_2)^\\tau\\cong\\rho_1^\\tau\\times\\rho_2^\\tau\\\\1^\\tau\\cong1\\]Something like the dual:\\[(\\tau_1+\\tau_2)\\rightarrow\\rho\\cong(\\tau_1\\rightarrow\\rho)\\times(\\tau_2\\rightarrow\\rho)\\\\0\\rightarrow\\rho\\cong1\\\\\\]In exponential notation:\\[\\rho^{\\tau_1+\\tau_2}\\cong\\rho^{\\tau_1}\\times\\rho^{\\tau_2}\\\\\\rho^0\\cong1\\]",
        "url"      : "/2018/02/17/Sum-and-Product-Types/",
        "date"     : "2018-02-17 18:50:00 +0000"
    } ,
    
    {
        "title"    : "Basic Programming Language Theory",
        "category" : "Programming Language Theory",
        "tags"     : "Programming Languages",
        "content": "  Total Programming Language          What does it mean for a PL to exist?      Statics      Dynamics      Total Programming LanguageE.Coli of Total PLs - Godel’s T:It codifies higher order functions and inductive types (nat)What does it mean for a PL to exist?A PL consists of two parts:  Statics: What are the programs?  Dynamics: How do we run them?Basic criterion: CoherenceStaticsWe will talk about abstract syntax and context-sensitive-conditions on well-formation.Formal = typing is inductively defined.We only have information about the types but not the elements themselves.So we only care about two things, higher order functions, and nat.\\[\\tau:=nat|\\tau_1\\rightarrow\\tau_2\\\\e:=x|z|s(e)|iter(e_0x.e_1)(e)|\\lambda x:\\tau.e|e_1(e_2)\\]So the typing:\\[\\Gamma\\vdash e:\\tau\\]is inductively defined by rules, which is the least relation closed under undersome rulesNote: \\(\\Gamma\\) can be viewed as a “context”. It could be written as something like\\(\\Gamma=x_1:\\tau_1,x_2:\\tau_2,\\ldots,x_n:\\tau_n\\)e.g. natural numberHypothetical Judgement (Structural):Note: 1 and 2 are indefeasible. 3, 4, 5 is defeasible (Will be different in substructural type systems).      Reflexivity\\(x:\\tau\\vdash x:\\tau\\)        Transitivity\\(if:\\Gamma\\vdash e:\\tau\\ and\\ \\Gamma,x:\\tau\\vdash e':\\tau' \\\\then:\\Gamma\\vdash[e/x]e':\\tau'\\)        Weakening\\(if:\\Gamma\\vdash e':\\tau'\\ then\\ \\Gamma,x:\\tau\\vdash e':\\tau'\\)        Contraction\\(if:\\Gamma,x:\\tau,y:\\tau\\vdash e':\\tau' \\\\then:\\Gamma,z:\\tau\\vdash[z,z/x,y]e':\\tau'\\)        Exchange\\(if:\\Gamma,x:\\tau,y:\\tau\\vdash e':\\tau' \\\\then:\\Gamma,y:\\tau,x:\\tau\\vdash e':\\tau'\\)  DynamicsExecute close code (no free variables)We need to specify:  $e\\ val$  States of execution $S$  Transition $S\\mapsto S’$  Initial State and final stateSome rules:  zero is a value\\[\\frac{}{z\\ val}\\]  successor is a value\\[\\frac{}{s(e)\\ val}\\]  function is a value\\[\\frac{}{\\lambda x:\\tau.e\\ val}\\]  functions can be executed\\[\\frac{e_2\\ val}{(\\lambda x:\\tau.e)(e_2)\\mapsto[e_2/x]e}\\]  \\[\\frac{e_1\\mapsto e_1'}{e_1(e_2)\\mapsto e_1'(e_2)}\\]    \\[\\frac{e\\mapsto e'}{iter{\\tau}(e_0;x.e_1)(e)\\mapsto iter{\\tau}(e_0;x.e_1)(e')}\\]    Values are stuck/finished\\[\\frac{e\\ val}{\\nexists e'\\ st\\ e\\mapsto\\ e'}\\]  Determinism/functional language\\[\\forall e\\exists\\leq 1\\ v\\ st\\ v\\ val\\ and\\ e\\mapsto^\\ast v\\]Note: dynamics doesn’t care about types!Coherence of Dynamics/Statics/Type Safety($\\infty$):  Preservation\\[\\frac{e:\\tau\\ and\\ e\\mapsto e'}{e':\\tau}\\]  Progress\\[\\frac{e:\\tau}{either\\ e\\ val\\ or\\ \\exists e'\\ e\\mapsto e'}\\]  Termination\\[\\forall e:\\tau\\exists unique\\ v:\\tau,v\\ val,e\\mapsto^\\ast v\\]Here we proof Termination ($T(e)$ means $e$ terminates). According to Godel, if we proof termination, we are provingthe consistency of the arithmetic, so we need methods that go beyong the arithmetic(Godel’s incompleteness):  $e$ itself can’t be a variable becasue it is closed  \\[z\\mapsto^*z\\ val\\]    \\[s(e)\\mapsto^*s(e)\\ val\\]    \\[\\lambda x:\\tau.e\\mapsto^*\\lambda x:\\tau.e\\ val\\]    But for \\(e_1(e_2)\\), even by inductive hypothesis, we know \\(e_1\\mapsto^*v_1\\)and \\(e_2\\mapsto^*v_2\\), and \\(e_1:\\tau_2\\rightarrow \\tau,e_2:\\tau_2\\), we can only do\\(e_1(e_2)\\mapsto^*v_1(e_2)=(\\lambda x:\\tau_2.e')(e_2)\\mapsto[e_2/x]e'\\), but we can’tget any further. We do need more info about $e’$!Therefor we introduct a strong property called hereditary termination: \\(HT_\\tau(e)\\)  Want: $HT_{nat}(e)$ implies $T(e)$  Define: $HT_\\tau(e)$ by induction on type $\\tau$ [Tait’s Method]          \\(HT_{nat}(e)\\ iff\\ e\\mapsto^*z\\) or \\(e\\mapsto^*s(e')\\) with \\(HT_{nat}(e')\\) (it is well-defined because it is the strongest predicate satisfying these rules)      \\(HT_{\\tau_1\\rightarrow\\tau_2}(e)\\ iff\\ e\\mapsto^*\\lambda x:\\tau_1.e'\\) andfor every $e_1$ such that \\(HT_{\\tau_1}(e_1),HT_{\\tau_2}([e_1/x]e')\\) (meaning “type goes down”).To write this in another form,\\(HT_{\\tau_1\\rightarrow\\tau_2}(e)\\ iff\\ (if\\ HT_{\\tau_1}(e_1),then\\ HT_{\\tau_2}(e(e_1)))\\)      And we have $Thm(v2)$:If \\(e:\\tau\\) then \\(HT_\\tau(e)\\), and then therefore \\(T_\\tau(e)\\)So now for inductive hypothesis, we not only know \\(e_1\\mapsto^*v_1\\) 15 and \\(e_2\\mapsto^*v_2\\),but also \\(HT_{\\tau_2\\rightarrow\\tau}(e_1)\\) and \\(HT_{\\tau_2}(e_2)\\). And becausewe know that $e_1$ is a $\\lambda$, we now know that \\([e_2/x]e'\\) is $HT$!BUT! $Thm(v2)$ is only stating about $e$ being closed terms, but for $\\lambda$:\\(\\frac{x:\\tau_1\\vdash e_2:\\tau_2}{\\lambda x:\\tau_1.e_2:\\tau_1\\rightarrow\\tau_2}\\),$e_2$ is an open term, meaning it is a variable, so our theorm doesn’t quite work.We must account for open terms!$Thm(v3)$[Tait]:If \\(\\Gamma\\vdash e:\\tau\\) and \\(HT_\\Gamma(\\gamma)\\), then \\(HT_\\tau(\\hat{\\gamma}(e))\\)So \\(HT_\\Gamma(\\gamma)\\) means that if \\(\\Gamma=x_1:\\tau_1,x_2:\\tau_2,\\ldots,x_n:\\tau_n\\),then $\\gamma=x_1\\hookrightarrow e_1,x_2\\hookrightarrow e_2,\\ldots,x_n\\hookrightarrow x_n$ (substitution)such that \\(HT_{\\tau_1}(e_1),\\ldots,HT_{\\tau_n}(e_n)\\), and \\(\\hat{\\gamma}(e)\\) meansto do the substitution. So we are good now!",
        "url"      : "/2018/02/17/Basic-Programming-Language-Theory/",
        "date"     : "2018-02-17 15:15:00 +0000"
    } ,
    
    {
        "title"    : "Garbage Collection",
        "category" : "Garbage Collection",
        "tags"     : "Garbage Collection, Slides",
        "content": "  Variable Storage and Lifetime  Terminology          Roots, Liveness, and Reachability      Liveness of Allocated Objects        Classic GC Algorithms          Reference Counting GC                  Strengths          Weaknesses                    Mark &amp; Sweep GC                  Strengths          Weaknesses                    Copying Collector                  Strengths          Weaknesses                      The Principle of Locality  Generational GC  LinksVariable Storage and LifetimeWe need to talk about memory first before talking about GC. So where can variables be stored?  Static (compile-time or load time)  Stack (runtime) - aka user/runtime/system stack  Heap (runtime)For primitive variables, there are 3 categories (given different lifetimes)  Globals (static storage)          Variables declared outside of any function or class (outermost scope)      Scope: accessible to all statements in all functions in the file      Lifetime: from start ofprogram (loading) to end (unloading)      Good practice: use sparingly, make constant as often as possible      Stored in read-only or read-write segments of the process virtual memory space - allocated/fixed before prograrm starts                  Read-only segment holds translated/native code as well if any                      Locals (stack storage)          Parameters and variables declared within a function      Scope: accessible to all statements in the function they are defined      Lifetime: from start to end of the function invocation      Stored in User/Runtime stack in process virtual memory space                  Allocated/deallocated with functino invocations and returns                      Dynamic variables, aka pointer variables (heap storage)          Pointer variables that point to variables that are allocated explicitly      Scope: global or local depending on where they are declared      Lifetime: from program point at which they are allocated with new to the one that at which they are deallocated with delete      Pointer variables (the address) are either globals or locals      The data they point to is stored in the heap.      Here is a graph of a process memory:As we all know, we only do garbage collection on the heap for implicit memory allocation.Terminology  Collector          Part of the runtime that implements memory management        Mutator          User program - change (mutate) program data structures        Stop-the-world collector - all mutators stop during GC  Values that a program can manipulate directly          In processor registers      On the program stack (includes locals/temporaries)      In global variables (e.g., array of statics)        Root set of the computation          References to heap data held in these locations      Dynamically allocated data only accessible via roots      A program should not access random locations in heap      Roots, Liveness, and Reachability  Individually allocated pieces of data in the heap are          Nodes, cells, objects (interchangeably)      Commonly have header that indicates the type (and thus can be used to identify any references within the object)                  AKA boxed                      Live objects on the heap          Graph of objects that can be “reached” from roots                  Objects that cannot be reached are garbage                    An object in the heap is live if                  Its address is held in a root, or          There is a pointer to it held in another live heap object                    Liveness of Allocated Objects  Determined indirectly or directly  Indirectly          Most common method: tracing      Regenerate the set of live nodes whenever a request by the user program for more memory fails      Start from each root and visit all reachable nodes (via pointers)      Any node not visited is reclaimed        Directly          A record is associated with each node in the heap and all references to that node from other heap nodes or roots      Most common method: reference counting      Must be kept up to date as the mutator alters the connectivity of the heap graph      Classic GC AlgorithmsThere are mainly three classic GC algorithms  Reference counting  Mark &amp; Sweep  CopyingFor the first two method, we need a thing called Free List which keeps 1+ lists of free chunks that we then fill or break off pieces of to allocated an objectReference Counting GC  Each obejct has an additional atomic field in header that holds the humber of pointers to that cell from roots or other objects  All cells placed in free list initially with count of 0      freeList points to the head of the free list    Each time a pointer is set to refer to this cell, the count is incremented  Each time a reference is removed, count is decremented          If the count goes to 0                  There is no way for the program to access this cell                    The cell is returned to the free list        When a new cell is allocated          Reference count is set to 1      Removed from free list                  Assume, for now, that all cells are the same size and each has 3 fields left and right which are references                    Strengths  Memory management overheads are distributed throughout the computation          Management of active and garbage cells is interleaved with execution      Incremental      Smoother response time        Locality of reference          Things related are accessed together (for memory hierarchy performance)      No worse than program itself        Short-lived cells can be reused as soon as they are reclaimed          We don’t have to wait until memory is exhausted to free cells      Immediate reuse generates fewer page faults for virtual memory      Update in place is possible      Weaknesses  High processing cost for each pointer update          When a pointer is overwritten the reference count for both the old and new target cells must be adjusted      May cause poor memory performance      Hence, it is not used much in real systems        Extra space in each cell to store count (normally sizeof(int))  Cyclic data structures can’t be reclaimed (e.g. doubly linked lists)Mark &amp; Sweep GC  Tracing collector          mark-sweep, mark-scan      use reachability (indirection) to find live objects        Object are not reclaimed immediately when they become garbage          Remain unreachable and undetected until storage is exhausted        When reclamation happens the program is paused          Sweep all currently unused cells back into the freeList      GC performs a global traversal of all live objects to determine which cells are reachable (live or active)                  Trace, starting from roots, marking them as reachable          Free all unmarked cells                      Each cell contains 1 bit (markBit) of extra info  Cells in freeList have markBit set to 0  No Update(...) routine necessaryStrengths  Cycles are handled quite normally  no overhead placed on pointer manipulations  Better than (incremental) reference countingWeaknesses  Start-stop algorithm (aka stop-the-world)          Computation is halted while GC happens      Not practical for real-time systems        Asymptotic complexity is proportional to the size of the heap not just the live objects for sweep  Fragments memory (scatters free cells across memory)          Loss of memory performace (caching/paging)      Allocation is complicated (need to find a set of cells for the right size)        Residency - heap ocupancy          As this increases, the need for garbage collection will become more frequent      Taking processing cycles away from the applicatino      Allocation and program performance degrades as residency increases      Copying Collector  Tracing, stop-the-world collector          Divide the heap into two semispaces                  One with current data          The other with obsolete data                    The roles of the two semispaces is continuously flipped      Collector copies live data from the old semispace                  FromSpace          To the new semispace (ToSpace) when visited          Pointers to objects in ToSpace are updated          Program is restarted                    Scavengers                  FromSpace is not reclaimed, just abandoned                    Strengths  Have lead to its widespread adoption  Active data is compact (not fragmented as in mark-sweep)          More efficient allocation, just grab the next group of cells that fits      The check for space remaining is simply a pointer comparison        Handles variabl-sized objects naturally  No overhead on pointer updates  Allocation is a simple free-space pointer increment  Fragmentation is eliminated          Compaction offers improved memory hierarchy performance of the user program      Weaknesses  Required address space is doubled compared with non-copying collectors          Primary drawback is the need to divide memory into two      Performance derades as residency increases (twice as quickly as mark&amp;sweep because half the space)        Touches every page (VM) of the heap regardless of residency of the user program          Unless both semispaces can be held in memory simultaneously      The Principle of Locality  A good GC should not only reclaim memory but improve the locality of the system on the whole          Principle of locality programs access a relatively small portion of their address space at any particular time (temperal and spacial locality)      GC should ensure that locality is exploited to improve performance wherever possible      memory hierarchy was developed to exploit the natural principle of locality in programs                  Different levels of memory each with different speeds/sizes/cost          Registers, cache, memory, virtual memory                    Generational GC  Observations with previous GCs          Long-lived objects are hard to deal with      Young objects (recently allocated) die young                  weak-generational hypothesis: most are young (80-90%)                    Large heaps (that can’t be held in memory) degrade performance      Goal: Make large heaps more efficient by concentrating effort where the reatest payoff is  Segregate objects by age into two or more heap regions          Generations                  Keep the young generation separate                    Collected at different frequencies                  The younger the more often          The oldest, possible never                      Can be implemented as an incremental scheme or as a stop-the-world scheme          Using different algorithms on the different regions        Promotion          Move object to older generation if its survives long enough        Concentrate on youngest generation for reclamation          This is where most of the recyclable space will be found      Make this region small so that its collection can be more frequent but with shorter interruption        A younger generation can be collected without collecting an older generation  The pause time to collect a younger generation is shorter than if a colection of the heap is performed  Young objs that survive minor collections are promoted          Minor collections reclaim shortlived objects        Tenured garbage: garbage in older generations  Allocation always from minor          Except perhaps for large or known-to-be-old objects        Minor frequent, major very infrequent  Major/minor collections can be any type          Mark/sweep, copying, mark/compact, hybrid      Promotion is copying        Can have more than 2 generations          Each requiring collection of those lower/younger        Minor Collection must be independent of major          need to remember old-to-young references      Usually not too many - mutations to old objects are infrequent        What about young-to-old?          We don’t need to worry about them if we always collect the young each time we collect the old (major collection)        Write barriers          Catching old-to-young pointers      Code that puts old-generation object into a remembered set                  Traversed as art of root set          All field assignments aka POINTER UPDATES IN YOUR CODE!                      Alternative to write barriers          Check all old objeccts to see if they point to a nursery object      Will negate any benefit we get from generational GC      LinksGarbage Collection(YouTube: Part 1/3)(YouTube: Part 2/3)(YouTube: Part 3/3)",
        "url"      : "/2017/11/18/Garbage-Collection/",
        "date"     : "2017-11-18 17:30:00 +0000"
    } ,
    
    {
        "title"    : "Remote Shell Session Setup: iTerm2+tmux+mosh",
        "category" : "Random",
        "tags"     : "Mosh, iTerm, tmux",
        "content": "  Requirements and Comments  My Solution          Mosh      tmux      iTerm2      Requirements and CommentsInspired by My Remote Shell Session SetupMy work requires connecting to a few different hosts via ssh. Similar to My Remote Shell Session Setup while still a little different, my personal requirements are as follows:  I want one tab of the terminal I’m using to be connected to one remote host  I want the shell to survive unaffected with no context loss the following events          connection failure      route change (like, toggling the VPN or changing Wi-fi)      laptop sleep (like, me closing the lid)      local terminal restart or laptop reboot        I want to be able to copy-paste  I want colors  I want to launch it with a single commandSome requirements are just copy pasted. Specifically for requirement #1，I used to manage various tabs by using screen/tmux alone. The problem with that approach is when you want to switch to next tab/session, you have to press the escape key (usually ctrl-b or something else) plus another key, which is 3 keys in total. 3 keys are too many for me; I want 2 keys only, so using the tab of the terminal itself might be a good idea. Also, because I need to get access to different hosts, I actually need multiple tmux windows, which is quite inconvenient.My SolutionSimilar to My Remote Shell Session Setup, I also use iterm2+mosh+tmux, with some different settings. I use Mac and brew, but similar commands should be available for linux as well. If you are using Windows, the best solution is to buy a Mac.Moshbrew install mosh and replace your usual ssh command with mosh and it would just works.tmuxbrew install tmux. The main reason why I need tmux is that mosh doesn’t quite support scrollback well. In My Remote Shell Session Setup, the author suggests one solution but scrolling is way less fluid than native, and most importantly, it requires building mosh from source on the server side and you need sudo apt-get to do that. Usually we don’t have root access to the server, so I give up the scrollback feature. If you have root access, that’s great and follow his steps to get scrollback working. If not, use tmux/screen for scrollback.iTerm2Go to Preference-&gt;Profiles and click the plus sign at the bottom to create a new profile. I create one profile per remote host I want to connect to. You can specify a name for your profile, and a shortcut key for opening that profile. Then type the mosh command you use to connect to the server in the “Send text at start” field. Here is my example:where sshcvp is the alias of mosh --ssh='ssh -p 10140' r123s19 -- sh -c \"tmux ls | grep -vq attached &amp;&amp; tmux a || tmux new\". Why the heck is that so long? Becasue \"tmux ls | grep -vq attached &amp;&amp; tmux a || tmux new\" is just the tmux way of saying screen -R, which attaches to an unattached session if one exists or creates a new session if all sessions are attached or there is no session at all. I don’t like screen becasue it is not aesthetically appealing so I choose tmux. Why use sh -c but not directly run the following command? Because mosh doesn’t like grep and we can’t run grep directly. Adding sh -c is the workaround. Issue with mosh when running with grep.Then you can just open a bunch of different profiles, each per tab, which are connected to different hosts. You can open more tabs than you usually need, just to be safe. Then click Window-&gt;Save Window Arrangement, which will save your arrangement for all of your tabs, and can be restored easily. To test that, press Cmd-q to close iTerm2. Then open iTerm2 again, go to Window-&gt;Restore Window Arrangement and select your saved arrangement and resotre it. You can see that all your tabs including your connections to those various hosts come back.",
        "url"      : "/2017/10/09/Remote-Shell-Session-Setup/",
        "date"     : "2017-10-09 15:45:00 +0000"
    } ,
    
    {
        "title"    : "Matrix Game Theory",
        "category" : "Game Theory",
        "tags"     : "Game Theory, Math",
        "content": "  Zero Sum Game          Saddle Point      2$\\times$2 Game      $m\\times n$ Zero Sum Game                  Some Other Lemma and Prop                      Non-zero Sum Game          Mix Strategy      Definitions, Axioms, and Nash’s Theorm                  Nash’s Theorm                    Matrix Game: Two players, each makes a choice secretly and play simutaneously. And there is payoff.Zero Sum GameSaddle Point$(X,Y)$ is a saddle point if entry $x$ is the largest value in column $X$ and smallest value in its row.Thm: All saddle points has the same value and appears as corners of a rectangle.2$\\times$2 GameAll examples below are between two players Colin and Ros. Because it is a zero sum game, we only write Rose’s payoff, and Colin’s payoff is just the inverse.Example:                   A      B              A      2      -3              B      -1      3      If Colin plays $\\frac{2}{3}A,\\frac{1}{3}B$:Rose A average payout is $2\\times\\frac{2}{3}-3\\times\\frac{1}{3}=\\frac{1}{3}$Rose A average payout is $-1\\times\\frac{2}{3}+3\\times\\frac{1}{3}=\\frac{1}{3}$For Colin playing $\\frac{2}{3}A,\\frac{1}{3}B$ minimizes Rose’s maximum average payout; similarly Rose wants to maximize her minimum average payout. So:\\[max_{y\\in[0,1]}\\{min\\{(2y-1(1-y), -3y+3(1-y))\\}\\}\\]So she wants:\\[2y-1(1-y)=-3y+3(1-y)\\\\y=\\frac{4}{9}\\]So the average payout is $2y-(1-y)=\\frac{1}{3}$Note: If there is a saddle point in the game, then it can’t be solved.For            a      b              c      d      with no dominant row/column:\\[a&gt;c\\iff b\\leq d\\\\a\\leq c \\iff b&gt;d\\]So for Colin whose payout is $x$:\\[ax+b(1-x)=cx+d(1-x)\\\\(a-b-c+d)x = d-b\\\\x=\\frac{d-b}{a-c+d-b}\\]$m\\times n$ Zero Sum GameVon Neumann’s Minimax Thm: Every $m\\times n$ game has a solution. Each player has a distribution of their choices such that Rose’s average minimum payout is maximized, and Colin’s maximum average payout is minimized, and two payouts are the same.The proof is omitted not because I am lazy but for exercise (fact!). ¯_(ツ)_/¯Some Other Lemma and PropLet $A$ be a $m\\times n$ matrix. $P$ is the payout vector for Rose, and $q$ is for column\\[\\begin{align}P &amp;= \\begin{bmatrix}p_{1} \\\\p_{2} \\\\\\vdots \\\\p_{m}\\end{bmatrix}\\end{align}\\in[0,1]^m\\ s.t.\\ \\sum_{i=1}^mp_i=1\\\\P^TA=[p^TA_1,p^TA_2,\\dots,p^TA_n]\\ s.t.\\ i^{th}\\ entry\\ is\\ Rose's\\ average\\ payoff\\ when\\ Colin\\ plays\\ i^{th}\\ strategy.\\\\\\]\\[\\begin{align}Aq &amp;= \\begin{bmatrix}A_{1}q \\\\A_{2}q \\\\\\vdots \\\\A_{m}q\\end{bmatrix}\\end{align}\\\\r=maximin=max(min(p^TA_i))\\ (for\\ Rose)\\\\c=minimax=min(max(Aq))\\ (for\\ Colin)\\]Lemma:\\[for\\ any\\ strategy:\\ \\forall p,q\\ s.t.\\ p,q\\in[0,1]^m,\\sum p_i=\\sum q_i=q\\\\min_{i=1,2,\\dots n}((p^TA)_i)\\leq p^TAq\\\\max_{i=1,2,\\dots m}((Aq)_i)\\geq p^TAq\\\\\\implies min(p^TA)\\leq max(Aq)\\\\\\implies max(min(p^TA))\\leq min(max(Aq))\\\\\\implies r\\leq c\\]Prop: $p,q$ are strategies for Rose, Colin such that\\[min(p^TA)=max(Aq)=V\\\\Then\\\\If\\ p_i&gt;0\\implies(Aq)_i=V\\\\If\\ q_i&gt;0\\implies(p^TA)_i=V\\]So how do we find the minimax\\maximin? Use linear programming, which is not interesting at all so we will just skip that.Non-zero Sum GameAll examples below are between two players Colin and Rose, where Colin’s payoff is the column one and Rose’s payoff is the row; i.e. for $(X,Y)$, Colin has $X$ and Rose has $Y$.Let start with a simple example            (-1,-1)      (-10, 0)              (0, -10)      (-5, -5)      Obviously, the point (-5, -5) is the pure Nash equilibrium.Mix StrategyA mix strategy is just a vector of probability of strategy.If $q$ is a mix strategy for Colin, a best response for Rose is any $p$ with $p_i=0$ if $(Rq)$ is not a max entry.A Nash Equilibrium is a pair $(p,q)$ of mix strategy such that each is a best response for the other.Thm: for $2\\times2$ game, $(p,q)$ is a Nash Eq if $p^TC$ and $Rq$ have equal entries.Definitions, Axioms, and Nash’s TheormWe know that Nash equilibrium is not necessary global optimal. For the example above, they could have picked (-1, -1), which is better than (-5, -5), but they end up with the worse one. Therefore, we might want someone to pick a position for them, instead of letting them come up with a solution themselves.Given a game, the God selects a position for R and C that is a “fair”. To be a “fair” decision \\((x*,y*)\\) the outcome should be:  Pareto Optimal: No $(x,y)$ with \\((x,y)\\geq(x*,y*)\\)  \\((x*,y*)\\) should be at least their maximin strategy with repsect to their own game (or they will just pick the maximin strategy)We call the numbers in the matrix utility, because it is not necessarily be money.Also, we can map the matrix geometrically, where each payoff is just a point. I won’t show it here, because it should be obvious. After placing all the points, we can draw a polygon by connecting those points. So the boundary on the North-East direction of the convex hull is pareto optimal. If the maximin solution point is inside the convex hull, then the boundary of the convex hull on the North-East direction of the maximin solution (which is a point) is called negotiation set.Then we have the following axioms: Any “fair” decision should be:  In negotiation set.  If either player’s utilities, say Roses’, is tranformed by $g(x)=mx+n,m&gt;0$, then \\((g(x*),y*)\\) is a fair decision in the new game.  If the payoff polygon is symmetric about the line $x=y$, then \\((x*,y*)\\) is on this line.  Suppose $P$ is a payoff polygon, and a fixed “status quo” point, which could be the maximin or other “fall back” point, $SQ$ is given. Suppose $Q$ is another polygon contained in $P$ where $SQ$, \\((x*,y*)\\in Q\\), then \\((x*,y*)\\) is the fair decision for $Q$.Nash’s TheormNash’s Thm:There is only one point \\((x*,y*)\\) that satisfies all of the above axioms.If $SQ=(x_0,y_0)$, then \\((x*,y*)\\) maximizes $(x-x_0)(y-y_0)$, or \\((x*,y*)=(x_0,y_0)\\)The proof is omitted not because I am lazy but for exercise (fact!). ¯_(ツ)_/¯",
        "url"      : "/2017/09/10/Matrix-Game-Theory/",
        "date"     : "2017-09-10 15:23:00 +0000"
    } ,
    
    {
        "title"    : "Combinatorial Game Theory",
        "category" : "Game Theory",
        "tags"     : "Game Theory, Math",
        "content": "  Basic Definitions  Types of Positions  Definitions, Lemma, and Propositions  Inverse Element  Impartial Games          Nim      Sprague–Grundy Theorem        Partisan Games          Hackenbush                  Some Definitions          Dyadic Number                          Construction                                The Simplicity Principle                    Basic Definitions  Normal play: first player who cannot move loses (and therefore no draws)Two kinds of normal play:  Impartial: For every position the moves available doesn’t depend on whose turn it is.Partisan: Not impartialTwo players are called Louise and Richard. Let $\\alpha$ be a position in some normal game:\\[\\alpha=\\{\\alpha_1,\\alpha_2,\\dots,\\alpha_n\\mid\\beta_1,\\beta_2,\\dots,\\beta_m\\}\\]where $\\alpha_i$ is the position where Louise can move from $\\alpha$, and $\\beta_j$ is the position where $R$ can move from $\\alpha$Types of Positions(L) Louise has a winning strategy regardless of who moves first at $\\alpha$(R) Richard has a winning strategy regardless of who moves first at $\\alpha$(N) Player who moves next has a winning strategy(P) Player who moves previous has a winning strategyDefinitions, Lemma, and Propositions  Prop: If \\(\\gamma=\\{\\alpha_1,\\alpha_2,\\dots,\\alpha_n\\mid\\beta_1,\\beta_2,\\dots,\\beta_m\\}\\),then $\\gamma$ has type:                   Some $\\beta_i$ is (R) or (P)      All $\\beta_i$ is (L) or (N)              Some $\\alpha_i$ is (L) or (P)      N      L              Some $\\alpha_i$ is (R) or (N)      R      P        Def: Given position $\\alpha$,$\\beta$, not necessary in the same game, define $\\alpha+\\beta$ to be a new position where move at $\\alpha+\\beta$ consists of moves in $\\alpha$ or $\\beta$            $\\alpha+\\beta$      L      R      P      N              L      L      ?      L      ?              R      ?      R      R      ?              P      L      R      P      N              N      ?      ?      N      ?        Def: If $\\forall\\gamma$, $\\alpha+\\gamma=\\beta+\\gamma$, we say $\\alpha\\equiv\\beta$  Lemma: If $\\alpha\\equiv\\beta$, then $\\alpha$ and $\\beta$ has same type  Prop:(1) $\\forall\\alpha\\equiv\\alpha$(2) $\\alpha\\equiv\\beta\\implies\\beta\\equiv\\alpha$(3) $\\alpha\\equiv\\beta,\\beta\\equiv\\gamma\\implies\\alpha\\equiv\\gamma$(4) $\\alpha+\\beta\\equiv\\beta+\\alpha$(5) $(\\alpha+\\beta)+\\gamma\\equiv\\alpha+(\\beta+\\gamma)$  Lemma:If $\\alpha\\equiv\\alpha’$, then $\\alpha+\\beta\\equiv\\alpha’+\\beta\\forall\\beta$  Lemma:If $\\beta$ is (P), then $\\forall\\alpha,\\alpha+\\beta\\equiv\\alpha$  Prop: If $\\alpha,\\alpha’$ are (P), then $\\alpha\\equiv\\alpha’$  Lemma: If $\\alpha+\\beta,\\alpha’+\\beta$ are both (P), then $\\alpha\\equiv\\alpha’$Inverse ElementWe have the identity element (P) now, what about the inverse element, namely: $\\alpha+(-\\alpha)=(P)$?For impartial games: $\\alpha+\\alpha=(P)$. The strategy is that the second player just copies the first player’s move in other game.In general normal game:\\[\\alpha=\\{\\alpha_1,\\alpha_2,\\dots,\\alpha_n\\mid\\beta_1,\\beta_2,\\dots,\\beta_m\\}\\\\-\\alpha=\\{\\beta_1,\\beta_2,\\dots,\\beta_m\\mid\\alpha_1,\\alpha_2,\\dots,\\alpha_n\\}\\\\\\alpha+(-\\alpha)=(P)\\]Impartial GamesNimHere is the rule and other info about Nim. In short there are multiple heaps of stones. Each turn each player takes out any number(at least one) of stones from one heap.We define $*a$ as one heap with $a$ stones (Nimber)  Def: $*a_1+*a_2+\\dots+*a_k$ is balanced if $\\forall k$, $2^k$ in binary appears in even number of $a_i$’s.We know that no stone at all is balanced. Then how do we make unbalanced pile balanced with one move?Look for the max $2^l$ that appears on odd number of times; take that away, and adjust the lower positions as you like. It is always valie because $2^n-1=2^{n-1}+2^{n-2}+\\dots+2+1$Note: $*a_1+*a_2+\\dots+*a_n\\equiv*0$ when $\\sum*a_i$ is balancedClaim: Every unbalanced pile is equivalent to $*b$:\\[\\exists b\\in\\mathbb{Z}\\geq0\\\\*a_1+\\dots+*a_k=*b\\]Idea: $*a_1+\\dots+*a_k+*b\\equiv*0$Then: $b=\\sum_{i=0}^{\\infty}c_i 2^i$ where\\[c_i=\\left\\{ \\begin{array}{rcl}1 &amp; \\mbox{if} &amp; 2^i\\ appears\\ in\\ an\\ odd\\ number\\ of\\ a_i \\\\ 0  &amp; \\mbox{if} &amp; otherwise \\\\\\end{array}\\right.\\]Sprague–Grundy TheoremSprague-Grundy Theorm: Every impartial game is equivalent to a nimber.Def: If $S={a_1,\\dots,a_n}\\subseteq\\mathbb{Z}_{\\geq0}$, then\\[mex(S):=smallest\\ b\\in\\mathbb{Z}_{\\geq0}\\setminus S,\\ (mex(S)\\geq0)\\]Claim: \\((\\alpha=\\{*a_1,\\dots,*a_n\\})+*b\\) is (P) when $b=mex(a_1,\\dots,a_n)$ (note that $\\alpha$ can be the position an any game, not just nim). We skip the proof here because I am lazyCorollary: if \\(\\alpha_i\\equiv*a_i\\), $i=1,2,\\dots,n$, then \\(\\{\\alpha_1,\\dots,\\alpha_n\\}\\equiv*b\\), where \\(b=mex(a_1,\\dots,a_n)\\)Partisan GamesHackenbushHere is the rule and other info about Hackenbush, and let’s have Richard plays Red and Louise plays Black. Since it is a partisan game, we might want to have some kind of definition of “advantage”. All the following definitions are from Louise’s perspective; namely, Black’s advantage is positive and Red’s is negative.We use $\\bullet n$ notation to define an advantage of $n$.Some DefinitionsOnly ground and nothing else is $\\bullet0$Ground with one black on it is $\\bullet1$, and ground with one red on it is $\\bullet(-1)$Ground with $n$ blacks on it is $\\bullet n$, and vice versa for red.What is advantage of $\\frac{1}{2}$?We want $\\alpha+\\alpha=\\bullet1$, or $\\alpha+\\alpha+\\bullet(-1)=\\bullet0$, which is (P)So we define one black on the ground, and one red on the black is $\\bullet\\frac{1}{2}$. Similarly, one black on the ground and $k$ red on the black is $\\bullet\\frac{1}{2^k}$. (It doesn’t matter how those reds are put on the black, as long as once we take out the black and they all die)  Prop: $\\bullet\\frac{1}{2^k}+\\bullet\\frac{1}{2^k}=\\bullet\\frac{1}{2^{k-1}}$Dyadic NumberDyadic numbers are rationals of form $\\frac{a}{2^k},a,k\\in\\mathbb{Z}$Construction$0$ is born on day 0$-1,1$ are born on day 1$-2,2,-\\frac{1}{2},\\frac{1}{2}$ are born on day 2$\\dots$If $a_0&lt;a_1&lt;\\dots&lt;a_n$ are born on days $0,1,\\dots,n$, the numbers born on day $n+1$ are: $a_0-1,a_k+1,\\frac{a_i+a_{i+1}}{2}\\forall i=0,\\dots,k-1$It can be easily known that all dyadic numbers will be born.  Lemma: Every open interval in $\\mathbb{R}$ has a unique oldest dyadic number  Def: $\\bullet\\frac{a}{2^k}=\\bullet(\\sum_{d\\in\\mathbb{Z}}2^d):=\\sum\\bullet2^d$  Lemma: If $a_i=0\\ or\\ 2^d$, and if $a_1+\\dots+a_k=0$, then $\\bullet a_1+\\bullet a_2+\\dots+\\bullet a_k=\\bullet0$Collery: If $a,b$ are dyadic numbers:\\[-\\bullet a=\\bullet-a\\\\\\bullet a+\\bullet b=\\bullet(a+b)\\]If $a$ is dyadic:\\[a&gt;0\\implies\\bullet a\\ is\\ (L)\\\\a&lt;0\\implies\\bullet a\\ is\\ (R)\\\\a=0\\implies\\bullet a\\ is\\ (P)\\]The Simplicity Principle  Theorm: \\(\\gamma=\\{\\alpha_1,\\dots,\\alpha_m\\mid\\beta_1,\\dots,\\beta_n\\}\\). Suppose $\\alpha_i\\equiv\\bullet a_i$, $\\beta_j\\equiv\\bullet b_j$ for some dyadic number $a_i,b_j\\ \\forall i,j$, assuming $a_1&lt;a_2&lt;\\dots&lt;a_m$, $b_1&lt;b_2&lt;\\dots&lt;b_n$. Then if $a_m&lt;b_1$, then $\\gamma\\equiv\\bullet c$, where $c$ is the unique oldest dyadic number in interval $(a_m,b_1)$  Lemma: Let $c=\\frac{a}{2^k},\\ a\\neq0,k\\geq1$. Suppose a player moves $\\bullet c$ to $\\bullet c’$. If Louise moved, $c’\\leq c-\\frac{1}{2^k}$ If Richard moved, $c’\\leq c+\\frac{1}{2^k}$",
        "url"      : "/2017/08/26/Combinatorial-Game-Theory/",
        "date"     : "2017-08-26 09:51:00 +0000"
    } ,
    
    {
        "title"    : "Markov Chains",
        "category" : "Statistics",
        "tags"     : "Statistics",
        "content": "  The Setting  Mathematical Representation of the Evolution  Some Properties  Example: The Connectivity ProblemThe SettingThere is a system with $n$ possible states/values. At each step, the state changes probabilistically.Let $X_t$ be the state of the system at time $t$So the evolution of the system is: $X_0,X_1,X_2,\\cdots,X_t,\\cdots$ and $X_0$ is the initial state.The system is memoryless: the probability that $X_t$ is in a certain state is determined by the state of $X_{t-1}$:\\[Pr[X_t=x|X_0=x_0,X_1=x_1,\\cdots,X_{t-1}=x_{t-1}]=Pr[X_t=x|X_{t-1}=x_{t-1}]\\]For simplicity, we assume that:  Finite number of states  The underlying graph is strongly connected: for any two states, there is a path from 1 to 2 and a path from 2 to 1Mathematical Representation of the EvolutionIn general, if the initial probabilistic state is $[p_1\\ p_2\\ \\cdots\\ p_n]=\\pi_0$, where $p_i$ is the probability of being in state $i$, $\\sum p_i=1$, and the transition matrix is $T$, such that:\\[T[i,j]=Pr[X_1=j|X_0=i]\\]After $t$ more steps, the probabilistic state is:\\[[p_1 \\ p_2 \\ \\cdots \\ p_n]\\centerdotT^t\\]And the probability of bing in state $i$ after $t$ steps is:\\[\\pi_t[i]=(\\pi_0T^t)[i]\\]Some Properties  Suppose the Markov chain is “aperiodic”, then as the system evolves, the probabilistic state converges to a limiting probabilistic state:\\[As\\ t\\rightarrow\\infty\\\\[p_1\\ p_2\\ \\cdots\\ p_n]\\centerdot T^t\\rightarrow\\pi\\\\\\pi\\centerdot T=\\pi\\]So the resulting $\\pi$ is called stationary/invariant distribution, which is unique.  Let $T_{ij}$ be the time of reaching state $j$ when you start at state $i$, then:\\[\\mathbb{E}[T_{ii}]=\\frac{1}{\\pi[i]}\\]This is known as the Mean Recurrence Theorem.Example: The Connectivity ProblemGiven a undirected graph $G=(V,E),\\lvert V\\rvert=n,\\lvert E\\rvert=m$, and $s,t\\in V$, check if there is a path from $s$ to $t$.It is easy to do in polynomial time with BFS or DFS, but how about using only $O(\\log n)$ space?Here is a possible randomized algorithm:v = sfor k = 1,2,...,N:    v = random-neighbor(v)    if v == t, return YESreturn NOFor $N=poly(n)$, then this uses $O(\\log n)$ space.But what is the success probability? If $s$ and $t$ are disconnected, we give the correct answer.What if $s$ and $t$ are connected?If we have a graph $A$ represented by the following adjacency matrix, and we start at any vertex and randomly walk to a neighbor, how does the transition matrix look like?\\[A=\\left( \\begin{array}{c}         0 &amp; 1 &amp; 1 &amp; 1 \\\\        1 &amp; 0 &amp; 1 &amp; 0 \\\\        1 &amp; 1 &amp; 0 &amp; 1 \\\\        1 &amp; 0 &amp; 1 &amp; 0    \\end{array} \\right)\\\\then\\\\T=\\left( \\begin{array}{c}         0 &amp; 1/3 &amp; 1/3 &amp; 1/3 \\\\        1/2 &amp; 0 &amp; 1/2 &amp; 0 \\\\        1/3 &amp; 1/3 &amp; 0 &amp; 1/3 \\\\        1/2 &amp; 0 &amp; 1/2 &amp; 0    \\end{array} \\right)\\\\\\]The stationary distribution is:\\[\\pi=[\\frac{deg(1)}{2m},\\frac{deg(2)}{2m},\\frac{deg(3)}{2m},\\cdots,\\frac{deg(n)}{2m}]\\]So\\[\\mathbb{E}[T_{ii}]=\\frac{2m}{deg(i)}\\]What we care is really $\\mathbb{E}[T_{ij}]$, where $i$ and $j$ are connected.We pick a path from $i$ to $j$: $i=i_1,i_2,i_3,\\cdots,i_r=j\\ (r\\leq n)$\\[\\begin{align}\\mathbb{E}[T_{ij}] &amp; \\leq\\mathbb{E}[T_{i_1i_2}+T_{i_2i_3}+\\cdots+T_{i_{r-1}i_r}]\\\\&amp; = \\mathbb{E}[T_{i_1i_2}]+\\mathbb{E}[T_{i_2i_3}]+\\cdots+\\mathbb{E}[T_{i_{r-1}i_r}]\\\\&amp; \\leq 2m+2m+\\cdots+2m=2mn\\leq n^3\\end{align}\\]Why $\\mathbb{E}[T_{uv}]\\leq 2m$?\\[\\begin{align}\\mathbb{E}[T_{vv}]&amp;=\\frac{2m}{deg(v)}\\\\&amp;=\\sum_{i=0}^kPr[first\\ step\\ v\\ to\\ u_i]\\centerdot\\mathbb{E}[T_{vv}\\lvert first\\ step\\ v\\ to\\ u_i]\\\\&amp;=\\sum_{i=0}^k\\frac{1}{deg(v)}\\centerdot(1+\\mathbb{E}[T_{u_iv}])\\\\&amp;\\geq\\frac{1}{deg(v)}\\centerdot(1+E[T_{u_0v}])\\\\&amp;\\Rightarrow 2m\\geq 1+\\mathbb{E}[T_{uv}]\\\\&amp;\\Rightarrow \\mathbb{E}[T_{uv}]\\leq 2m\\end{align}\\]So if we set N in the algorithm to be $1000n^3$, then:\\[Pr[error]=Pr[T_{st}&gt;1000n^3]\\leq\\frac{1}{1000}\\]by Markov’s inequality.",
        "url"      : "/2017/06/26/Markov-Chains/",
        "date"     : "2017-06-26 16:51:00 +0000"
    } ,
    
    {
        "title"    : "System Signals",
        "category" : "Operating System",
        "tags"     : "",
        "content": "  Exception and Interrupt  Signal  Sending a Signal  Receiving a Signal  Pending and Blocked Signals  Process Data Structures  Process Groups  Some Examples of Sending SignalsException and InterruptBefore we talk about signals, lets go over interrupts and exceptions briefly. The operating system maintains a table of exceptions and interrupts, the entry of each of which corresponds to a handler that gets called when the system gets a specific exception/interrupt. Exceptions and interrupts are inplemented in the hardware and the handlers are maintained by the operating system.But these two things don’t seem to be enough. What if a process is running and the user hits ctrl-c? How does the process know that it needs to be terminated? So we need some kinds of mechanism for the process to know and act upon some specifi events.SignalA signal is a small message that notifies a process that an event of some type has occurred in the system.  akin to exceptions and interrupts (like a software version of exception/interrupt)  sent from the kernal (sometimes at the request of another process) to a process  signal type is identified by small integer ID’s (1-30)  only info in a signal is its ID and the fact that it arrivedSending a Signal  Kernal sends (delivers) a signal to a destination process by updating some state in the context of the destination process (flip a bit in the process)  Kernal sends a signal for one of the following reasons:          Kernal has detected a system event such as divide-by-zero (SIGFPE) or the terminal of a child process (SIGCHLD). SIGCHLD is used to implement background process handling for the shell      Another process has invoked the kill system call explicitly request the kernal to send a signal to the destination process (kill means sending signals)      Sending a signal means flipping a bit corresponding to a signal in a process (pending signal mask). If multiple signals are sent, the bit stays on. So a flipped bit means one or more signals is received. It doesn’t cause the process to do actions reponding to the signal; it just delivers a message that a signal is received. A process acts upon a signal upon return to user mode from supervisor mode. When the process gets back to user mode, it walks through the signal table; if one of the bits is set, it clears the bit and calles the corresponding signal handler.Receiving a SignalA destination process receives a signal when it is forced by the kernal to react in some way to the delivery of the signalThree possible ways to react:  Ignore the signal (reset the bit and do nothing)  Terminate the process (with optional core dump)  Catch the signal by executing a user-level function called signal handler          Akin to a hardware exception handler being called in response to an asynchronous interrupt      Pending and Blocked SignalsA signal is pending if sent but not yet received (not yet act upon it)  There can be at most one pending signal of any particular type (only one bit)A process can block the receipt of certain signals  Blocked signals can be delivered, but will not be received until the signal is unblockedProcess Data StructuresSo for each process, there are 3 corresponding data structures:  An array of signal handlers  Pending signal mask  Blocking signal maskSo the process will act upon a signal if the pending signal mask is on and blocking signal mask is off.Process GroupsEvery process belongs to exactly one process groupThere is exactly group leader of each group, which is the first process that is put into the group. It is nothing special except that its pid is the groupid. Each process group has only one input and one output. There can be only one foregroup process group and many background process groups for each session, which is an abstraction that associated with a text input device (keyboard) and a text output device (screen). That means only one process group can interact with the keyboard.Some Examples of Sending SignalsSo /bin/kill is the program that can send arbitrary signal to a process or process group, e.g. (negative pid is groupid)Typing ctrl-c/ctrl-z sends a SIGINT/SIGTSTP to every job in the foreground process groupThere are default actions corresponding to each signals, but we can write our own signal handler but implementing the signal functionhandler_t *signal(int signum, handler_t *handler)",
        "url"      : "/2017/06/26/Signals/",
        "date"     : "2017-06-26 10:51:00 +0000"
    } ,
    
    {
        "title"    : "Bit Hacks",
        "category" : "Algorithm",
        "tags"     : "Algorithm",
        "content": "  Find the minimum r of two integers x and y  Modular Addition  Round up to a Power of 2  Least-Significant 1  Log Base 2 of a Power of 2  Population Count  Queens ProblemFind the minimum r of two integers x and yr = r ^ ((x ^ y) &amp; -(x &lt; y));Modular AdditionCompute (x + y) mod n, assuming that 0 &lt;= x &lt; n and 0 &lt;= y &lt; n.z = x + y;r = z - (n &amp; -(z &gt;= n));Round up to a Power of 2Compute $2^{\\left \\lceil{\\log n}\\right \\rceil }$// 64-bit integers--n;n |= n &gt;&gt; 1;n |= n &gt;&gt; 2;n |= n &gt;&gt; 4;n |= n &gt;&gt; 8;n |= n &gt;&gt; 16;n |= n &gt;&gt; 32;++n;Decrement and increment handles the case where n is already a power of 2.Least-Significant 1Compute the mask of the least-significant 1 in word x.r = x &amp; (-x);Log Base 2 of a Power of 2Compute log x, where x is a power of 2const uint64_t deBruijn = 0x022fdd63cc95386d;const unsigned int convert[64] = {0,  1,  2, 53,  3,  7, 54, 27,4, 38, 41,  8, 34, 55, 48, 28,62,  5, 39, 46, 44, 42, 22,  9,24, 35, 59, 56, 49, 18, 29, 11,63, 52,  6, 26, 37, 40, 33, 47,61, 45, 43, 21, 23, 58, 17, 10,51, 25, 36, 32, 60, 20, 57, 16,50, 31, 19, 15, 30, 14, 13, 12,};r = convert[(x*deBruijn) &gt;&gt; 58];Why it works?A deBruijn sequence $s$ of length $2^k$ is a cyclic 0-1 sequence such that each of the $2^k$ 0-1 strings of length $k$ occurs exactly once as a substring of $s$.Example k=3  0b000111010   0001    0012     0113      1114       1105        1016         0107          100convert[8] = {0, 1, 6, 2, 7, 5, 4, 3};0b0011101 * 2^4 = 0b11010000;0b11010000 &gt;&gt; 5 = 6;convert[6] = 4;Population CountCount the number of 1 bits in a word x// Create masksB5 = (-1) ^ ((-1) &lt;&lt; 32);B4 = B5 ^ (B5 &lt;&lt; 16);B3 = B4 ^ (B4 &lt;&lt; 8);B2 = B3 ^ (B3 &lt;&lt; 4);B1 = B2 ^ (B2 &lt;&lt; 2);B0 = B1 ^ (B1 &lt;&lt; 1);// Compute popcountx = ((x &gt;&gt; 1) &amp; B0) + (x &amp; B0);x = ((x &gt;&gt; 2) &amp; B1) + (x &amp; B1);x = ((x &gt;&gt; 4) + x) &amp; B2;x = ((x &gt;&gt; 8) + x) &amp; B3;x = ((x &gt;&gt; 16) + x) &amp; B4;x = ((x &gt;&gt; 32) + x) &amp; B5;Queens ProblemPlace n queens on an n x n chessboard so that no queen attacks another.Board representation:  array of $n^2$ bytes?  array of $n^2$ bits?  array of $n$ bytes?  3 bitvectors of size $n,2n-1,2n-1$, each is enough to put in a word (unsigned int).The n-bit vector down represent whether a queen is in a given column. So placing a queen in column c is not safe if down &amp; (1 &lt;&lt; c) is nonzero.The 2n-1 bit vector right represent whether a queen is in a given diagonal. So placing a queen in row r and column c is not safe if right &amp; (1 &lt;&lt; (n - r + c)) is nonzero.The 2n-1 bit vector left represent whether a queen is in a given reverse diagonal. So placing a queen in row r and column c is not safe if left &amp; (1 &lt;&lt; (r + c)) is nonzero.",
        "url"      : "/2017/06/20/Bit-Hacks/",
        "date"     : "2017-06-20 12:22:00 +0000"
    } ,
    
    {
        "title"    : "Cache Oblivious Algorithms",
        "category" : "Algorithm",
        "tags"     : "Algorithm",
        "content": "  Introduction to Cache Oblivious Algorithms          The Cache Model      Cache Oblivious Algorithm      Basic Algorithms      Divide and Conquer      Order Statistics (median finding)                  Analysis                    Matrix Multiplication                  Standard, Naive Way          Black Algorithm                    Static Search Tree (Binary Search)                  Analysis: $O(\\log{B}N)$ memory transfers                    Cache aware sorting      Cache Oblivious Sorting                  K-funnel          Revisit K-funnel                    Introduction to Cache Oblivious AlgorithmsCache oblivious algorithms are the algorithms that performs well even when they are not aware of the caching model.The Cache ModelWe assume two level memory: cache and main memory. The block size is $B$. The size of the cache is $M$. The size of main memory is assume infinte. If we want to access some data that is not in the cache, we need to pull the whole block that contains the data to the cache first. And we might kick something else and we need to write it back to memory.  Accesses to cache are free, but we still care about the computation time (work) of the algorithm itself.  If we have an array, it might not be aligned with the blocks. So it will be some extra things at the beginning and the end that doesn’t consume a whole block but we need the extra 2 blocks.  Count block memory transfers between cache and main memory. (number of block read/write) Memory Transfer $MT(N)$ is a function of $N$, but $B$, $M$ are parameters and does matter.Cache Oblivious Algorithm  Algorithms don’t know $B$ and $M$  Accessing memory automatically fetch block into memory &amp; kick the block that will be used furthest in the future (idealized model)  Memory is one big array and is divided into blocks of size $B$Note: if the algorithm is cache oblivious and is efficient in the 2-level model, it will be efficient for k-level model (L1, L2, L3, … caches)Basic Algorithms  Single Scanning    Scanning(A, N)  for i from 0 to N      visit A[i]        e.g. sum array.  $MT(N)=\\frac{N}{B}+2=O(\\frac{N}{B}+1)$ For +2, see the second point in the cache model.  $O(1)$ parallel scans    reverse(A,N)  for i from 0 to N/2      exchange A[i] with A[N-i+1]        Assuming $\\frac{M}{B}&gt;=2,\\ MT(N)=O(\\frac{N}{B}+1)$    Binary SearchWe hope to get $\\log_BN=\\log N/\\log B$, but actually it is $\\log(\\frac{N}{B})=\\log N-\\log B$ (At first each access corresponding to one block. At the very last few search they are in the same block)Divide and Conquer  Algorithm divides problem into $O(1)$ case  Analysis considers point at which the problem          fits in cache ($&lt;=M$)      fits in $O(1)$ block ($O(B)$)      Order Statistics (median finding)  Conceptually partition the array into $N/5$ 5-tuples  Compute the medium of each one ($MT(N)=O(\\frac{N}{B}+1)$)  Recursively compute median $x$ of these medians ($MT(N)=O(N/5)$)  Partition around $x$ ($MT(N)=O(\\frac{N}{B}+1)$)  Recurse in one side ($MT(N)=O(7N/10)\\approx O(3N/4)$)    Analysis    $MT(N)=MT(N/5)+MT(3N/4)+O(\\frac{N}{B}+1)$  If we assume $MT(1)=O(1)$, see what we get ($L(N)$ is the number of leaves of recurssion tree):\\[L(N)=L(N/5)+L(3N/4)\\\\L(1)=1\\\\Suppose\\ it\\ is\\ N^{\\alpha}\\\\N^{\\alpha}=(N/5)^{\\alpha}+(3N/4)^{\\alpha}\\\\1=(1/5)^{\\alpha}+(3/4)^{\\alpha}\\\\\\alpha\\approx0.8398\\\\L(N)=N^{0.8398}=\\omega(\\frac{N}{B})\\ if\\ B=N^{0.2}\\]Now we assume $MT(B)=O(1)$, then number of leaves is $(\\frac{N}{B})^{\\alpha}=O(\\frac{N}{B})$Matrix MultiplicationStandard, Naive Way$C=AB$ Assume that $A$ is row-major, $B$ is col-major, $C$ is row-major (best possible memory layout)$O(\\frac{N}{B})$ to compute $C_{ij}$ so total $O(N^3/B)$Black AlgorithmRecursively divide the matrix into 4 parts, each is $N/2\\times N/2$We assume we store matrices recursively block. So:\\[MT(N)=8MT(N/2)+O(N^2/B+1)\\\\MT(B)=1\\\\MT(c\\sqrt{M})=\\frac{M}{B}\\]So we stop the recursion tree at the base case, the number of leaves is $O((N/\\sqrt{M})^3)$So the total is\\[(N/\\sqrt{M})^3\\centerdot \\frac{M}{B}=N^3/(B\\sqrt{M})\\]Static Search Tree (Binary Search)Goal: $\\log_BN$  Store %N% elements in order in a complete binary tree on %N% nodes  Cut the tree in the middle level of edges so so that the each part has height $(\\log N)/2$ and the upper part has $2^{(\\log N)/2}$ nodes which is $sqrt{N}$, and there are $\\sqrt{N}$ subtrees at the bottom, each of which has size $\\sqrt{N}$. So there are $\\sqrt{N}+1$ subtrees in total.  Recursively layout $\\sqrt{N}+1$ subtrees and concatenatee.g. the index label is the order that is stored in the array.            1     2             3  4     7     10       135   6 8   9 11  12   14  15Analysis: $O(\\log{B}N)$ memory transfers  consider recursive level of detail at which the size of each subtree $&lt;=B$, so the height of the subtree $&gt;=\\log B$  root-to-node path visits $&lt;=\\log N/((\\log B)/2)$ subtrees  each subtree is in at most $2$ blocks  So the number of memory transfers $&lt;=4\\log_BN=O(\\log_BN)$  There exist a dynamic type of this data structure, which can do insert and delete in $O(\\log_BN)$ time, but it is super complicatedCache aware sorting  repeated insertion into B-tree, $MT(N)=O(N\\log_BN)$ really bad!!! even worse than random access!!! which is $O(N)$  binary merge sort\\[MT(N)=2MT(N/2)+O(\\frac{N}{B})\\\\MT(cM)=O(\\frac{M}{B})\\\\MT(N)=\\frac{N}{B}\\centerdot\\log(N/M)\\]  $\\frac{M}{B}$-way merge sort          divide into $\\frac{M}{B}$ subarrays      recursively sort each subarray      merge: using 1 cache block per subarray      \\[MT(N)=\\frac{M}{B}\\centerdot MT(N/(\\frac{M}{B}))+O(\\frac{N}{B})\\\\MT(cM)=O(\\frac{M}{B})\\\\MT(N)=\\frac{N}{B}\\centerdot\\log_{\\frac{M}{B}}(\\frac{N}{B})\\]Cache Oblivious SortingActually need an assumption of the cache:\\[M=\\Omega(B^{1+\\epsilon})\\ for\\ \\epsilon&gt;0\\]e.g. $M&gt;=B^2$Use $N^{\\epsilon}$-way mergesrotK-funnelIt merges k sorted lists of total size $&gt;=\\Theta(k^3)$ using $O(k^3/B\\log_{\\frac{M}{B}}k^3/B+k)$ memory transfersSo we have now Funnelsort. use $k=N^{1/3}$  divide array into $N^{1/3}$ equal segments  recursively sort each  merge using $N^{1/3}$-funnel\\[MT(N)=N^{1/3}MT(N^{2/3})+O(\\frac{N}{B}\\centerdot\\log_{\\frac{M}{B}}(\\frac{N}{B})+N^{1/3})\\\\MT(cM)=O(\\frac{M}{B})\\\\N=\\Omega(M)=\\Omega(B^2)\\\\\\frac{N}{B}=\\Omega(\\sqrt{N})=\\Omega(\\sqrt[3]{N})\\]So as long as it is not in the base case, $\\frac{N}{B}\\centerdot\\log_{\\frac{M}{B}}(\\frac{N}{B})$ dominates $N^{1/3}$\\[MT(N)=\\frac{N}{B}\\centerdot\\log_{\\frac{M}{B}}(\\frac{N}{B})\\]Revisit K-funnelFirst take a look at space excluding input and output buffers\\[S(k)=(\\sqrt{k}+1)S(\\sqrt{k})+O(k^2)S(k)=O(K^2)\\]So merge means to fill up the top buffer  merge two children buffer as long as they both are non-empty  whenever one empties, recursively fill it  at leaves, read from input listAnalysis:  consider first recursive level of detail, J, at which evey J-funnel fits 1/4 of the cache (It means $cJ^2&lt;=M/4$)  can also fit one block per input buffer (of J-funnel)\\[J^2&lt;=\\frac{1}{4}M\\Rightarrow J&lt;=\\frac{1}{2}\\sqrt{M}\\\\B&lt;=\\sqrt{m}\\\\\\Rightarrow J\\centerdot B&lt;=\\frac{1}{2}M\\]  swapping in (reading J-funnel and one block per input buffer) ($O(\\frac{J^2}{B}+J)=O(J^3/B$)  when input buffer empties, swap out and recursively fill and swap back in. Swapping back in cost $O(J^3/B)$  charge cost to elements that fill the buffer (amortize analysis)  $J^3$ such elements  number of charges of $O(1/B)$ cost to each elements is $\\log K/\\log J=\\Theta(\\log K/\\log M)$So the total cost is\\[O(\\frac{k^3}{B}\\frac{\\log k}{\\log M}+k)\\\\=O(\\frac{k^3}{B}\\log_{M/B}\\frac{k}{B}+k)\\]assuming\\[k=\\Omega(M)=\\Omega(B^2)k/B=\\Omega(\\sqrt{k})\\]",
        "url"      : "/2017/06/11/Cache-Oblivious-Algorithms/",
        "date"     : "2017-06-11 10:41:00 +0000"
    } ,
    
    {
        "title"    : "Basic Quantum Computing",
        "category" : "Algorithm",
        "tags"     : "Algorithm, Quantum",
        "content": "  Basic Quantum Computing with Least Physics Possible          Classical Probability Bit      Quantum Bit (qubit)      Some Notations      Quantum Transformations      Basic Quantum Gates (Between 2/3 qubits)      One Small Quantum Algorithm Example      Basic Quantum Computing with Least Physics PossibleClassical Probability BitWe will start by talking about classical probability bit. Here is the one bit representation $p_0$, $p_1$ are the probability of being $0$, $1$ respectively.\\[\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right)\\\\s.t.\\ p_0+p_1=1,\\ p_0,p_1\\in\\mathbb{R},\\ 0\\leq p_0,p_1\\leq1\\]And here is the n-bits representation\\[\\left( \\begin{array}{c}         p_{0\\cdots00} \\\\         p_{0\\cdots01} \\\\        p_{0\\cdots10} \\\\        \\vdots \\\\        p_{1\\cdots11} \\end{array} \\right)\\in\\mathbb{R}^{2^n}\\\\        s.t.\\ \\sum p_x=1,\\ p_x\\in[0,1]\\in\\mathbb{R},\\ x\\in\\{0,1\\}^n\\]So we can do some transformation to the bit:\\[\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right)\\xrightarrow{I}\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right)\\]\\[\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right)\\xrightarrow{NOT}\\left( \\begin{array}{c} p_1 \\\\ p_0 \\end{array} \\right)\\]Or if we have two bits\\[\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right),\\left( \\begin{array}{c} p_0 \\\\ p_1 \\end{array} \\right)\\]and want to do an XOR with two dependent bits\\[\\left( \\begin{array}{c} p_{00} \\\\ p_{01} \\\\ p_{10} \\\\ p_{11} \\end{array} \\right)\\xrightarrow{XOR}\\left( \\begin{array}{c} p_0 \\\\ 0 \\\\ 0 \\\\ p_1 \\end{array} \\right)\\]But it is not possible (physically) to do something like this, which kind of does XOR with two independent bits:\\[\\left( \\begin{array}{c} p_{00} \\\\ p_{01} \\\\ p_{10} \\\\ p_{11} \\end{array} \\right)\\xrightarrow{XOR'}\\left( \\begin{array}{c} p_0\\centerdot p_0 \\\\ p_0\\centerdot p_1 \\\\ p_1\\centerdot p_0 \\\\ p_1\\centerdot p_1 \\end{array} \\right)\\]In other words, physically, in our universe, we can only do $l_1$ norm linear tranformation.Quantum Bit (qubit)For qubit, we are able to do $l_2$ norm. A single qubit is difine as follows:\\[\\left( \\begin{array}{c} \\alpha \\\\ \\beta \\end{array} \\right)\\in\\mathbb{C}^2\\\\s.t.\\ \\lvert\\alpha\\rvert^2+\\lvert\\beta\\rvert^2=1,\\ \\lvert\\alpha\\rvert:=\\sqrt{\\alpha\\centerdot\\overline{\\alpha}}\\\\Prob[m=0]=\\lvert\\alpha\\rvert^2=\\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right)\\\\Prob[m=1]=\\lvert\\beta\\rvert^2=\\left( \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right)\\]Certainly we can do transformation to one qubit. In this case, the transformation we have is $T\\in\\mathbb{C}^{2\\times2}$. Before talking about transformations, we first introduce k-qubits as follows\\[\\left( \\begin{array}{c}         \\alpha_{0\\cdots00} \\\\         \\alpha_{0\\cdots01} \\\\        \\alpha_{0\\cdots10} \\\\        \\vdots \\\\        \\alpha_{1\\cdots11} \\end{array} \\right)\\in\\mathbb{C}^{2^k}\\\\        s.t.\\ \\sum_{x\\in\\{0,1\\}^k} \\lvert\\alpha_x\\rvert^2=1\\]and our transformation becomes $T\\in\\mathbb{C}^{2^k\\times2^k}$Some Notations\\[v=\\left( \\begin{array}{c}         \\alpha_1 \\\\         \\alpha_2 \\\\        \\vdots \\\\        \\alpha_d \\end{array} \\right)\\in\\mathbb{C}^{d}\\\\v^{\\dagger}=(\\bar{\\alpha_1},\\cdots,\\bar{\\alpha_d})\\\\&lt;v,w&gt;=v^{\\dagger}\\centerdot w=\\overline{&lt;w,v&gt;}=&lt;v|w&gt;\\\\v\\perp w\\Leftrightarrow v^{\\dagger}w=0\\\\\\Vert v\\Vert=\\sqrt{\\sum_{i=1}^{d}\\alpha_i \\bar{\\alpha_i}}=v^{\\dagger}v\\\\T=\\left( \\begin{array}{c}         T_{11} &amp; \\cdots &amp; T_{1d} \\\\         \\vdots &amp; \\ddots &amp; \\vdots \\\\        T_{d1} &amp; \\cdots &amp; T_{dd} \\end{array} \\right)\\\\\\bar{T}=\\left( \\begin{array}{c}         \\overline{T_{11}} &amp; \\cdots &amp; \\overline{T_{d1}} \\\\         \\vdots &amp; \\ddots &amp; \\vdots \\\\        \\overline{T_{1d}} &amp; \\cdots &amp; \\overline{T_{dd}} \\end{array} \\right)\\\\\\left( \\begin{array}{c}         \\alpha_0 \\\\         \\alpha_1         \\end{array} \\right)\\otimes\\left( \\begin{array}{c}         \\alpha_0 \\\\         \\alpha_1 \\\\        \\end{array} \\right)=\\left( \\begin{array}{c}         \\alpha_0\\beta_0 \\\\         \\alpha_0\\beta_1 \\\\        \\alpha_1\\beta_0 \\\\         \\alpha_1\\beta_1 \\\\        \\end{array} \\right)\\\\\\mathbb{C}^k\\otimes\\mathbb{C}^l=\\mathbb{C}^{kl}\\\\if\\ |\\tilde{\\Phi}&gt;=U|\\Phi&gt;,|\\tilde{\\Psi}&gt;=U|\\Psi&gt;,then\\ &lt;\\Phi|\\Psi&gt;=&lt;\\tilde{\\Phi}|\\tilde{\\Psi}&gt;and\\ &lt;\\tilde{\\Phi}|=&lt;\\Phi|U^{\\dagger}\\\\|0&gt;:=\\left( \\begin{array}{c}         1 \\\\         0  \\\\    \\end{array} \\right)\\\\|1&gt;:=\\left( \\begin{array}{c}         0 \\\\         1  \\\\    \\end{array} \\right)\\\\|+&gt;:=\\left( \\begin{array}{c}         1/\\sqrt{2} \\\\         1/\\sqrt{2}  \\\\    \\end{array} \\right)=1/\\sqrt{2}(|0&gt;+|1&gt;)\\\\|-&gt;:=\\left( \\begin{array}{c}         1/\\sqrt{2} \\\\         -1/\\sqrt{2}  \\\\    \\end{array} \\right)=1/\\sqrt{2}(|0&gt;-|1&gt;)\\\\H:=1/\\sqrt{2}\\left( \\begin{array}{c}         1 &amp; 1 \\\\         1 &amp; -1  \\\\    \\end{array} \\right)\\\\H|0&gt;=|+&gt;\\\\H|1&gt;=|-&gt;\\\\H|+&gt;=|0&gt;\\\\H|-&gt;=|1&gt;\\\\|X,Y&gt;:=|X&gt;\\otimes|Y&gt;\\\\if\\ f:\\{0,1\\}^k\\rightarrow\\{0,1\\},then\\ |X,Z&gt;\\xrightarrow{U_f}|X,Z+f(X)&gt;\\]Quantum Transformations$T$ is a quantum transformation iff  $T$ is linear: $T\\in\\mathbb{C}^{d\\times d}$  $T$ is norm preserving $\\Leftrightarrow\\forall v\\in\\mathbb{C}^d,\\Vert v\\Vert^2=\\Vert Tv\\Vert^2\\Rightarrow T\\centerdot T^{\\dagger}=I$Basic Quantum Gates (Between 2/3 qubits)\\[SWAP:=\\left( \\begin{array}{c}         1 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 1 &amp; 0\\\\         0 &amp; 1 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 1    \\end{array}\\right)\\\\i.e.\\ (X,Y)\\xrightarrow{SWAP}(Y,X)\\\\CNOT:=\\left( \\begin{array}{c}         1 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 1 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 1\\\\         0 &amp; 0 &amp; 1 &amp; 0     \\end{array}\\right)\\\\i.e.\\ (X,0/1)\\xrightarrow{CNOT}\\left\\{ \\begin{array}{rcl}(X,0/1) &amp; \\mbox{if} &amp; X=0 \\\\ (X,1/0)  &amp; \\mbox{if} &amp; X=1 \\\\\\end{array}\\right.\\\\CCNOT:=\\left( \\begin{array}{c}         1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\         0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\         0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\     \\end{array}\\right)\\\\i.e.\\ (X,Y,Z)\\xrightarrow{CCNOT}(X,Y,Z\\oplus X\\wedge Y)\\\\(X,Y,0)\\xrightarrow{CCNOT}(X,Y,X\\wedge Y)\\\\\\]Note: all transformations are inversible. Specifically, for the above 3 trans., the inverse of them are themselves. Namely, apply it twice and we will be back to the original position.To implement some classical function $f$, we need the input bits as well as some extra bits for input to have extra space to work with.One Small Quantum Algorithm ExampleHere we will talk about a small example of quantum algorithms, just to get a feel of how quantum gates works and why it is sometimes more efficient than classical algorithms.Deutsch ProblemGiven \\(f\\:\\{0,1\\}\\rightarrow\\{0,1\\}\\), we ask if \\(f(0)=f(1)\\).Classically we have to evaluate twice: both \\(f(0)\\) and \\(f(1)\\).Quantumly we can solve it with one transformation and evaluate it.$input:\\vert0,1&gt;$  $\\vert0,1&gt;\\xrightarrow{H}\\vert+,-&gt;$  $\\vert+,-&gt;\\xrightarrow{U_f}???$So\\[|b,-&gt;\\xrightarrow{U_f}\\cdots\\rightarrow|(-1)^{f(b)}b,-&gt;\\\\And\\\\|+,-&gt;\\xrightarrow{U_f}1/\\sqrt{2}((-1)^{f(0)}\\centerdot|0&gt;+(-1)^{f(1)}\\centerdot|1&gt;)\\otimes|-&gt;\\rightarrow\\\\\\left\\{ \\begin{array}{rcl}\\pm|+&gt; &amp; \\mbox{if} &amp; f(0)=f(1) \\\\ \\pm|-&gt;  &amp; \\mbox{if} &amp; f(0)\\neq f(1) \\\\\\end{array}\\right.\\]So if we measure the result, we will see $”0”\\ iff\\ f(0)=f(1)$, and $”1”\\ iff\\ f(0)\\neq f(1)$",
        "url"      : "/2017/06/10/Basic-Quantum-Computing/",
        "date"     : "2017-06-10 10:05:00 +0000"
    } ,
    
    {
        "title"    : "赫炎前四章感想",
        "category" : "Random",
        "tags"     : "赫炎",
        "content": "打完四章多一些，先说说感想吧。赫炎描绘了一个破败，腐烂不堪的社会。在这无可救药的社会里，生活着各式各样的人。卖鸦片的章鱼老鸨，情报屋的双子，每天挖地洞的疯子，当然还有我们的男主一家。不得不说赫炎对人物的刻画实在是非常不错，寥寥几句对话就能让你对每个人物的性格特征熟记于心，本命アティ！正也因为这人物刻画，玩家能从这原本腐朽的社会看到了一丝丝光明。很多对话以及心理描写玩家看了能会心一笑，实在是非常不容易。总的来说，整个气氛用一个词来形容就是破败感，残缺不堪的社会以及人们。这也衬托出了所谓人类的美丽。就算是生活在这最底层的异形的社会中，人类总是充满着耀眼的光辉。作者的文笔以及BGM还是非常不错的。当然最坑爹的就是战斗场面基本复制黏贴。。笔墨都用来写别的了，嘛倒也无可厚非。",
        "url"      : "/2017/06/09/%E8%B5%AB%E7%82%8E%E5%89%8D%E5%9B%9B%E7%AB%A0%E6%84%9F%E6%83%B3/",
        "date"     : "2017-06-09 23:06:00 +0000"
    } ,
    
    {
        "title"    : "Fibonacci Heap",
        "category" : "Algorithm",
        "tags"     : "Algorithm",
        "content": "  Some Notes about Fibonacci Heaps          Operations:      Amortized Time Complexity      Structure      Representation of F-Heap      Definitions      Idea      Implementation of Operations                  delete_min(h)          decrease_key(delta, i, h)          delete(i, h)                    Additional Details (Cascade Cut)      Marking Nodes      Crucial Properties      Observations      Redefinition      Revist decrease_key      Revist delete      Summary      Some Notes about Fibonacci HeapsOperations:  make_heap Return an empty heap  insert(i,h) Add item $i$ to heap $h$.  find_min(h) Return the smallest item in heap $h$  delete_min(h) Delete min from heap $h$ and return it  meld(h1, h2) Return the heap formed by putting all elements in $h_1$ with all the elements in $h_2$. $h_1$ and $h_2$ are destroyed.  decrease_key(delta, i, h) Assume that position of $i$ is known. Decrease the value of item $i$ of delta (delta &gt; 0)  delete(i, h) Assume that position of $i$ is known. Delete $i$ from heapAmortized Time Complexity  delete_min(h) and delete(i, h) takes $O(\\log n)$ time  Other operations take $O(1)$ timeStructure  Heap Ordered Trees: Rooted tree containing a set of item, 1 item / node, with items arranged in heap order  Heap Order: Key of item $x$ is no less than the key of the item in its parent $p(x)$, provided $x$ has a parent.  Linking: Combine two item-disjoint trees into one. (Make one root (bigger) a child of the other (smaller))  Fibonacci heap (F-heap): Collection item disjoint heap ordered trees. (the algorithms impose an additinoal constraint).  rank $r(x)$: Number of children of node $x$. It turns out that if $x$ has $n$ descendants, the number of children is at most $\\log n$.  Nodes will be marked or unmarked.Representation of F-Heap  Each node contains pointer to its parent (or to $null$ if it doesn’t have one).  Each node has a pointer to one of its children (or to $null$ if it doesn’t have any).  The children of each node are in a doubly linked circular list.  Node has its rank $r(x)$ and a bit indicating its mark.  All the roots of the (sub)heaps are in a circular list.  There is a pointer to a root containing an item of minimum key ($minimum\\ node$ of the F-heap).Definitions  $S$: Collection of Heaps.  $\\Phi(S)$: Potential of $S$.  $m$ operations with times $t_1,t_2,\\cdots,t_m$  $a_i$ amortized time for operation $i$.  $\\Phi_i$: Potential after operation $i$.  $\\Phi_0$: Initial potential.  $\\sum t_i=\\sum(a_i-\\Phi_i+\\Phi_{i-1})=\\Phi_0-\\Phi_m+\\sum a_i$  $\\Phi_0$ is initially zero.  $\\Phi_i$ is non-negative.Idea  Potential of a collection of heaps: The total number of trees they contain.  Initial potential is zero  make_heap, find_min, insert, and meld take $O(1)$ time.  Insertion increases the number of trees by one. And other operations do not affect the number of trees  delete_min: Amorzied time $O(\\log n)$ where $n$ is the number of items in the heap. Increases the number of trees by at most $\\log n$.  Linking Step: Decreases the number of trees by one.Implementation of Operations  make_heap Just retur $null$  find_min(h) Return the minimum node of $h$  insert(i, h) Create a heap of only node $i$ and replace $h$ by meld of $h$ and the new heap  meld(h1, h2) Combine the root lists of $h_1$ and $h_2$ into one list and set the minimum node to the appropriate new minimum node.delete_min(h)  Remove the minimum node ($x$) from $h$  Concatenate the list of children of $x$ with the list of roots of $h$ (other than $x$)  Repeat the following Linking Step until it no longer applies          Find any two trees whose roots have the same rank and link them (the new root has rank +1)        Form a list of the remaining roots.  Find the item of minimum key.Note: Implementation (use an array indexed by ranks).decrease_key(delta, i, h)  Key of item $i$ is decreased by delta  Cut out the node $x$ containing item $i$ from its parent  $x$ and its descendants is added as a new tree of $h$.  The appropriate update operations are performeddelete(i, h)  Find node $x$ containing item $i$.  Cut out the node $x$ containing item $i$ from its parent.  Form a new list of roots by concatenating the list of children of $x$ with the original list of roots.  The appropriate update operations are performed.  delete takes $O(1)$, except when the minimum element is deleted.Additional Details (Cascade Cut)  When node $x$ has been made a child of another node by a linking step and it loses 2 of its children through cuts, we cut the edge joining $x$ and its parent and we make $x$ ad new root (as in decrease_key)  A decrease_key or delete operation my casue a possibly large number of cascading cuts.Marking Nodes  Purpose: Keep track of where to make cascade cuts.  Unmark $x$: When making a root node $x$ a child of another node in a linking step  When curring edge joining $x$ and its parent $(p(x))$, we decrease the rank of $p(x)$ and check it $p(x)$ is a root          if $p(x)$ is not a root, we mark it if it is unmarked and cut the edge to its parent it it is marked. (latter cut my be cascading)        Each cut takes $O(1)$.Crucial Properties  Each tree in an F-heap has a size at lease exponential in the rank of its root. i.e. the number of children is at most $\\log n$.  The number of cascading cutus that take place during a sequence of heap operations is bounded by the number of decrease key and delete operations.Observations  The purpose of cascade cuts in to preserve property 1.  Loss of two children rule limits the frequency of cascade cuts.Lemma 1: Let $x$ be any node in a F-heap. Arrange the children of $x$ in the order they were linked to $x$, from earliest to latest. Then the $i^{th}$ child of $x$ has rank of at least $i-2$.Corollary 1: A node of rank $k$ in an F-heap has at least $F_{k+2}\\geq\\phi^k$ descendants, including itself, where $F_k$ is the $k^{th}$ Fibonacci number and $\\phi$ is the golden ratio.Redefinition  Potential: Total numbere of trees plus twice the number of marked nonroot nodes.  The bounds of $O(1)$ for make_heap, find_min, insert, and meld remain valid, as does not $O(\\log n)$ bound for delete_min.  delete_min(h): increases the potential by at most $1.4404\\log n$ minus the number of linking steps, since, if the minimum node has rank $k$, then $\\phi_k\\leq n$ and thus\\[k\\leq\\log n/\\log\\phi\\leq1.4404\\log n\\]Revist decrease_key  Causes potential to increase by at most three mins the number of cascading cuts, since          the first cut converts a possible unmarked nonroot node into a root      each cascading cut converts a marked nonroot node into a root      the last cut can convert a nonroot node from unmarked to marked        It follows that decrease key has an $O(1)$ amortized time bound.Revist deleteJust combine the analysis of decrease_key with delete_minSummaryIf we begin with no F-heaps and perform an arbitrary sequence of F-heap operations, then the total time is at most the total amortized time, where the amortized time is $O(\\log n)$ for each delete_min or delete, and $O(1)$ for each other operations.",
        "url"      : "/2017/06/09/Fibonacci-Heap/",
        "date"     : "2017-06-09 10:05:00 +0000"
    } ,
    
    {
        "title"    : "Introduction to Algorithm",
        "category" : "Algorithm",
        "tags"     : "Algorithm",
        "content": "  Some Notes of Introduction to Algorithm          Fiboniacci Number      Order Statistics      Hash Functions                  Division Method          Multiplication Method          Universal Hashing          Perfect Hashing                    Augmented Data Structures                  Dynamic Order Statistics          Interval Tree                    Amortized Analysis                  Potential Method                    Competitive Analysis      Karp-Rabin Algorihm: Find s in t      Some Notes of Introduction to AlgorithmFiboniacci Number\\[\\begin{bmatrix}    F_{n+1} &amp; F_{n} \\\\    F_{n} &amp; F_{n-1} \\\\\\end{bmatrix}=\\begin{bmatrix}    1 &amp; 1 \\\\    1 &amp; 0 \\\\\\end{bmatrix}^n\\]$Running\\ Time = \\theta(\\log_2(n))$Order StatisticsGiven n elements in an array, find $k^{th}$ smallest element.  Quick Select          Expected running time $\\theta(n)$      Worse case $\\theta(n^2)$        Worse-case linear time order statistics    Select(i, n)1. Divide n elements into [n/5] groups of 5 elements each. Find the median of eachgroup. O(n)2. Recurrsively select the medium x of the [n/5] group medians. T(n/5)3. Partition with x as pivot, let k = rank(x). O(n)4. if i==k then return x  if i&lt;k then recurrsively select ith smallest element in left part  else then recurrsively select (i-k)th smallest element in upper part      Hash FunctionsDivision Method$h(k) = k\\ mod\\ m$pick $m$ to be prime and not too close to power of $2$ or $10$.Multiplication Method$h(k)$ $=$ $A\\cdot k$ $mod$ $2^w$ » $(w - r)$, $A\\ odd\\land2^{w-1}$ &lt; $A$ &lt; $2^w$Universal HashingLet $u$ be a universe of keys, and let $H$ be a finite colleciton of hash functionsmapping $U$ to {$0,1,\\dots,m-1$}.$H$ is $universal$ if $\\forall x,y\\in U,x\\ne y$\\[\\lvert\\{h\\in H;h(x)=h(y)\\}\\rvert=\\lvert H\\rvert/m\\]i.e. if $h$ is chosen randomly from $H$, the probability of collision between $x$and $y$ is $1/m$.Perfect HashingGiven $n$ keys, construct a static hash table of size $m=O(n)$ such that searchingtakes $O(1)$ time in the worst case.Idea: 2 level scheme with universal hashing at both levels and NO collisions atlevel 2.if $n_i$ items that hashes to level 1 slot $i$, then use $m_i=n_i^2$ slots inthe level 2 table $S_i$.Augmented Data StructuresDynamic Order StatisticsSupports: Insert, Delete, Search(x), Select(i), Rank(x).Idea: use a R-B tree while keeping sizes of the subtree.$size[x]=size[left(x)]+size[right(x)]+1$Select(root, i):    k = size[left(x)] + 1 // k = rank(x)    if i == k then return x    if i &lt; k then return Select(left(x), i)    else return Select(right(x), i - k)$Running\\ Time = \\theta(\\log_2(n))$Interval TreeSupports: Intert, Delete, Interval-Search: Find an interval in the set that overlaps a given query interval.Idea: use a R-B tree while keeping the largest value $m$ in the subtree.\\[m[x]=max\\{high[int[x]], m[right(x)], m[left(x)]\\}\\]Interval-Search(i) // finds an interval that overlaps i    x = root    while x != nil and (low[i] &gt; high[int[x]] or low[int[x]] &gt; high[i]) do // i and int[x] don't overlap        if left[x] != nil and low[i] &lt;= m[left[x]] then x = left[x]        else x = right[x]    return xAmortized AnalysisPotential MethodFramework:  Start with data structure $D_0$  operation $i$ transforms $D_{i-1} \\to D_i$  cost of the operation is $c_i$  Define a potential function:\\[\\Phi:\\{D_i\\}\\to\\mathbb{R}\\ such\\ that\\ \\Phi(D_0)=0\\land\\Phi(D_i)\\geq0\\forall i\\]  Amortized cost $\\hat{c_i}$ with respect to $\\Phi$ is\\[\\hat{c_i}=c_i+\\Phi(D_i)-\\Phi(D_{i-1})\\]  Total amortized cost of n operations is\\[\\begin{align}\\sum_{i=1}^{n}\\hat{c_i}&amp;=\\sum_{i=1}^{n}(\\hat{c_i}+\\Phi(D_i)-\\Phi(D_{i-1}))\\\\&amp;=\\sum_{i=1}^{n}\\hat{c_i}+\\Phi(D_n)-\\Phi(D_0)\\\\&amp;\\geq\\sum_{i=1}^{n}c_i\\end{align}\\]Competitive AnalysisAn online algorithm A is $\\alpha$-$competitive$ if $\\exists k$ such that for anysequence of operations $S$,\\[Cost_A(S)\\leq\\alpha\\cdot C_{opt}(S)+k\\]where $C_{opt}(S)$ is the optimal, off-line, “God’s” algorithm.Karp-Rabin Algorihm: Find s in tRolling Hash ADT:  r.append(c): r maintains a string x where $r=h(x)$, add char c to the end of x  r.skip(): delete the first char of x. (assume it is c).Then just use ADT to “roll over” t to find s.Note: If their hashes are equal,there is still a probability $\\leq 1/\\lvert S\\rvert$ that they are actual not thesame string.To implement ADT: use hash simple hash function $h(k)=k\\bmod m$ where $m$ is a randomprime $\\geq\\lvert S\\rvert$We can treat $x$ as a multidigit number $u$ in base $a$, where $a$ is just the alphabetsize.So:  $r()=u\\bmod m$  $r$ stores $u\\bmod m$ and $\\lvert x\\rvert$, (really $a^{\\lvert x\\rvert}$), not $u$.r.append(c)    u = u * a + ord(c) mod m       = [(u mod p) * a + ord(c)] mod m      = [r() * a + ord(c)] mod mr.skip(c) // assume char c is skipped    u = [u − ord(c) * (pow(a, |u| - 1) mod p)] mod p      = [(u mod p) − ord(c) * (pow(a, |u| - 1) mod p)] mod p      = [r() − ord(c) * (pow(a, |u| - 1) mod p)] mod p",
        "url"      : "/2017/06/08/Introduction-to-Algorithm/",
        "date"     : "2017-06-08 19:47:00 +0000"
    } ,
    
    {
        "title"    : "Program Profiling",
        "category" : "Profiler",
        "tags"     : "Profiler, Slides",
        "content": "ProfilerYouTube Part1YouTube Part2",
        "url"      : "/2017/06/08/Program-Profiling/",
        "date"     : "2017-06-08 17:44:00 +0000"
    } ,
    
    {
        "title"    : "Dynamic Compilation",
        "category" : "Compiler",
        "tags"     : "Compiler, Slides",
        "content": "Dynamic Compilation Part1-3Dynamic Compilation Part4YouTube Part1YouTube Part2YouTube Part3YouTube Part4",
        "url"      : "/2017/06/08/Dynamic-Compilation/",
        "date"     : "2017-06-08 17:40:00 +0000"
    } ,
    
    {
        "title"    : "Interpreter Optimization",
        "category" : "Interpreter",
        "tags"     : "Interpreter, Slides",
        "content": "InterpreterYouTube",
        "url"      : "/2017/06/08/Interpreter-Optimization/",
        "date"     : "2017-06-08 17:37:00 +0000"
    } ,
    
    {
        "title"    : "Programming Language Virtual Machine",
        "category" : "Programming Languages, Virtual Machine",
        "tags"     : "Programming Languages, Virtual Machine",
        "content": "Virtual Machine(YouTube: Part 1/2)(YouTube: Simple class bytecode walk through)(YouTube: Part 2/2)",
        "url"      : "/2017/06/08/Programming-Language-Virtual-Machine/",
        "date"     : "2017-06-08 17:27:00 +0000"
    } ,
    
    {
        "title"    : "Some Slides about PaaS",
        "category" : "Cloud Computing",
        "tags"     : "PaaS, Slides",
        "content": "PaaS",
        "url"      : "/2017/06/08/Some-Slides-about-PaaS/",
        "date"     : "2017-06-08 17:09:00 +0000"
    } ,
    
    {
        "title"    : "Some Slides about Containers",
        "category" : "Cloud Computing",
        "tags"     : "Container, Slides",
        "content": "Anchor-The Rapid Rise of Containers in ProductionKubernetes",
        "url"      : "/2017/06/08/Some-Slides-about-Containers/",
        "date"     : "2017-06-08 17:09:00 +0000"
    } ,
    
    {
        "title"    : "Operating System Virtualization",
        "category" : "Operating System, Virtualization, Cloud Computing",
        "tags"     : "Virtualization",
        "content": "  Isolation  KVM          Intel VT-x      And then, there is memory      Final Notes on KVM        Xen          Xen and memory      Xen Paravirtualized I/O        Final Notes on Xen  Linux Containers  LXC  What’s in a Name?  cgroups  OS Virtualization versus Linux ContainersThese notes will attempt to describe how operating systems virtualization (henceforth to be termed ``virtualization’’) is implemented. There are essentially two approaches in use today: hardware assisted virtualization and paravirtualization. We’ll discuss them in terms of their most prevalent open source examples: KVM and Xen.Virtualization, like many terms associated with cloud computing, is often ill defined. For the purposes of this course (and as a good benchmark definition) virtualization isthe process of abstracting the physical machine hardware to the point where it is possible to run a full operating system using the abstractions only.That is, a virtualized system is one in which the operating system “thinks” it is running “alone” on a physical machine, but instead it is controlling abstractions in place of the physical hardware.Even this definition is a bit tricky since Intel introduced hardware support for virtualization. These extensions to the x86 architecture implement in hardware “abstract” versions of other hardware features (like page tables) calling into question the definition of the word “abstraction.” From a processor architecture perspective, then, virtualization is the isolation of physical machine resources such that a full machine operating system can use the resources independently as if they machine were dedicated to it.From a systems design perspective, virtualization is the notion that the operating system (and not the process) is the resource container. That is, each operating system believes it has full control over a set of machine resources and some other “system” (usually called a hypervisor) is able to arbitrate sharing among operating systems (in a way analogous to the way in which an operating system arbitrates sharing among processes). The Linux containers community might take exception to this last definition but so be it.Virtualization (as it is commonly implemented today) attempts to meet two requirements:  resource arbitration: virtualized systems share a common set of physical resources according to a set of policies that must be arbitrated  isolation: because operating systems are designed to be the software that implements hardware control, virtualization must ensure that virtualized operating systems cannot interfere with each other through manipulation of the hardware resources.There are generally four approaches to virtualization:  hardware emulation – The hardware an operating system uses is really a software interpreter (including JIT capabilities) of hardware instructions that the operating system issues.  hardware assisted – The hardware includes support for virtualization that can be used to implement operating system isolation without the “cooperation” of the operating system.  paravirtualization – The operating system being virtualized includes functionality (i.e. “cooperates”) in a way that allows it to be run in isolation.  containers – Not properly an operating system isolation technique, containers are none the less often discussed along with OS virtualization. Containers implement isolation within one operating system in a way that allows each user to “believe” she has a dedicated “copy” of the operating system alone.These methodologies have had different implementations by various vendors. For example, VMware’s proprietary virtualization technology began as hardware emulation but is today implemented via hardware assist. Sun Microsystem’s Zones are a Solaris container solution that has morphed into an open approach for Linux called OpenVZ.Today, the most widely used virtualization technologies are VMware, KVM, and Xen. There is also significant and growing interest in Linux containers in the form of LXC and Docker. This statement, in no way, is intended to pass judgment on the value of alternative technologies. It is just a statement about popularity.IsolationAt the root of all virtualization approaches is the need to provide isolation. Even to implement effective resource arbitration, some degree of isolation is necessary or participants will be able to subvert what ever sharing mechanisms are in place.In an unvirtualized setting, the job of implementing isolation falls to the operating system and the unit of isolation is the process. That is, each user can launch one or more processes that she “owns.” Within each process, the user appears to control all resources (memory, cpu, network connections, etc.). Outside the process, however, the operating system implements mechanisms that allow multiple processes to share the resources (memory, cpu, network, etc.) of a single machine. This sharing is a form of isolation because the actions a program takes within a process do not need to take into account the existence of other processes. For example, the cpu registers that implement the memory stack appear dedicated to the process and, as a result, the program does not need to guard itself against another user’s use of the stack.Process isolation simplifies program development and, in some cases, improves software engineering (by isolating software faults).Modern hardware provides support for process isolation using two built-in facilities: memory protection (typically via virtual memory) and system calls. Memory protection allows the operating system to dedicate a portion of the physical memory to a process and (most importantly) to reclaim that memory from the process when the operating system need to give it to another process. System calls allow a process to transition control flow into code that is trusted to control the hardware directly according to the isolation specifications. Thus a process executes code in a protected memory region. When that code need to manipulate shared hardware resources in a way that might allow it to violate the isolation properties of the operating system, it makes a system call to trusted code that first checks the access and performs it only if it is within specification (e.g. the process is accessing disk sectors according to the permissions scheme implemented by the OS).To implement OS virtualization, the system must have access to analogous functionality. It must be able to provide memory protection between virtualized operating systems (that are otherwise given autonomous control over their memories) and it must provide a method of transitioning control flow into trusted code when an OS wishes to perform an operation that could possibly violate isolation.In general, a virtualization methodology must  isolate memory between operating systems  provide control operations that allow the creation, destruction, and access control of operating systems  isolate I/O so that operating systems can access I/O devices as if they were dedicated.Thus when analyzing an OS virtualization system, one should ask three high level questions:  How is memory isolated?  How is trusted control implemented?  How is I/O implemented?We’ll look at the answers to some of these questions for a couple of virtualization systems. In what follows, we’ll use the term “hypervisor” to refer to the software that is responsible for implementing OS virtualization and “guest” or “guest OS.”KVMWhat follows in the discussion of KVM is due to several sources all of which are excellent in their own way. I do not cite them specifically but, instead, encourage you to use them together (as I have) to understand KVM.  The KVM source code  @braoru’s KVM presentation  Stefan Hajnoczi’s KVM presentation  IIT Class project by Senthil, Puru, Prateek, and Shashank An unattributed paper on the BHyVe hypervisorI’ve borrowed liberally (particularly with respect to figures) from these sources.KVM (Kernel-based Virtual Machine) is an open source virtualization technology currently supported by the Linux kernel for the x86 architecture. It is based on an earlier (and more portable) system called QEMU (see another wonderful tutorial by Steven Hajnoczi) that uses a software emulation which (while portable) carriers a significant performance penalty. KVM relies on virtualization hardware facilities that are now available for many (but not all) of the Intel processors that are compatible with the x86 instruction set architecture. Intel terms these capabilities “Intel VT” and they come in the form of “VT-x” and “EPT.” The community typically refers to these as “HVT” for “hardware virtualization technology.”KVM uses VT-x to implement protected state transitions between a guest and the hypervisor. With VT-x, the processor defines two new protection modes: host mode and guest mode.Intel VT-xTo see how these new features interact with the x86 it is instructive to review how an unvirtualized OS is implemented. The x86 ISA defines protection rings that allow an unvirtualized operating system to implement system calls. A user process runs in ring 3. Certain instructions (like those that manipulate memory protection features) will not be executed when the processor is running in ring 3. All such instructions will run in ring 0, however. One of three things happen when a process in ring 3 tries to issue an instruction that is a ring 0 instruction:  a fault occurs  the process issues a trap indicating that it wants code in ring 0 to issue the instruction on behalf of the process in ring 3  nothingIt is this last option that turned out to be most troubling. If all “protected” instructions generated a fault in ring 3, then it would have been possible (one suspects) to implement VT-x functionality without extending the ISA. However, the original ISA specification includes “quiet” no-ops for certain protected instructions making it difficult to implement OS virtualization.Thus Intel added host mode and guest mode. In each mode, the processor can be set in any ring. However protected instructions from Guest-0 generate faults that can be checked in host mode.The main idea behind KVM is to run a process on the host OS that KVM turns into a guest OS. To do so, a host OS process loads up the code for the guest that is necessary and then “launches” the VM in the correct guest mode ring level. Using the hardware triggers provided by VT-x, KVM (host ring 0) directs privileged events to QEMU code (running in host ring 3). If you are stuck with the Linux process model in your head, then the way to think of this is  A user process in a guest OS is a process  there is a kernel in the guest OS that implements process isolation  there is a host kernel that handles guest OS isolation  there is a process (QEMU) running on the host that runs much of the code to keep the kernel on the Host OS from being too bulkyThe state management via VT-x is relatively straight forward in that the extensions define a data structure to consult (set up in Host mode) when making transitions.When the guest requires service when either  there is an operation that the hypervisor must perform, or  something in the guest (say a system call made by a process running in the guest) demands that the guest access the machine hardware.KVM gets control via VMEXIT when the guest “traps” out of its usual operating mode for either of these reasons. Either it handles the request (in the case that it is a hypervisor function) or it forwards the request to the host’s kernel for service (in the case that it is a hardware access function). In this latter case, KVM gets control again so that it can deliver the requested host kernel functionality to the guest.In the case of hypervisor functionality, unless it is a simple kernel function, KVM pushes the actual processing into a user space process (QEMU) by returning from a system call that the process has made that has been blocked. When QEMU handles the request, it calls back into the host kernel via the system call.This previous discussion outlines the control flow relationships between a guest process, a guest kernel, KVM, the host kernel, and a host process (that contains the guest process, the guest kernel, and the QEMU code). Notice that the goal of KVM is to make a guest OS “look” like a regular process to the host OS since most of the functionality needed to service the guest is actually available in the host. Notice also that the guest doesn’t “know” it is embedded in a host process because the VT-x support handles transitions into the hypervisor when ever the guest might be doing something that requires privileged control.This high-level description is intended to cover control flow transitions. Notice that in an operating system, control flow can take many forms, almost all of which are asynchronous from the perspective the operation system kernel (guest of host). For example, when the hardware timer interrupts, it throws an interrupt to the host kernel. That interrupt may also need to be vectored (dispatched) to a guest kernel that need to keep track of the time as well. KVM in the host is responsible for handling interrupt dispatch. It uses the VT-x extensions to transition between host mode (where the interrupt is initially fielded) and guest mode (where the guest kernel’s interrupt handler must be executed). When KVM initializes a kernel, part of the boot-up process is to record how interrupt dispatch is to be implemented for the guest.And then, there is memoryAs discussed earlier, control flow is only part of the isolation picture. Another important part is the way in which the guest and host memory management systems interact. In particular, on an unvirtualized Linux system, the virtual memory implements memory isolation at the process level. A process wishing to grow or shrink its memory or to change its memory’s permissions must ask the kernel to do so on its behalf (operations that manipulate memory objects are privileged instructions). What is less clear but equally true is that all instructions that touch memory are checked (very fast) by the hardware to ensure that they are  in bounds  conformant to the permission specificationsIn the case where a memory access (read or write) fails to meet the kernel’s legal criteria, a fault is thrown which (under Linux) usually results in the process being terminated.In addition to governance, the memory system also implements efficient resource utilization in the form of demand paging. When a legal page from a process is needed and is not already present, the process throws a page fault that the kernel catches and services.The key data structure for implementing process memory management is the page table. The page table format for the x86 is defined (it is a two-level hierarchy) by the architecture. Thus, each legal page of process memory is described by two page table entries – a page directory entry (top level) and a page table entry – that are check on every memory access.To make this checking fast, each processor version also includes a TLB that caches recently used page directory and page table entries. The implementation of the TLB varies but it in each case it is designed to short-cut a full page table lookup if the mapping information has recently been accessed.Herein lies the rub.The x86 architecture is designed with the notion that there is one page table and one TLB per operating system. Worse, each page table entry refers to a physical frame number in the machine’s memory. Thus each guest will believe that it has the ability to use all frames in the physical memory – a situation that the hypervisor must prevent.In the case of KVM-style virtualization, however, each guest has a page table as does the host. Further (unlike in the control-flow case) it is not possible to interpose KVM on each memory access because the access checking is done strictly in hardware.KVM relies on another Intel VT feature called the “Extended Page Table” (EPT) to resolve this difficulty. EPT allows the host kernel to set up a “shadow” page table in the processor that gets used when the processor is running in guest mode. It is a shadow page table in that it gets used by the hardware to check memory accesses (like a true page table). However, the references in the shadow page table are to host page table entries (the host page table essentially forms another two levels of hierarchy). Thus, the host can maintain one, large page table that it partitions among guest page tables, each of which “believes” it has the full, unshared memory space.TLB management in the case of EPT is a joy, to put it mildly. Notice that “the” TLB for the machine now needs to handle shadow mapping and host mapping. Worse, when a guest VM is descheduled because another guest is to be run, the TLB entries for the old guest must be invalidated. In an unvirtualized setting, the TLB is usually flushed at process context switch time for the same reason. However, with an extra level of indirection, a full flush can cause a serious performance problem. Thus EPT (in some implementations) also implements a tagged TLB which allows the hypervisor to decide on exactly which TLB entries to flush when guests are switched.Final Notes on KVMThis presentation is a little backwards from what it would be in a class on virtualization in the sense that it describes Intel VT functions in terms of what KVM does. Indeed, the subject is richer than just the KVM picture might make it seem. First, Advanced Micro Devices implements hardware virtualization support as well in its x86 compatible processors. Indeed, many believe that AMD’s original AMD-V set of extensions were (and are) superior to Intel’s. When you study virtualization for the sake of virtualization, you should consider the similarities and differences between the two for the x86. KVM can use either Intel VT or AMD-V to implement the same functionality using the same software architecture.The careful reader at this point may be troubled by the lack of a discussion of I/O in KVM. This omission is intentional and for the sake of brevity rather than due to negligence or maliciousness. KVM’s original I/O model was designed to support “full” virtualization in the sense that the guest OS did not need to cooperate. To implement full virtualization, the I/O devices had to be emulated at a very low level so that guests would be able to use their own native device drivers without modification. Thus the original KVM contained software emulations of hardware Ethernet cards and SCSI disk controllers that “pretended” to be physical hardware devices. While flexible, this approach suffered from I/O performance penalties that led the KVM community to move away from full virtualization for I/O. Virtio is an effort to use paravirtualization (described below in the context of Xen) for I/O as a way of alleviating this performance issue. The non-paravirtualized KVM I/O features are still available, but the rumor is that they are being phased out as support for Virtio makes it into other operating systems (like Windows).XenMuch of what follows regarding the implementation of Xen is due to David Chisnall’s “Definitive Guide to the Xen Hypervisor”. There certainly have been developments in the Xen community that have occurred after its publication. This lecture is intended to cover only the basics of the internals at a reasonably high level.Virtualization is generally thought of an an IBM invention from the early 1970s. More recently (mid 1990s), VMware implemented virtualization for the x86 hardware using several different techniques (the industrial versions of which being similar to KVM’s approach). However, the first technology to make virtualization available as open source at a large scale was Xen. Like VMware and KVM, Xen’s technological goals for virtualization were to implement isolation at a level of abstraction that could permit operating systems to function independently of each other and of their host.However, the Xen approach differed from VMware and KVM (QEMU) in a few important ways. First, Xen was concerned, primarily, with the performance of virtualized operating systems. At this time it was founded, hardware support for virtualization was not widely available meaning that some form of software emulation was needed. VMware used (among other techniques) binary rewriting of guest OS code to insert emulation and QEMU was simply a full emulation system (although it also adopted binary rewriting in very clever ways). These techniques permitted full virtualization but there was concern within “the community” that they were too slow to be of practical use on an industrial scale.To meet the performance challenges as they saw them, the Xen team decided to sacrifice the notion that a virtualized guest could be hosted without modification. That is, Xen decided not to implement full virtualization but, instead, to implement paravirtualization which relies on changes to the guest. The key idea centered on changing the way in which each guest OS interacted with the x86 protection mechanisms so that isolation could be implemented without additional hardware extensions.The basic concepts behind Xen are deceptively simple. The goal was to run the guest in ring 1 of the x86 protection hierarchy. As with KVM, Xen had to solve the problems of  implementing privileged instructions issued by the guest(s) in a protected way (in ring 0),  the problem of implementing memory isolation between guest operating systems, and  implementing isolated I/O using shared machine devices.Unlike KVM, however, they relied on code changes to the guest kernel to implement features in a way that ensured good performance (better than with emulation techniques).Unsurprisingly, the Xen architecture is also different than KVM’sThe guest is not logically a process from the perspective of the host. Rather, each guest runs independently on the hypervisor which (in theory) is the control software for the host. That is, there is no host OS running in ring 0. Instead, the hypervisor runs in ring 0, and each guest runs in ring 1.If implemented completely literally, this architecture would require the hypervisor to include device drivers and user-interface code to allow guests to access hardware resources and to control the system. To avoid this complexity, Xen allows for a “special” guest to be specified (called “dom0” – pronounced “dom zero”) that also runs on the hypervisor. Guests, in the Xen parlance, are each termed “domU” to indicate that they are each a user “domain.”Under Xen, dom0 serves two purposes:  it runs the device driver code necessary to actuate the hardware devices  it runs the user-interface code necessary to instantiate and control other guestsThus the Xen hypervisor either services requests directly from its guests or routes those requests (after checking them for validity and security) to dom0 which executes them on behalf of the guests.To implement privileged control flow, each guest must be examined and modified so that each ring 0 instruction that cannot run in ring 1 be translated into a trap to the Xen hypervisor. The binary rewriting techniques discussed briefly in the previous section perform the same function, but at the instruction level without changes to the guest source code.Xen’s approach was to mandate that all such changes be made at the source level in the guest and that the guest be recompiled for execution with a Xen hypervisor. Working at the source level is, in some sense, easier since the operating system source explicitly encodes its privileged operations. Further, well modularized operating systems typically group these privileged operations together into a common set of modules, both to promote portability (these are typically architecture specific) and robustness (they are shared among all OS subsystems).One often overlooked issue with this approach is that it also changes the failure modes of the guest. In a full virtualization setting, the guest either runs or it appears to the guest that the hardware has failed. Under Xen, a guest may have misimplemented its Xen-specific code but otherwise be intact. Xen needs to ensure that any misimplementation in the guest does not compromise the overall system and also that the guest fail gracefully due to such a misimplementation.All privileged instructions that must be implemented securely in ring 0 must be replaced in the guest with a special kind of system call termed a hyper call. Like a system call, a hyper call pushes arguments on the stack (or marshals them in registers for speed) and then issues a “trap” instruction that causes the processor to switch to ring 0 and to jump to a well-known (and trusted) location so that the hyper call can be serviced. That is, a hyper call is really just a system call that a guest OS makes from ring 1 to ring 0 in (more or less) the same way that a guest process makes a system call from ring 3 to the guest OS in ring 1.Notice that traps themselves may need to be changed and also that interrupts must be vectored by the hypervisor in the same way as they are under KVM. However, because the guest can be changed, rather than intercepting them, Xen implements an event system that guests use to ask the hypervisor for interrupt support. Thus, interrupt code in the guest falls under the broad rubric of “privileged” code that must be modified. We’ll discuss this issue further in the section on paravirtualized I/O.Xen and memoryXen’s approach to managing memory isolation is also relatively straight forward to understand at a conceptual level (the implementation, however, is quite another matter). The key realization is that x86 page tables can be used directly by the guest as long as updates to page table entries are handled by Xen in ring 0. Put another way, as long as the guest starts in a protected state (Xen sets up its initial page table entries securely) as long as the guest asks Xen to implement any updates to the guest page tables, and Xen checks these updates, the guest can use the page tables directly.Thus Xen mandates that the guest change all of the code that attempts to write data into a page table entry (page directory or page table) to make a hyper call instead. It enforces this restriction by making the memory used to implement the page tables read only. Thus an attempt by a guest to write its page tables directly will cause a fault (which the hypervisor catches).The other part of Xen cleverness with respect to memory management has to do with how it manages the virtual to physical mapping of memory. With HVM, Xen supports shadow page tables (in two different ways). Without HVM, however, it allows each guest to “see” the physical frames in memory it has been allocated. When a guest needs a physical frame, it requests one from Xen and Xen expects that the guest will keep its own mapping of how that frame is used.One way in which the frame is used is in a page table entry. When a guest creates a virtual to physical mapping in a page table entry (as noted above) it must request that Xen update the page table on its behalf. Xen checks to make sure that the physical frame number specified in each update is “legal” for that guest (i.e. that the guest owns the physical frame).This technique will work in any situation where the guest must use a physical memory address as long as the guest requests state updates from Xen. For example, a guest that wishes to initiate a DMA will supply a physical frame number, but Xen must be able to check that the guest owns the frame before sending the DMA request to the machine’s DMA controller.Xen Paravirtualized I/OOne of the problems that an operating system virtualization system must solve is that of providing isolated I/O to guests via a common set of shared physical devices. It is possible to emulate I/O devices in software and then to instrument each emulation so that I/O is isolated (the initial KVM approach). Xen, however, requires that the guests modify the device drivers they use to allow protection and isolation characteristics to be inserted.The approach is called “The Split Driver” model. Each device is controlled by two drivers (one used by the guest and one used by dom0) that work together. The model is essentially an RPC-like model in which a client operates an interface that marshals arguments and communicates them in a canonical transport format to a server that unmarshals them and executes any commands that are contained therein. Results from the execution are marshaled and communicated back to the client where they are unmarshaled and presented to the user.In Xen, the guest driver (called “the front end”) is analogous to the client in an RPC setting. It is responsible for implementing the driver’s interface to the guest and for communicating device requests to the “back end” of the driver running in dom0. The back end operates the specific device and returns what ever results are needed by the front end to communicate to the guest.The communication mechanism between front end and back end is a combination of shared memory, queuing, and asynchronous events. Xen includes a facility (called Grant Tables) that allows access controlled sharing of pages between domains (all domUs and dom0). Thus one domain can send a page to another by allowing it to be unmapped in the sender and mapped in the receiver without the overhead associated with copying the data. Xen split drivers use this technique to send data between the front end and back end and vice versa.Each data transfer is associated either with a request event or a response event. Further, most (but not all) devices implement essentially a write/read model of I/O (from the perspective of the guest). Thus the guest initiates an I/O by writing a request from the front end to the back end. The back end reads these requests, services them with respect to the device, and writes a response that the front end later reads.Requests and responses are managed in a ring that logically lives in Xen’s memory. The ring only contains event descriptors (the data is moved separately using shared pages if possible). In addition, both the front end and back end portions of the driver can register a call back for Xen to run when a descriptor requires processing.It is through this event delivery mechanism that Xen achieves I/O virtualization. Event are delivered over “channels” that are set up in the hypervisor so that each guest appears to have exclusive access to the devices configured in the system. The event mechanism ensures that requests from one guest are not delivered to another nor is it possible for one guest to pretend to be another. The front end of the driver must also be modified to trigger any necessary interrupts in the guest in response to event delivery.Like with KVM, this high level description does not portray the complexities associated with the implementation of these concepts. The x86 architecture is really several architectures (some legacy) all of which must appear to function properly when executing a virtualized guest. The details associated with this proper function are, to be sure, non trivial.Final Notes on XenXen was and is a controversial technology. Originally, the notion of paravirtualization was thought to be too onerous an engineering burden for Xen to achieve wide-spread adoption. That prediction did not come to pass, however, both as the main line Linux kernel picked it up and the various Linux distros (Fedora, Red Hat, Suse, Ubuntu, Gentoo, etc.) agreed to support it.Then, for reasons that are somewhat obscure, the two main Linux distros (Fedora/Red Hat and Ubuntu) dropped Xen support in favor of KVM. There is some debate among The Clouderati on the importance of Linux distros in the time of cloud. The shift in popularity from Xen to KVM was, in part, driven by the decision to drop support in just two of the distros, which is a fact worth considering.Currently, it seems that Xen is back in Fedora 16 and Ubuntu (at least, at some level).From a performance perspective, the Direct paging scheme described above (paravirtualized memory without HVM) was considered too slow for many Linux functions (like fork) that required large numbers of page table updates. Xen’s HVM support is faster, but (like KVM) it really depends on having a good TLB hit rate to get close to native speed.Contrary to popular belief, paravirtualized I/O seems to be best practice for open source Linux device drivers. Much of the resistance to paravirtualization was centered on the perceived inability to use it for proprietary operating systems like Windows (because they are not open source). Windows support for Virtio is now available, however, so that impediment seems to have been specious.Linux ContainersAt this point, hopefully it is clear that both KVM and Xen are “machine” virtualization approaches. That is, they present an abstract version of a machine (the x86 arcrchitecture in the KVM case) to a guest OS that is isolated from other guests running on the host.There is another way to look at the problem of isolation, however, and that is from the perspective of the user and/or the process. A user of a virtualized operating system is, fundamentally, running a process. It is possible to consider a virtualization approach that isolates a process rather than the operating system in which the process is running.This approach is the one that is implemented by Linux containers. There have been other container approaches (Parallels is perhaps the best known) but LXC is the current leading technology with respect to the main line Linux kernel and distros.The main idea behind Linux Containers in general, and LXC in particular, centers on the notion that processes can be made to implement user isolation in a way that provides a similar degree of user control that OS virtualization does. Specifically, one of the advantages of OS virtualization is that it allows each user to run as root in her own OS. At worst, a user can damage her own OS instance by misapplying root privileges but such effects are restricted to the instance by the hypervisor.If the user could be given root, she could manage her process tree as if she were “the root” in an OS if her actions as root are isolated.Thus the goal of Linux containers is to implement an isolation mechanism that is comparable to OS virtualization using the Linux process abstraction.LXCMuch of what following comes from Rami Rosen’s excellent presentation on LXC.LXC is an amalgamation of several Linux features including  cgroups – a facility for resource partitioning in the kernel  namespaces – kernel level name control  apparmor – capability-based mechanism for implementing fine-grained permissions  seccomp – sandboxing capability  chroot – directory structure partitioning schemeFundamentally, LXC implements namespace isolation and then uses cgroups to try and provide some form of performance isolation.What’s in a Name?Namespace isolation is a fairly simple concept that is hard to implement in Linux. Conceptually, it is an extension of virtual memory. A process can only access memory it can address and virtual memory uses a secure mapping function (controlled by the kernel) to prevent it from addressing memory outside of its permission domain. Namespaces extend this mapping idea to all other resources that a process might access (except file descriptors which are already implemented via a secure kernel map). Thus when a process accesses a machine resource via a “name,” the kernel ensures that the resource belongs to the process by indirecting the name dereference through a kernel map.Moreover, the kernel maps can ensure that different processes use the same name for separate machine resources, thereby implementing isolation.For example, namespaces allow a process to change the the Linux hostname that it (and all other subprocesses) get back from a call to gethostname() without changing the host name that other processes get. Without namespaces, the kernel keeps a record of the hostname for the host. A call by root to change the host name changes it for the entire system and all calls to get the host name return what ever the current value is.With name spaces, a process (as root) can change the host name and only it and its descendants will see the change. All other processes are unaffected.Linux currently implements namespaces for six types of resources:  mnt – mount points and file systems  pid – processes  net – networking  ipc – Systems V IPC  uts – unix naming  user – user IDsSome of these are trickier than others. For example, the net namespace must provision a completely separate network stack (in terms of state variables) in the kernel. Networking is fairly stateful (routing tables, IPtables rules, device interfaces, etc.) making this task fairly extensive.However, with namespaces in place, it is possible to create a process (say when a user logs in) that has a complete set of Linux “names” allocated to it that can be changed from within the process (thereby affecting all children) without affecting other processes. In particular, using chroot, it is possible to make the file system that the process sees as the root file system look like the root file system of an unvirtualized Linux system. The boot code and logic won’t be effective, but all other Linux functions can then execute within the namespace as if they are running on a non-isolated kernel.LXC does all of the housekeeping (and there is a great deal of it) necessary to create a namespace that can run a full Linux distro as if the distro were running alone on a Linux kernel by itself.cgroupsNamespaces provide logical isolation meaning that processes running in separate namespaces have access to resources as if they are running on separate machines. However, all processes (regardless of name space) share the same kernel running on the machine – the kernel that implements the name spaces.The problem here is that the Linux kernel inherits from Unix exactly the opposite design specification from that which LXC is attempting to implement. That is, the original designers of Unix were trying to remove the possibility of isolation from the OS in their design (as a way of promoting sharing). LXC is, in some sense, attempting to retrofit isolation into a system explicitly designed not to support it.Namespaces are an elegant solution for logical isolation, but because the kernel is shared, the processes can still interfere with each other because they are still sharing the same set of kernel resources. Further, Linux contains only modest kernel level features that are designed to prevent the activity of one process from affecting the performance that another gets from the system. For example, memory intensive jobs that cause the system to page to the disk frequently affect system responsiveness for all jobs. Linux does its best to try and be “fair” about system usage, but the fundamental model used by the original Unix authors is that all processes on the system are assumed to belong to a group of users that are cooperating. Thus performance isolation is not an original design feature.Linux cgroups attempt to add the concept of performance isolation to the kernel in a way that permits fine-grained control. There are 11 different subsystems that can be controlled separately via an extensive set of configuration files. These files – together – constitute a policy specification for the cgroup that governs all processes assigned to the group. A running process can be added to at most one cgroup and it and its children will be controlled by the policies specified therein.LXC combines namespaces and cgroups so that isolated processes will get controlled fair use of the kernel resources. That is, each separate namespace will be entitled to use a specific “fraction” of the kernel resources that are available thus partitioning the Linux machine into separate isolated Linux instances.OS Virtualization versus Linux ContainersInevitably, the question of which approach is “best” arises. This is an interesting question that, like most interesting questions, requires some thought.First, it is important to understand whether a comparison should be made with respect to the intersection of capabilities or not. That is, one can askGiven the subset of functionality that is common to all alternative approaches, which is best?When one encounters the debate between OS virtualization proponents and Containers proponents, this is usually the form of the question that is assumed tacitly. Even then, however, the debate is not as straight forward as it might seem.Containers are an excellent choice (perhaps the best) if the goal is to provide hosted Linux access. That is, if the goal is to provide a facility where users can provision their own Linux systems and they don’t have strong preferences for the specific Linux they need, containers work well. In particular, they can achieve much higher tenancy densities that virtual machines yielding better server consolidation.However, the use cases for which this advantage is obvious is much more narrow than one might believe initially. For example, if the Linux usage is to host web services, the networking model that LXC implements is troublesome in that it doesn’t really allow “standard” networking configuration techniques to work. Put another way, containers are virtualized Linux access – not virtualized Linux infrastructure. If the applications running under Linux require infrastructure support, the containers are not necessarily better.Another difference is with the ability to host a variety of different operating systems. Obviously, LXC can host Linux guests and not Windows, for example, Less obviously, different Linux distros can also be problematic due to kernel affinity. That is, each disto version is developed against a specific version of the kernel. Often, to run a version one must change the kernel to match the version compatible with the distro. All hosted Linux on a container system must share the same kernel. Thus, the container Linux can only host distro versions that are compatible with its kernel. Worse, when the container machine upgrades, it can change the distros that can be supported.Thus, in a cloud context, determining which is “better” is a difficult task. For IaaS, clearly OS virtualization has become predominant. However, as PaaS evolves (where applications have less contact with the infrastructure components Linux exposes) containers may prove to be a better choice. It isn’t yet clear.",
        "url"      : "/2017/06/08/Operating-System-Virtualization/",
        "date"     : "2017-06-08 16:56:00 +0000"
    } ,
    
    {
        "title"    : "SDN Introduction",
        "category" : "Software-Defined Networking, Cloud Computing",
        "tags"     : "SDN",
        "content": "  Software Defined Networking and The Cloud  OpenFlow  SDN  Thoughts on SDN and CloudSoftware Defined Networking and The CloudCloud computing (as it is defined today) depends on the ability to provision an IP network that reaches all of the resources a user wishes to employ. The challenge, however, is that this network must be provisioned and ultimately deprovisioned (decommissioned) dynamically and the “normal” IP management protocols were not designed for dynamic reconfiguration. The main goals for IP, when it was designed, were routing robustness and delivery determinism (including reliable delivery). Because latencies were so high and connectivity to intermittent, the protocols were designed to react slowly so that transient changes in the network did not cause instability.Fiber-based networks (which are more reliable, lower latency and higher bandwidth than wire networks) introduced the possibility of implementing dynamic network reconfiguration without sacrificing network stability. In particular, it became possible to use table-driven forwarding rules at the link level.Before this time, IP defined network routes at layer 3 – the network routing level. Link level protocols were “stateless” in that they need only manage the transfer of data between two fixed end points. The “state” defining the end points doesn’t change (or it can be rebuilt using a broadcast via ARP). Thus all information pertaining to a route that data must take as it traverses the network was originally confined to Layer 3. This information is managed in a per-hop “routing table” that indicates the point-to-point network link that data must take to make its next hop.With ATM and SONET, however, table-driven link-level protocols made it possible for an abstract “network link” to be implemented as a routed “path” across intermediate link nodes. These original technology-specific protocols eventually informed a standard for such link-level state management called MPLS.OpenFlowLike most techniques in networking, the idea of manipulating layer 2 network state is not unique. MPLS defines “label based routing” as a methodology for doing a table look up in the switch to determine where a packet is going based on a tag (or label) in the packet. Since it is a layer 2 protocol, it can (and usually does) rewrite the tag before the packet egresses based on the “path” through the layer 2 network that the packet needs to take (the table specifies the output port and the next tag).Turns out that “normal” IP network switches implement some of this functionality as well. First, it is important to understand the difference between a router and a switch.  router: examines the destination address of a packet and determines the next link to use for the packet. Morally, all Linux systems are routers since they implement Layer 3 routing although the next link is often either the transport layer (if the packet is inbound and addressed to the host) or some network interface to which the host is connected (for outbound packets).  switch: examines the destination address in a layer 2 packet (called a MAC address) and determines the next port to which the packet should be sent.If you stare at these two definitions for a minute you still might come away thinking that a router and a switch dot he same thing: route inbound packets to an outbound interface based on destination address. From an engineering perspective this conceptual similarity is accurate. However, architecturally they are different.For IP, a router must implement a “longest-prefix-match” algorithm to determine the outbound network interface. IP addresses come in classes and, of course, IP address ranges can be subnetted. Thus the job of the router is to find the “best” (longest) match between prefixes in its routing tables and the destination address. Indeed, the original motivation for MPLS and its predecessors was that longest-prefix-match was too complex to implement in an ASIC. Today’s routers use more complex ASICs and do the routing entirely in hardware, but originally, routing was a software activity.A switch does not implement hierarchical addressing. In fact, originally (and for so called “dumb” switches today) there was no table look up at layer 2. In a switch, a packet that came in on a port was sent unceremoniously to all other ports. Since the switch was stateless, it couldn’t “know” which outbound port a packet should use so it simply sent it to all of them.Managed Switches, however, are smarter (and more scalable) than their dumb counter parts. They pay attention to ARP broadcasts and record MAC address-port mappings in forwarding tables. Because the MAC address is not hierarchical (it is strictly point to point), a switch need not implement longest-prefix-match and thus the table management is very fast and cheap to implement in hardware.Switch tables are managed internally using only local information. That is, because a switch is theoretically between two end points of a link, it need not consult with other switches when it builds its tables (say to avoid routing loops). However, it wasn’t long before network architects discovered that manipulating these tables via a control interface could prove useful (say because a spanning tree algorithm was malfunctioning).OpenFlow is a standard control protocol for managing forwarding table information at layer 2. Instead of allowing the internal switch logic to build and rebuild its tables, OpenFlow specifies that each switch has a controller (one controller can serve many switches) that is responsible for managing the switches tables.The advantage of OpenFlow is that the controller is programmable meaning that it can implement policies that determine what table updates to send the switches it controls. For example, it is possible to implement Access Control Lists for MAC address forwarding that change dynamically. The model is that a switch will forward packets using its hardware and tables if there is a table match. If there is not, it will contact the controller and ask for a table entry for the packet. Thus, by keeping an ACL and invalidating switch tables when it changes, an OpenFlow controller can implement policy in the layer 2 path.The disadvantage of this approach is that the controller’s logic (at least some portion of it) may need to be executed while the packet is in flight. Unless pre-computed tables are loaded into the switches, the typical interaction is for a packet is to contact the controller the first time it sees a packet with a MAC address it doesn’t recognize (i.e. for which there is no table entry) to get a table entry. This table initialization address a performance overhead to the initial packet reception since the controller is a software entity and may be remote from the switch.Another perceived disadvantage of OpenFlow is the possibility for layer 2 network chaos. Switch table entries are no longer strictly based on local information. If all switches are controlled by a single controller, the controller can keep the tables consistent, but in a scaled network setting, controllers may need to agree on policies to prevent loops, partitions, etc. Great care must be taken if this agreement is not transactional.SDNAn this point in the cloud SDN story, some debate has arisen. Some proponents of SDN believe that OpenFlow is software defined networking. That is, using policies specified in software run by the OpenFlow controllers, it is possible to provision and decommission virtual networks (based on ACLs that are installed dynamically) linking together cloud resources. Others maintain that OpenFlow is merely an interface through which other SDN services implement policy.The basis for this latter argument is in the notion that once switch behavior can be programmed, it is possible to implement different isolated network architectures over the same physical hardware. To see why this architectural control is important, consider the problem of provisioning a network for a set of VM’s in a cloud. For the purposes of illustration, I’ll use the AWS nomenclature in the following example.Recall that when a user creates one or more instances they are assigned to a security group that corresponds to an isolated layer 3 and layer 2 network between the VMs. If there is just one VM, it is the only host in the security group. The security group also specifies firewall rules that describe layer 3 routing into and out of the group. All VMs inside the group communicate as if the network were physically dedicated to them.Using “normal” networking protocols (i.e. without OpenFlow and/or SDN) creating a security group involves the following steps:  Create an isolated layer 2 network that the hosts running the VMs can access.  In a layer 3 router that is also on this isolated layer 2 network, create and install a layer 3 routing table entry that forwards packets traversing the security group boundary to a firewall.  Create and install firewall rules that describe access controls for the security group.  Attach the VMs at their respective hosts to the layer 2 network.  Set the default routes in the VMs to the router with the routing tables for the security group.Depending on the networking equipment that is used to connect the VMs to the router and the router to the firewall, there are several implementation options available. The easiest and most compatible with standard off-the-shelf networking components using VLAN truncking between the hosts running the VMs and the router. Each security group corresponds to a separate VLAN. Hosts must be able to pass packets tagged with different VLANs and must also be responsible for ensuring that VLAN-tagged packets are delivered only to the correctly specified VM. The router must also be trunked and must logically sit on all VLANs. That takes care of layer 2.For layer 3, each security group gets its own subnet. The router must allow the cloud to install and remove layer 3 routes corresponding to different subnets as security groups are created and destroyed.With SDN, however, it is possible to define these functions in terms of a new network architecture. Rather than defining a relationship between security groups, VLANs, and IP routes, the SDN-controlled network can define “roles” for different programmable components (presumably using a combination of open flow and layer 3 route control) in the network.For example, it is possible to build a layer 3 network between VMs that does not use an intermediate router. Instead, each host becomes an “edge router” that can send packets to a layer 2 network that sets up secure “circuits” between edge routers. In this model, each host hosting a VM maintains a routing table entry for the security group that correspond to a network or subnet. The layer 2 network, then, accepts commands from the cloud to set up a set of virtual circuits (implemented using switch forwarding tables) between all pairs of hosts participating in a security group. When a VM routes a packet, it goes to the host first (to make sure the VM isn’t spoofing its layer 3 address) and then the host forwards the packet to the layer 2 circuit switch.The advantage of this approach is that there need not be a centralized router that is programmed with routing table entries. The disadvantage is that the layer 3 edge routers and the virtual circuit set up and tear down must be coordinated.Thoughts on SDN and CloudFor the purposes of cloud computing it is not clear whether one approach is significantly more powerful than the other. Indeed, it is possible to implement edge routers using standard networks and Linux machines. It is also possible to implement layer 2/layer 3 provisioning using a router and VLANs with SDN.However, the proponents of SDN point out that architecturally the cloud is implementing a limited form of software defined networking when it provisions security groups. There is a controller (the cloud) that implements policy (isolated layer 2/layer 3 network and firewall rules) using programmable network devices (hosts for VLANs, router, and firewall device). They reason that these activities are independent of cloud computing (cloud computing is a special case) and thus should be implemented as their own service that the cloud uses.The scalability of the approach is also an argument that gets made although far less convincingly. Many SDN papers hypothesize a new hierarchical separation of concerns for The Internet. The reasoning is that the current approach relies on a consistent set of layer 2/3 protocols everywhere the Internet is to go. If The Internet were designed as a common set of “core” protocols (say the current IP protocols) and edge routers that can tunnel new protocols through the core, innovations in networking will be possible. It is true that an overlay approach using tunnels is more flexible. The feature of The Internet that is most compelling, however, is its ability to remain stable at a global scale. It is not at all clear that the additional flexibility offered by SDN will scale to Internet sizes.",
        "url"      : "/2017/06/08/SDN-Introduction/",
        "date"     : "2017-06-08 16:48:00 +0000"
    } ,
    
    {
        "title"    : "AWS Introduction",
        "category" : "Cloud Computing",
        "tags"     : "AWS, IaaS",
        "content": "  Amazon Web Service  Resource Pools  Interacting with AWS  EC2          Virtual Machines      Instances      Instance Impermanence      Elastic Block Store: Persistent Volumes      Completing the Instance Story      Networking      Amazon Web ServiceAmazon provides web-service based access to its infrastructure via two separate but interoperating facilities:  Elastic Compute Cloud (EC2) – virtual machines, network, block storage, identity management  Simple storage service (S3) – scalable, eventually consistent object storageIn addition, it also offers a number of higher level services that make use of this infrastructure, such as  NoSQL database: DynamoDB  SQL database: RDS  Data warehousing: Redshift  In-memory object caching: Elasticache  Archival storage: Glacier  Network overlays: VPC  Authoritative DNS: Route53  Web caching and CDN: CloudFront  Hadoop: Elastic Map Reduce  Data stream processing: Kinesis  Streaming app support: AppStream  Search: CloudSearch  Workflow: SWF  Bulk email: SES  Alerting: SNS  Reliable queuing: SQS  Media transcoding: ElasticTranscoder  Performance monitoring: CloudWatch  Web containers: ElasticBeanstalk  Image templating: Cloud Formation  Application configuration and deployment: OpsWorks  Secure key management: CloudHSM  logging and auditing: CloudTrail  Docker: Container Registry and ServiceAll services are accessible via SOAP, REST, command-line tools (Java), SDKs (Java, PHP, .Net, Python, Ruby, iOS, Android) graphical AWS console.Resource PoolsAWS implements a pooling model in which resources are partitioned into separate groups, more or less in a hierarchy. Only S3 is global to the entire cloud although the credentials are valid for all AWS services no matter which pool they are in.The top level designator for AWS is the region. Each region implements its own namespace. Thus names of cloud objects in one region are not valid in the others. Alternatively, objects that are replicated among regions (e.g. images) have different names.The current set of regions are:  ap-northeast-1 – Asia Pacific (Tokyo) Region  ap-northeast-2 – Asia Pacific (Seoul) Region  ap-southeast-1 – Asia Pacific (Singapore) Region  ap-southeast-2 – Asia Pacific (Sydney) Region  ap-south-1 – Asia Pacific (Mumbai) Region  ca-central-1 – Canada (Central) Region  eu-central-1 – EU (Frankfurt) Region  eu-west-1 – EU (Ireland) Region  eu-west-2 – EU (London) Region  sa-east-1 – South America (Sao Paulo) Region  us-east-1 – US East (Northern Virginia) Region  us-east-2 – US East (Ohio) Region  us-west-1 – US West (Northern California) Region  us-west-2 – US West (Oregon) RegionEach region contains several availability zones. Resources in separate availability zones should be characterized by independent failure probabilities. For example, a VM running in us-west-2a and one running in us-west-2b should be isolated from each other with respect to datacenter infrastructure failures (e.g. power outtages, network failures, disk failures, etc.)Interacting with AWSAmazon’s preferred method of AWS access is through the AWS console. However doing so makes it difficult to control AWS programatically. Each command has a documented REST and SOAP interface. The service documentation contains a link for each service that itself links the API reference. For example, the EC2 documentation is described here.Perhaps the most straight forward way to access AWS, however, is through the CLI. AWS consists of many simple primitives rather than a few complex ones. This design choice promotes both performance and robustness in a distributed setting since the coordinated functionality of each primitive is minimized. It makes using AWS tedious, at times, however because many different primitives may be necessary to accomplish a single task. Put another way, the AWS API is designed for scalability more than it is designed for convenience and/or ease-of-use.The CLI can be accessed, essentially, in two ways that are (more or less) differentiated by the way in which AWS security credentials are checked. Amazon makes available CLI tools that are written in Java and use SOAP. These tools use X.509 certificates to authenticate each request. AWS also supports a number of SDKs (for different languages) that use a secret key to sign REST requests. Euca2ools is a set of wrappers around the Boto SDK for Python that mimics the AWS CLI commands. I’ll use euca2ools in what follows but the syntax should be the same for Amazon’s ec2 tools.euca2ools requires access to your AWS credentials which are  an X.509 certificate  an X.509 private key  an AWS access key  an AWS secret keyIt also requires endpoints (URLs) for the tools to contact for different services. Most of the commands allow you to specify these on the command line, but the typical approach is to set them as environment variables.WARNING: You must protect your credentials. If they are stolen or made visible, they provide access to your AWS account.Here is a sample set of exports for bash that set some of the relevant environment variable. You can source this file in bash and that shell will be able to run euca2ools against AWS.export REGION=us-west-2EUCA_KEY_DIR=$(cd $(dirname ${BASH_SOURCE:-$0}); pwd -P)export EC2_URL=http://ec2.$REGION.amazonaws.comexport S3_URL=http://s3.amazonaws.comexport TOKEN_URL=http://storagegateway.$REGION.amazonaws.comexport AWS_AUTO_SCALING_URL=http://autoscaling.$REGION.amazonaws.comexport AWS_CLOUDWATCH_URL=http://monitoring.$REGION.amazonaws.comexport AWS_ELB_URL=http://elasticloadbalancing.$REGION.amazonaws.comexportEC2_PRIVATE_KEY=${EUCA_KEY_DIR}/XXXXXXXX.pem.txtexportEC2_CERT=${EUCA_KEY_DIR}/YYYYYYYY.pem.txtexport EC2_ACCOUNT_NUMBER='NNNNNNNN'export EC2_ACCESS_KEY='PPPPPPPP'export EC2_SECRET_KEY='SSSSSSSS'export AWS_ACCESS_KEY='PPPPPPPP'export AWS_SECRET_KEY='SSSSSSSS'export AWS_CREDENTIAL_FILE=${EUCA_KEY_DIR}/iamrcexport EC2_USER_ID='UUUUUUUU'You’ll need to fill in the strings that are repeated capital letters with the various keys and key files, and cert files, user ID and account number for your account. This information can be obtained from the AWS Management console.EC2Amazon’s EC2 provides essentially three functionalities:  virtual machine provisioning  network provisioning (including firewalls)  block storage provisioning (persistent volumes)We’ll cover each and then describe how they are managed using euca2ools using the following figure:Virtual MachinesLinux virtual machines, under AWS, are called instances. Each instance consists of a  boot record  kernel  ramdisk  image (root file system for Linux)  ephemeral disk storageFor windows, it is slightly different in that the kernel and ramdisk are combined, but logically the components needed are equivalent.Typically, the functionality that is associated with the virtual machine (VM) is defined by the image. For Linux, the image contains a single root file system which usually holds a specific version of a specific distribution (e.g. CentOS 6.4.1, Ubuntu 12.04, etc.) The kernel and ramdisk that are loaded with the image into the instance must be compatible with the image, and the boot record is one that the hypervisor must understand. Amazon installs the kernels, ramdisks, and boot records for the images it makes available publicly. It is also possible for users to upload their own images, and (using a paravirtualized boot loader) their own kernels and ramdisks.To see what images are available on AWS, set the REGION variable in the file shown above (I’m using us-west-2 in this example) and typeeuca-describe-images -aIt will take a while but you’ll get back a lengthy listing of the images that are installed and publicly available. Now tryeuca-describe-images -a | grep ami-f33fa9c3You should see something that looks likeIMAGE  ami-f33fa9c3 ubuntu-us-west-2/images/ubuntu-precise-12.04-amd64-server-20130516.manifest.xml 099720109477   available   public  x86_64  machine aki-fc37bacc instance-store paravirtual xen From the name, this image (identified by its AMI ID ami-f33fa9c3) is an Ubuntu 12.04 image compiled for 64 bit. Notice that it specifies a kernel ID: aki-fc37bacc. Checking the image catalog witheuca-describe-images -a | grep aki-fc37bacc | grep -v ami-the kernel is a custom kernel (contained in the image’s /boot directory) that is accessed vi PV_GRUB.IMAGE  aki-fc37baccec2-public-images-us-west-2/pv-grub-hd0_1.03-x86_64.gz.manifest.xml amazon available    public  x86_64  kernel              instance-store paravirtual  xen InstancesWhen an instance is instantiated from an image it is booted on a hypervisor and attached to a private network that is protected by a firewall and a NAT (Network Address Translation) gateway. It also need to have some form of external access enabled (typically ssh). Thus running an instance for the first usually time involves  booting an instance from an image  opening the ssh port in the firewall  telling AWS to insert the public key of a public-key/private key pair for sshIn addition, you need to select the “size” of the instance to boot from the set of available Instance Types. Each type specifies the amount of memory, the number of cores, and the size of the ephemeral disk that the instance will be given when it boots. Thus running an instance boots an image with the resource specified in the instance type, on a private network. Each instance type carries its own hourly charge so you need to be careful about which ones you choose.To get the instance running, you first need to ask AWS to create an ssh keypair, the public end of which it will inject for you into the instance should you so choose. The command to create a key pair iseuca-create-keypair -f keyfile keynameWhere keyfile is the name of a file you want to use to hold the private key and keyname is the name of the key pair. AWS associated the name “keyname” with the public key so that you can refer to it in the launch command as a way of instructing it to inject the public key into the instance. For example, the commandeuca-create-keypair -f testkey testkeycreates the keypair called “testkey” in AWS and writes the private key into the file testkey in the local directory.To launch an instance, you must specify (at a minimum) the ami ID and the key name. In addition, you should specify the instance type (there is a default but it may be more expensive than you’d like).euca-run-instances ami-f33fa9c3 -k testkey -t m1.smallLaunches the Ubuntu image described previously with the testkey keypair as an m1.small instance. When the instance boots, your account will be charged $0.10$ for each hour (or fraction of an hour) it uses.It takes some time to start an instance. Before the instance is ready, it will show up as having a status of “pending”INSTANCE   i-75ade57d  ami-f33fa9c3 ip-172-31-1-172.us-west-2.compute.internal pending testkey 0 m1.small  2014-04-23T20:11:35.000Z    us-west-2c  aki-fc37bacc monitoring-disabled        172.31.1.172    vpc-94ad41f1 subnet-d5467793    instance-store                  paravirtual xen     sg-b07fb3d5 default falseTo check its status you must poll the system:euca-describe-instancesand wait for the status to become running. When it shows as running the instance is booted and the ssh key has been inserted so that you can log in.Also notice that the instance has been give an instance ID that begins “i-“: i-75ade57d. You’ll need this ID to reference the instance in other commands. It is always available from describe instances.However you first need to open the ssh port in the firewall. In this example, because I didn’t specify otherwise, AWS chose the default security group for the instance. Each security group corresponds to a separate, isolated private network with its own firewall and NAT. It is possible to create different security groups (and to give them names) with the euca-create-group command. If you don’t specify a name to the run instance command, however, it will choose the security group with the name “default.”To open the ssh port in the default security group, runeuca-authorize default -P tcp -p 22 -s 0.0.0.0/0At this point you should be able to ssh into the instance using the private key returned by AWS when you created the keypair.ssh -i ./testkey ubuntu@ec2-54-187-86-32.us-west-2.compute.amazonaws.comNotice in the display from describe instances show above that the instance has both an external DNS name (ec2-54-187-86-32.us-west-2.compute.amazonaws.com) and an internal DNA name (ip-172-31-1-172.us-west-2.compute.internal). Only the external one is accessible from outside the firewall (it is NATted to the internal address and name). Also notice that I had to log in as the user “ubuntu.” The key insertion mechanism for this image enables the user ubuntu rather than the user “root” by default. The ubuntu user is sudo enabled, however, so you have root access to this instance.To terminate the instance (thereby causing AWS to cease charging you) runeuca-terminate-instances \"i-75ade57d\"You may need the quotes to avoid the shell parsing the “-“ in the ID.Instance ImpermanenceOnce of the curious aspects of the AWS cloud model (and, since it has influenced all of the others, we can “cloud computing” in general) is the difference between an instance and a VM in terms of permanence. A VM is intended to function as a “virtual” piece of hardware typically called a “server.” When a VM boots it is analogous to turning the sever on. When it shuts down, the model (typically) is that of turning a server off. The VM is assumed to be permanent (like a server) while it goes through its power up/power down life cycle.Instances are different in that they are temporary running versions of images (at least, they were in their first incarnation – see below). The designers of AWS believed  each instance should start from a known state  the infrastructure is robust enough to make instance failure rare  well written applications need to be prepared for catastrophic instance failureFor these reasons, instances have the property that they always start from the state that their image was in when it was registered with the cloud, and then they are terminated, all of the state changes that were made while they were running are lost.This property (which is actually quite a useful property when operating at scale) is the biggest source of confusion for most cloud users, especially those familiar with VMs and virtualization. When an “regular” instance is terminated, everything in the instance is deleted permanently. It feel strange because when he instance boots, there is a root file system that is readable and writable and also “ephemeral” disks that are empty but formatted as file systems that appear ready for use. Anything written to the root file system or the other ephemeral partitions while the instance is running is simply lost.For example, logging into an Ubuntu 12.04 instance run as an m1.small instance type, a df showsubuntu@ip-172-31-7-74:~$ dfFilesystem     1K-blocks   Used Available Use% Mounted on/dev/xvda1      10321208 796228   9000704   9% /udev              838184     12    838172   1% /devtmpfs             338516    168    338348   1% /runnone                5120      0      5120   0% /run/locknone              846284      0    846284   0% /run/shm/dev/xvda2     153899044 192068 145889352   1% /mntThe instance type claims that, for an m1.small, the instance gets 1 disk of 160 GB. The Ubuntu distribution is on the 10 GB root file system and the 153 GB partition mounted on /mnt is formatted as an ext4 file system. Any file written to the root file system or the file system mounted on /mnt will be lost when the VM terminates.Elastic Block Store: Persistent VolumesThe ephemeral nature of instances requires that the user take steps to store data that must persist across instance creation and termination in some form of persistent storage. Originally, there was only S3. The model was to load from S3 all data that the instance needed just after it booted (sometimes as part of its /etc/rc processing) and then to explicitly store back to S3 any data that needed to persist after the instance shut down. This load/checkpoint model proved too cumbersome for many to use effectively. Further, it can be time consuming (although not expensive since Amazon does not charge for the bandwidth used between EC2 and S3) as the fetch and put are not differential (although several clever systems were developed to alleviate this last problem).It is still possible to use S3 as an explicit storage facility for instance persistent data, but Amazon now includes a persistent storage volume facility called the Elastic Block Store (EBS).EBS volumes are essentially persistent disk partitions that can be created and destroyed dynamically under user control. Because they are logically not part of the instance, they must be attached to an instance before they are used and detached when they are no longer needed. terminating an instance automatically forces a detach.To create a volume, you must specify the volume size (in GB) and an availability zone for the volume. EBS volumes can only be attached to instances running in the same availability zone.Returning to the example, I started another instance let Amazon choose an availability zone for my instance start. It chose us-west-2c.INSTANCE   i-c4c68ecc  ami-f33fa9c3 ec2-54-187-111-144.us-west-2.compute.amazonaws.com ip-172-31-7-74.us-west-2.compute.internal   running testkey 0 m1.small  2014-04-23T23:00:42.000Z    us-west-2c  aki-fc37bacc monitoring-disabled    54.187.111.144  172.31.7.74 vpc-94ad41f1 subnet-d5467793    instance-store              paravirtual xen sg-b07fb3d5 default falseTo create a 10 GB volume that this instance can access, theneuca-create-volume -s 10 -z us-west-2cThe return message has the status of the volume as creating. Before the volume can be attached its status must go to available.euca-describe-volumespolls the status.VOLUME vol-7443fb78    10      us-west-2c  available 2014-04-23T23:13:41.660Z  standardNow it is possible (using the instance ID) to attach the volume. You need to specify the device name in the instance to use (/dev/xvdb in this example – note that in the df AWS has used xvda for its root and ephemeral).euca-attach-volume -i i-c4c68ecc -d /dev/xvdb vol-7443fb78Now poll again to wait until the attachment completesVOLUME vol-7443fb78    10      us-west-2c  in-use 2014-04-23T23:13:41.660Z standard    ATTACHMENT  vol-7443fb78    i-c4c68ecc  /dev/xvdb   attached 2014-04-24T00:10:13.000ZAt this point it is possible to log into the instance, mount the volume (it will appear in the instance as /dev/xvdb) and to format it as a file system. Once formatted (you only need to do this the first time after the volume has been created) the volume can be mounted as if it were a formatted disk partition.Before terminating, it is best to unmount the volume. Linux will try and flush the buffer cache and do an unmount as part of the terminate but there is no guarantee that the flush will be complete before the detach forced by AWS takes place. Detaching a mounted volume can cause data corruption so you should unmount before detaching and/or terminating to be safe.Having done that, however, data in that volume will persist past the termination of a an instance. If you were to write some data into the volume, unmount it, detach it, and terminate the instance. Then start another instance (it can even be from a different image as long as it recognizes the file system you put on the volume when you formatted it) attach the volume and mount it and the data will still be there. The EBS volume behaves like a portable disk that you can “plug” into an instance once it is running.Note that each EBS volume can be plugged into at most one instance at a time. It is not a file sharing protocol (which is another point of some confusion).Why?Returning to the figure, the volume itself is housed in some storage facility within AWS (it is not part of the instance). To access it, disk block requests are encapsulated in network packets and sent across a network link to the network-attached device that is serving the volume. Amazon does not say much about how EBS is implemented. For example, it is not clear whether the EBS block traffic goes over the same network interface that the instance traffic uses. It also isn’t clear whether the packet format is IP. Logically, though, the architecture is analogous to using iSCSI to encapsulate disk block traffic in IP packets.Thus, the protocol is between a single disk block device and a storage device – EBS is not a file system. In the same way that two machines cannot have the same disk plugged into them, two instance cannot both attach an EBS volume.However, it is possible to format an EBS volume as a file system and then to use a network file system (within the instances) to share it. One common set up that is used to share an EBS volume is to mount it in an instance and then have the file system exported as a NFS share to other instances.Completing the Instance StoryJust to make things more convenient (and confusing) it is possible to use an EBS volume as the backing storage for an instance. That is, in the original conception of an instance provided above, the storage for the instance is dynamically allocated when the instance is started and and released when the instance terminates. This is now called an “Instance Store” instance. An “EBS-backed Instance” is one in which the root file system containing the image is stored in EBS. In the describe images output, the image is tagged as either being an ebs instance (or not, implying that it is an instance store instance).The advantage of an EBS-backed instance is that the root file system can be made to persist across instance terminate, but in kind of a funny way. First, for EBS-backed instances it is possible to issue a STOP command. Stopping such an instance also stops AWS’s charging for it but it does tell AWS to reclaim the root file system space. Thus, creating an EBS-backed instance and then stopping it, is like terminating but without the loss of the contents. To get it back, you issue a START command.If you terminate an EBS-backed volume, the default action is to delete the root volume. You can override this behavior. If you do, the volume will be detached but it will remain available (and you will be charged for its storage). You can mount the volume in another instance (like any other volume) but you can’t restart an instance from it directly.Instead, to restart from a volume that has persisted past the termination of an instance, you need to take a snapshot of it and register the snapshot as a new image. In effect the snapshot-register operations reconstitute the root file system with the exact contents it has when the instance detached it.NetworkingThe networking model supported by AWS in general, and EC2 in particular, can also a source of some confusion. Each instance must be assigned to a security group. Security groups are created by the user using the create group command:euca-create-group cs290b -d rich-test-groupNote that the “-d” flag which allows you to add a description of what the group is for is not optional. AWS will assign an internal name to the security group that begins “sg-“ but you can use the name you’ve given the group instead to launch an instance. So a describe groupseuca-describe-groups cs290btakes your name and returnsGROUP  sg-f64f9a93 932234911131    cs290b  rich-test-group vpc-94ad41f1PERMISSION  932234911131    cs290b  ALLOWS  -1          TO CIDR 0.0.0.0/0   egressEach security group is logically a firewall behind which the instances that are in the group are placed in terms of networking. Thus launching an instance in this group:euca-run-instances ami-f33fa9c3 -k testkey -t m1.small -g cs290bwith the “-g” option tells AWS that an instance should be started and the firewall rules for the instance should be those associated with the security group “cs290b.”In the default configuration (without the use of the AWS VPC facility) each instance gets one network interface on a private network that the security group controls. All instances in the same security group are on the same private network and, thus, can communicate without regard for the security group rules. Instances in other security groups or network connections outside of AWS are subject to the security group’s filewall rules.To enable ssh (as we did for the default group in the previous example)euca-authorize cs290b -P tcp -p 22 -s 0.0.0.0/0After which, the decribe for the group showsGROUP  sg-f64f9a93 932234911131    cs290b  rich-test-group vpc-94ad41f1PERMISSION  932234911131    cs290b  ALLOWS  tcp 22  22  FROM CIDR   0.0.0.0/0   ingressPERMISSION  932234911131    cs290b  ALLOWS  -1          TO CIDR 0.0.0.0/0   egressEach instance gets an internal IP address on the private network and an internal DNS name. It also gets an external IP address and external name installed on the firewall/gateway associated with the security group.When two instances communicate, they should use their internal addresses. If they do not, the traffic will need to traverse some gateway within AWS and doing so affects network performance. For this reason, the DNS that Amazon operates inside AWS uses a split-horizon model in which DNS names resole to their internal addresses from within AWS but the external name will resolve to the external IP address from outside AWS.Also notice that an instance doesn’t have a way to discover its external IP address or DNS name using normal Linux tools. For example, a call to ifconfig returns the private IP address associated with the instances interface and a DNS lookup on the external name (as described in the previous paragraph) also returns the private IP.For example, an instance described asINSTANCE   i-bfaa7eb7  ami-f33fa9c3 ec2-54-187-63-154.us-west-2.compute.amazonaws.com ip-172-31-24-67.us-west-2.compute.internal   running testkey 0 m1.small  2014-04-25T16:56:45.000Z    us-west-2b  aki-fc37bacc monitoring-disabled    54.187.63.154   172.31.24.67    vpc-94ad41f1 subnet-062a3c72    instance-store                  paravirtual xen     sg-b07fb3d5 default falseyieldseth0      Link encap:Ethernet  HWaddr 06:95:3e:09:4b:4a            inet addr:172.31.24.67  Bcast:172.31.31.255  Mask:255.255.240.0          inet6 addr: fe80::495:3eff:fe09:4b4a/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:281 errors:0 dropped:0 overruns:0 frame:0          TX packets:252 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:32703 (32.7 KB)  TX bytes:27826 (27.8 KB)          Interrupt:27 from ifconfig. Notice that the public IP address is not listed.To resolve this problem, Amazon operates a web service at an unroutable IP address called the metadata service. Its address is 169.254.169.254 and a wget from this service will return “metadata” for the instance making the call. One of the entries in the metadata service (among many) is the public IPv4 address that is visible externally.For example,wget 169.254.169.254/latest/meta-data/public-ipv4for the instance described above yields 54.187.63.154.",
        "url"      : "/2017/06/08/AWS-Introduction/",
        "date"     : "2017-06-08 16:41:00 +0000"
    } ,
    
    {
        "title"    : "File System",
        "category" : "Operating System",
        "tags"     : "File System",
        "content": "  The Unix File System          What is a Unix file?      Problem: disks are organized as collections of “blocks”      What should the kernal do?      The inode      The file system      Directories, Path Names, and Mount Points      The Buffer Cache      File Decsriptors, Open File Table, Inode Table      Summary      The Unix File SystemI’m not particularly fond of Chapters 11 and 12 in S &amp; G, so I’m augmenting them with musings concerning the Unix file system. The primary reference is “The Design of the Unix Operating System” by Maurice Bach, Chapter 4, but almost any book on Unix internals will contain this information.What is a Unix file?  abstraction: sequential stream of bytes  system calls: read(), write(), open(), close(), seek()Problem: disks are organized as collections of “blocks”  a disk address is some kind of tuple          track/sector      cylinder/platter/sector        all Unix disk-drivers translate disk addresses to logical block numbers (1..n)  through the block driver interface you can request block “k” and the driver will convert that to a track/sector tuple.What should the kernal do?  The kernel must be able to translate user-process system calls (which refer to a file as a sequence of bytes) to logical block numbers whichare then translated into disk addresses by the individual disk driver.The inode  maps individual byte addresses relative to the beginning of the file     to logical block numbers for a particular disk  holds permission information for the file, whether the file     is a regular file, a directory, a “special” file, and the     current file size    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |     block #                  |    --------------------------------    |  permissions, ownership,     |    |  current file size, file     |    |  type                        |    --------------------------------  blocks are a single, fixed size  table index corresponds to logical position in the file,                                    block offset in file    --------------------------------    |     block 34                 |         0    --------------------------------    |     block 722                |         1    --------------------------------    |     block 1072               |         2    --------------------------------    |     block 6                  |         3    --------------------------------    |     block 377                |         4    --------------------------------    |     block 771                |         5    --------------------------------    |     block 7                  |         6    --------------------------------    |     block 83                 |         7    --------------------------------    |     block 212                |         8    --------------------------------    |     block 433                |         9    --------------------------------    |     block 812                |         single    --------------------------------    |     block 96                 |         double    --------------------------------    |     block 531                |         triple    --------------------------------    |  permissions, ownership, etc |    --------------------------------      the above example defines a file.   If blocks are 4096 bytes long, the 11033rd byte in the  file is found by first calculating the logical offset (in blocks) from   the beginning of the file as    11033 / 4096 = 2    At table entry 2 (block offset 2) we find logical disk block number  1072.  The byte offset within disk block 1072 is    11033 % 4096 = 2841    So given the inode shown above, the 11033rd byte of the file is  byte 2841 of block 1072 on the disk from whence this inode is  allocated.        latter table slots (single, double, triple in the figure)     refer to single-, double-, and triple-levels of indirection.        the number of direct, single, double, and triple slots in     each inode are implementation specific.  However, given these     numbers, the block size, and the size of a block number,     it is possible to calculate how large the largest file that     can be represented is.  For example,     assume    block number =       4 bytes block size =         4096 bytes # direct blocks =        10 # single indirect blocks =   1 # double indirect blocks =   1 # triple indirect blocks =   1 implies (10) directly accessible blocks + (4096 / 4) single indirectly addressable blocks + (4096 / 4)^2 double indirectly addressable blocks + (4096 / 4)^3 triple indirectly addressable blocks =  1,074,791,434 addressable blocks * 4096 bytes/block =  4,402,345,713,664 addressable bytes within a single file      The file system  a file system consists of a          superblock      a collection of inodes      a collection of data blocks allocated on a disk            when a file system is configured, the number of inodes that will     be available for files and the number of data blocks (presumably     the (disk size) - (the number of inodes) - 1 for the superblock) are     specified.    inodes on disk are          logically contiguous on disk so the inode number (the     logical offset from the beginning of the inode region)     identifies, uniquely, an inode.      marked with a free/used flag        the superblock keeps the starting block address of the     inode region and a bit map of free/used data blocks          if the superblock becomes corrupted, however,     but the inode region is known, a program can chase     all block numbers are re-create the superblock (see fsck)      Directories, Path Names, and Mount Points      a directory is a file that contains string-inode number pairs only        by knowing the inode number and the file system (identified by         the device number on which the file system resides)      in which that inode resides, the kernel can locate the file to      which that inode refers        for example, assume that a directory contains the following 5      entries      \".\"    : 147  \"..\"   : 91  \"cat\"  : 133  \"dog\"  : 211  \"fish\" : 12         the file \"cat\" is defined by the 133 inode within the file system in which it resides.  The superblock for that file system contains the starting logical block address for the inode region, so the kernel can translate inode number 133 into a logical disk address which can then be read into memory for access.               a path name is a chain of directory entries or a chain of directory     entries followed by a non-directory file.  For example     /cs/faculty/rich/cat where \"cat\" is not a directory, refers to four directories: \"/\",  \"/cs\", \"/cs/faculty\" and \"/cs/faculty/rich\". Assume that they are all on the same file system.  Then, each directory has an inode number and \"cat\" has an inode number.  The kernel maintains a table of mounted file systems that contains the superblock for each, and each \"root\" inode number.              To find /cs/faculty/rich/cat, the kernel first locates the     inode number for “/”, loads the data blocks for that directory into     memory, and searches for them linearly looking for the     string “cs”.  Say the root directory looks like      \".\"      : 0  \"..\"     : 0  \"usr\"    : 1021  \"green\"  : 777  \"cs\"     : 3355  \"tmp\"    : 23            the kernel locates “cs” in the directory and reads out     3355 as its inode number.  It then goes back to the file     system and reads the file (a directory in this case) corresponding     to inode number 3355.     The process recurses until a non-directory file is located (the     file type is kept in the inode).  The Buffer Cache  inodes and disk blocks are cached in memory  doubley-threaded list (hash and link lists)  one buffer cache per configured file system  hash table is used to look up block numbers  free list is used to locate free blocks  when a logical block number is requested from disk the hash is      consulted to determine if a copy is already available  while buffer is in use, it is taken off the free list  when process is finished with buffer, it is returned to the     end of the free list  when a new buffer is required it is taken from the beginning     of the free list (LRU)  if the buffer has been modified, it is scheduled for write-back     (dirty bits)File Decsriptors, Open File Table, Inode Table  each PCB has a table of file descriptor pointers  each file descriptor pointer points to an open file table     entry containing          reference count      offset      inode table index        each inode for an open file is stored in an “in-core” inode table     entry with a reference count  in this example          Proc A has forked Proc B, thereby copying file descriptors      reference count on open file table entry is 2 since     processes share the offet value      both of them are accessing the file A_filename      Proc C has opened the file B_filename        if another process had opened A_filename separately, what would     happen?          a new open file table entry would be allocated and     ref count would be set to 1      that entry would point to the inode table entry for     A_filename and its ref count would be set to 2        file descriptor your program gets is the index into the     per process file descriptor tableSummary  a regular Unix file is accessed as a linear list of bytes  the inode defines a map from linear byte offsets within the     file to logical disk block numbers on a particular disk  the disk driver converts logical disk numbers to track/sector     disk address tuples  an inode “lives” in a file system.  Its number is the logical     index from the beginning of the inode region for its home     file system  a file system consists of          a superblock      a contiguous array of inodes      a set of data blocks        the superblock is a map for the file system.  It contains          the block address of the inode array      the size of the inode array      a map of the free data blocks on disk        a directory is a file, built by the kernel, that     contains string-inode pairs  a path is a chain of directories  when a path is traversed, the kernel fetches the inodes for     the constituent directories, one-at-a-time, based on the     inode numbers that match the directory strings  buffer cache caches any disk blocks in front of the disk  process accesses file through file descriptors which     point to open file table entries which point to inde table     entries          one inode table entry for each open file      forked processes sharing offet into an open file     have file descriptors pointing to same open file table     entry      multiple processes with the same file open point to     separate open file table entries, but these point to     the same inode table entry      ",
        "url"      : "/2017/06/08/File-System/",
        "date"     : "2017-06-08 15:54:00 +0000"
    } ,
    
    {
        "title"    : "Memory Management",
        "category" : "Operating System",
        "tags"     : "Memory",
        "content": "  Linking and Loading  Base and Bounds  Static versus Dynamic Linking  Memory Partitioning          Internal Fragmentation      Variable Memory Partitions      External Fragmentation      How Does it Fit?      Which is best?        Swapping and Backing Store          Compacting Memory        Demand Paging          The Page Table and Address Mapping      The Details      Backing Store: Disk Block Descriptor Table      Frame Table      Page Faults        Initiating an Address Space          Dirty Pages      More on Freeing Frames      Identifying Run Sets and Page Stealing        The Clock Algorithm          Thrashing      Swapping and Paging Together        And that’s it.          Issues with Demand Paging      Segmentation      Hierarchical Page Table        Inverted Page TableThis lecture will attempt to cover the various concepts that are important to the memory management functions that most operating systems must perform.Linking and Loading  program addresses are logical, machine addresses are physical  linking – resolving logical addresses relative to the entire program          .o files have logical addresses      when multiple .o files are linked, addresses must be altered      symbol table contains necessary information      Unix utility ld() is the linker        loading – the introduction of a program into physical memory  logical program addresses must be converted to physical machine addressesWhen the code is loaded into memory, the compiler generates code so that variables are accessed via offsets from memory locations that are determined when the program boots. For example, consider the fictitious code depicted in the following figure:The local variables A,B, and C are all addressed via the SP register by computing a memory address by adding the contents of the SP to the contents of an offset register (R1 in the figure).Thus, when the program is loaded into memory, the loader “knows” that the stack pointer will start at location 0x7f00000c and it tells the OS to initialize the SP with this value. The call to foo() can been compiled to pus the space for three integers onto the stack and the code accesses the variables indirectly through the SP and individual offsets for each variable.That’s fine for the stack where there is always a machine register that defines the top of the stack. CPUs do not have explicit registers for the other segments in a Linux a.out process image, however. Thus, when accessing global variables, the compiler must either “burn” a register for the start of the data segment (which is costly since there aren’t enough registers as it is) or it must “hard code” addresses (as in the following figure):Again, the compiler has chosen to access memory via indirect addressing, but it does so with address constants to save registers for computation.This work fine when  the program is always loaded at the same location in memory  the program is the only program in memoryHowever, consider what happens if there are to be two processes in memory at the same time.On this figure, Proc1 has been loaded into the top half of the physical memory and Proc 2 into the bottom half. Presumably the OS is prepared to switch back and forth between the two processes using time slicing and context switching.However notice that if it is physical memory alone, the compiler must “know” where the program will be loaded so that it can adjust the address constants appropriately. The stack pointer isn’t a problem, but the global variables are.Worse, in this scenario, one program can read and update the variables belonging to the other simply by addressing the memory. For example, if Proc 1 executes the instruction sequenceLD SP, 0x7f00000Cit will suddenly be accessing the stack variables of Proc 2.Base and BoundsOne simple solution to both problems is for the CPU to include “base and bounds” registers that “fence” memory.  when the program is loaded, a base register is set to contain the physical address of where the first logical address of the program will be located in memory  a bounds register is loaded with the physical address of the last (highest addressed) physical memory location that the program can access  all program instructions refer to logical addresses  whenever an instruction attempts to access a logical memory location, the hardware automatically          adds the logical address to the contents of the base register to form the physical address of the access      compares the physical address to both the base register and the bounds register      if the physical address is less than the base or greater than the bounds an address out of range exception is generated      otherwise, the access processed with the new physical address        OS must ensure that base+bounds pairs for different programs do not overlap  can also be implemented with a check on the logical bounds (see page 284)Thus the compiler uses the same address constants for every program it compiles, usually assuming that the program will be loaded at location zero. When the OS loads the program, it sets the base register to the physical address where the program should start and the CPU adds this value to every address that the process uses when it is executing.For example, in the figure, the base register would be loaded with 0x3f800006 when Proc 2 is running. The compiler, however, would compile Proc 2 as if it were always loaded at location 0x00000000 and the CPU adds the contents of the base register to every attempt to access memory before the access takes place.Notice that when the CPU switches to run Proc 1 it must change the base register to contain 0x00000000 so that the addresses in Proc 1 are generated correctly. When the CPU switches back to Proc 2 it must reload the correct base address into the base register at context switch time, and so on.Notice also that it is possible, using this scheme, to switch Proc 1 and Proc 2 in memory. If, say, Proc 1 were written to disk and then Proc 2 were written to disk and then Proc 1 were read from disk, but this time into memory starting at 0x3f800006 it would run just fine as long as the base register were set to 0x3f800006 every time it runs.The limit register is just like the base register except it indicates the largest address that the process can access. Because it isn’t used to compute the address it is sometimes expressed as a length. That is, the limit register contains the maximum offset (from zero) that any address can take on.When a process issues a request for memory, then, the address that has been generated by the compiler is first checked against the value in the limit register. If the address is larger than the limit value, the address is not allowed (and a protection fault is generated, typically). otherwise, the address is added to the value in the base register and sent to the memory subsystem of the machine.Because these operations are done in the CPU hardware they can be implemented to run at machine speed. If the instructions necessary to change the base and limit registers are protected instructions, then only the OS can change them and processes are protected from accessing each other’s memory.Static versus Dynamic Linking  static linking: all addresses are resolved before the program is loaded  dynamic linking: addresses are resolved “on demand”  when an address exception occurs, the exception handler          checks the logical address to determine if it refers to a routine or variable that must be dynamically linked from the information kept in a “link table”      if it is a valid address, the routine is loaded and the memory management state is adjusted      for example: bounds registers would be incremented to reflect additional code or variable      instruction causing the exception is restarted      Memory PartitioningThe idea of loading programs that are relocatable at different locations in memory and using the CPU to implement memory protection is called memory partitioning. In the previous example, the memory is divided into two sections (called partitions). A program can run in each partition and when it does the CPU base and limit registers (set only by the OS) implement memory protection.This type of memory partitioning scheme is called “fixed” partitioning and it was used in some of the early mainframe computers. When the machine is configured, the administrator (or systems programmer) would set the number of partitions that the machine could use and the OS would schedule processes to use them.Internal FragmentationNotice, though, that the scheme can be inefficient with respect to memory usage. If all of the processes are the same and they are written to “take up” the size of a memory partition, then memory is fully utilized. However, that almost certainly is never the case. Instead, small processes that do not require all of the memory in the partition to which they are assigned simply leave that memory idle.In the Linux example shown above, the space between the heap (which grows toward higher memory addresses) and the stack (which grows toward lower memory addresses) is unused memory. If the text segment is loaded at the lowest address in a partition and the initial SP is set to the highest address in the partition when the process is first loaded, any space between the heap and stack boundaries is idle memory. The process may use it, but it is not available to other processes. Thus a process with a small text, data, heap, and stack still takes up a full partition even if it only uses a small fraction of the available memory space.This problem is called fragmentation since the memory is divided into fragments (each of which is contiguous) of “used” and “unused” memory. More specifically, when the partitions are fixed in size, the term internal fragmentation is often used to indicate that the fragmentation is internal to the partitions.Variable Memory PartitionsNotice that a partition is an artificial boundary that is determined by the values of the base and limit registers chosen by the OS. For example, if there are only six different values for the base register that the OS will ever choose, the memory will have seven partitions.It is possible, however, for the OS to choose different values dynamically as memory demand varies. Because the code is relocatable based on the base and limit registers it can vary the partition boundaries possibly reloading a program into a smaller or larger partition.For example, when the OS boots, if it runs a single process, that process can be given all of memory. If a second process arrives to be scheduled, the OS can  stop the first process  copy all of the processes used memory into half of the memory  reset the base, limit and SP to reflect the new memory region  load the new process into the memory that has been freed and set its base, limit and SP accordingly  start both processesIn practice, this relocating of an existing process is rarely implemented because it is expensive (in terms of overhead) and complex. The process must be stopped and its memory copied. If there is only one CPU in the system, then no other processes can run while this copy takes place. Also notice that the compiler must be very carefully written to not put memory addresses in registers (since some of those addresses, like stack addresses, will change when the process is moved). That is, if the field length shrinks (limit is decremented) the offsets of variables will change. The compiler can ensure that all references are relative to some register that is loaded when the program runs to make the code completely relocatable.For these reasons the typical implementation would give each process a maximum field length when it was created (usually based on some user input or administrator set parameters). Once a process began executing it would not change its field length. The OS would then assign a base register value when the process runs based on the other processes already running in memory.For example, consider the three processes that have been loaded into memory as shown in the following figure.In the figure, Proc1 occupies the first 100 MB of space in a memory that is 1GB is total size. Proc2 occupies the next 200 MB and Proc 3 occupies the 500 MB after that. The last 200 MB are free.When these processes were loaded, the OS chose the base register value for each so that the processes would pack into memory in this way. That is, the base register value for the next process is the sum of the base and limit values of the one before it. If a fourth process were to arrive, the OS would schedule it to the last 200 MB of memory accordingly.External FragmentationNext, consider what happens if Proc2 finishes execution.The region freed by Proc2 between Proc1 and Proc3 can now be assigned to a new process as can the free region between the end of Proc3 and the end of memory.Notice, though, that there are 400 MB free but the largest process that can be run is only 200 MB. What happens if a 300 MB process arrives? It cannot be started even though only 60% of the memory is occupied.This type of fragmentation is often called external fragmentation because the wasted space occurs outside of each assigned partition.How Does it Fit?Notice that the OS has a choice when it goes to assign a newly arrived process that does fit. For example, if a new process that requires 100 MB arrives, the OS can either place it in the hole left by Proc2 or in the hole between the end of Proc3 and the end of memory. Let’s say it chooses the hole left by Proc2:as shown for Proc4.So far so good, but now consider what happens if the fifth process to arrive requires 75 MB. In which hole should the OS place this job? Generally, there are three options:  first-fit:starting from one end, choose the first partition in which the process will fit,  best-fit:scan the entire list of free partitions and choose the one that exceeds the size of the process by the smallest amount (break ties any way you like)  worst-fit:scan the list of partitions and find the one that exceed the size of the process by the greatest amount (again, break ties in any way)In this example, if the OS used first-fit (and started from the top of the address space in the figure) it would place the 75 MB job in the hole between the end of Proc4 and the beginning of Proc3 since that would be the first hole it would come to in the list of holes. If the OS used best-fit, it would make the same decision (since 75 MB fits “better” into 100 MB than 200 MB) and if it used worst-fit it would choose the 200 MB hole between the end of Proc3 and the end of memory.Which is best?Which is best? That’s a matter of some debate. There has been a great deal of research looking at this question and, curiously, there isn’t a definitive theoretical result. You can argue it pretty much any way you like. Best-fit tends to maximize the number of small holes. Thus if your job size mix doesn’t include a lot of small jobs, it might not be best. Worst-fit tends to maximize the number of big holes. That sounds like a good thing, but in simulation it tends to do worse that best-fit or first-fit. First-fit is fastest (since you don’t need to scan the entire list of free partitions) and it tends to make the hole size tend toward the average. Generally speaking, in the absence of some specific distribution information about sizes, most implementations choose first-fit due to its simplicity. However that is a rule of thumb and not an axiom and there are other allocation schemes one can consider.Swapping and Backing StoreMemory is memory and bytes are bytes so as long as the OS can copy memory to disk and back again, it is possible for the OS to run more processes than can fit in memory by “parking” some of them on disk and swapping them for running processes periodically. Notice also that if the processes are relocatable in memory, they need not be swapped back into same region of memory when they are reloaded.To swap a process out, the OS must  cause the process to stop (usually by taking it off the run queue during an interrupt)  save off all of the processes CPU state (registers, for the most part)  find a place on disk to store the memory image of the process and its register state  copy the memory that the process occupies to disk  add the partition the process occupied to the list of free partitionsIn addition, it is nice if the OS zeros out the memory that the process just vacated so that another processes doesn’t get to see its “secrets” if it is loaded to the same region of memory. This zero fill can be done later just before the new process loads as well.Similarly, to swap a process in, the OS must  find a free memory partition large-enough to hold the process  copy its memory image from disk into the memory partition  make sure its saved CPU state is available when the process runs again  put the process on the runnable queueThus, as a form of slow motion “time slicing” the OS can multiplex the memory by swapping processes to disk. The disk and copy times are long compared to CPU execution speeds. Thus the interaction between the CPU process scheduler and the memory scheduler must be “tuned.” The typical interaction is to allow the in-memory processes to run for some long period while others wait on disk. As the disk processes “age” their priority for memory goes up (although at a slower rate than the aging rate for processes that are waiting for CPU in a CPU timeslicing context). Eventually, when a disk process has waited long enough, an in-memory process is selected for eviction to make room for the in-coming disk process, and so on.Notice that the first-fit, best-fit, worst-fit situation pertains to the disk as well. The backing store is usually a fixed region of disk space (larger than the size of main memory) that must be partitioned to hold process images while they are waiting to get back into memory.Modern operating systems such as Linux use swapping (hence the term “swap space” or “swap partition” that you may have heard in conjunction with configuring a Linux system). As we will see, however, they do so in a slightly different way than we have discussed thus far.Compacting MemoryBefore machines supported “demand paging” (see below) one use for swapping was to “clean” the memory of external fragmentation. If the OS determined that processes were not being accepted because there were too many holes and the holes, if coalesced, would allow more processes to be admitted, one thing it could do was to swap “all” processes out and then swap them back in again, packed one against the other. This form of “defragmentation” was not popular with interactive users, however, since it meant that the OS would need to stop everything until memory could be compacted.The notion of physical memory partitioning may seem quaint by modern standards. It is efficient with respect to execution speed, however, since the base and limit register accesses are almost trivial in terms of CPU cycles lost to memory protection. So much so that specialized supercomputers like those built by Cray used this scheme for many years.Demand PagingModern systems use a different scheme for memory management, however, that is often referred to as “virtual memory” but which is more properly termed demand paging.The basic idea is pretty simple. Instead of having a single base and limit register for the entire process (thereby requiring the whole process to be in contiguous memory), the CPU supports the notion of a “map” that automatically converts a relocatable address to a physical memory address. Thus, as is the case with memory partitioning, each process is compiled so that its internal addressing is relative to address 0x00000000 and the hardware maps each address to a physical memory address automatically. In the case of base and limit the mapping is an addition of the value contained in the base base register to the address to provide an address in physical memory.For demand paging, however, the map is a table in memory that the OS sets up which tells the CPU explicitly where to find a memory reference in physical memory. The OS loads the address of this table into a special CPU register when the process is scheduled and all references to memory made by the process are subjected to the mapping operation by the CPU so that they can be translated into physical addresses.However, to cut down on the number of entries in the table, individual address are not mapped. Rather, the memory is broken up into pages of a fixed size. All pages are the same size and, for reasons of efficiency, the size needs to be a power of 2.The Page Table and Address MappingThe first thing to appreciate about demand paging is the relationship between binary numbers and addresses. An address is really just an offset from zero that refers to a byte in memory that the CPU uses to access that byte.However, consider the following example address0x000002D0This address is 720 in decimal. Let’s look at it in binary, though0000 0000 0000 0000 0000 0010 1101 0000So far so good? Okay now think of it this way. Instead of it being the 720th byte in memory, imagine it to be the 208th byte in a 512 byte page that has been mapped to some physical page frame in physical memory. That is, the low-order 9 bits in the address give you an offset from the nearest 512 byte frame in the memory space. Put another way, you can think of an address as being an offset from zero or you can break it up into pages that are powers of two in size, in which case  the lower order bits give you the offset in the page  the high order bits give you the page number, in linear order, among all frames  the boundary between low-order and high-order is the number of bits necessary to express the page sizethus multiplying the page number by the page size and adding the offset gives you the linear address.For example, if the page size is 512 bytes, then boundary between page number and offset is defined so that the low-order offset is 9 bits since 2^9 is 512.          Page Number        |   Offset                             |0000 0000 0000 0000 0000 001 | 0 1101 0000                             |                             |The left most 23 bits in this address give the page number which, in this example, is 1. The offset is 0xD0 which, in decimal, is 208. Thus to compute the linear address we can multiply the page number by the page size and add the offset:Page Number     Page Size   Offset----------------------------------       1     x     512    +   208     == 720Why does this matter? Because it allows us to build a table that will map pages of address space to arbitrary pages in physical memory based on page number. An address translation requires the following steps be executed by the CPU each time an address is referenced by a process.  the page number is extracted from the high order bits of the address  the number is used as an index into a page table for the process that contains the page number of the page in physical memory  the physical page number is multiplied by the page size and added to the offset  the CPU uses this new quantity and the physical addressFrom here on in, we’ll use the terms  page number to refer to the page number in the process’ address space  frame number to refer to the page number in physical memory spaceThe page table maps page numbers to frame numbers. The page number is an index into the table and the entry contains the frame number.Each entry in the page table actually contains more than the frame number. In particular each page table entry contains  frame number  status bits          valid: contains valid data, but backing store is consistent      modified: contains valid data and backing store is inconsistent      referenced: used to determine run set (see        protection bits          usually 2 indicating Read and Write permissionsbelow)      The page table is a one-dimensional array of page table entries, indexed by page number, containing the following information:-----------------------------------------------------------------| Physical Frame # | valid | modified | referenced | protection |-----------------------------------------------------------------Ignoring these additional bits for a minute, the following figure shows a possible page mappingThe address space on the left is partially mapped to the physical memory on the right. Address 720 with a 512 byte page size indexes to the second entry (counting from zero) in the page table. This entry contains frame number 2 which, when multiplied by the page size and added to the 208 byte offset yields the physical address.It is important to understand that this address translation is done by the CPU (or more properly by the MMU – memory management unit) on every memory access – it cannot be avoided. Thus as long as the OS controls the set up of the page table processes cannot access each other’s memory.Further, each process needs its own page table. It is possible for processes to share memory, however, if the same frame number is listed in each sharing process’ page table. Notice also that if they were to share memory, they don’t need it to have the same address in their respective memory spaces.For example, page 7 in process 1 might refer to frame 5 which is being shared with process 2 which has it mapped to page 10. In this example, the 8th entry of process 1’s page table would contain 5 and the 11th entry in process 2’s page table would also contain 5 since they are both mapping frame 5.The DetailsThe hardware specifies what the format of this entry is exactly so it will change from platform to platform, but a machine that supports full demand paging will contain these elements. Note also, that the figure is not drawn to scale. The valid, modified, and referenced fields are typically bit fields, and the protection field is typically two bits wide.The protection field usually contains two bits that enable four kinds of access:  none: no access permitted  read only: process may read but not write  write only: process may write but not read  read/write: process may read and modify the pageWe won’t dwell on the protection bits much from here on out. The important thing to know is that when an access that violates the bits that are enabled occurs, the hardware throws a protection fault in response. For example, the read only bit will be set for all pages in the text segment of a Unix program and a protection fault occurs if you try to write those pages. Similarly, a no access page may be put between the stack and heap to allow the process to fail gracefully if you over-write either. This page is called a red page.For what remains, recall that the page number is an index into a table of these entries from which the frame number is recovered.Backing Store: Disk Block Descriptor TableNotice that the OS can use disk as a backing store to page frames out of physical memory. However, because processes usually exhibit spatial locality, only a subset of pages out of a process’ full page set need to be present most of the time. Thus, the OS can “park” individual pages that haven’t been accessed in a while on disk.To figure out where the data resides on disk, the system maintains a second table, also indexed by page number, that contains the following entries.-------------------------------------------------| swap device # | disk block # | swap file type |-------------------------------------------------Don’t worry about the type. Just notice that what the kernel is doing here is storing the disk location where the backing store for a given page is located. Logically, these two entries are part of the sample table entry. they are typically implemented as separate tables, however, since the hardware will want pages tables to look a certain way, but backing store descriptors are completely up to the OS to define.Frame TableThe system also maintains a table of records that describe what each frame of memory is currently storing. In each record, the kernel stores--------------------------------------------------| ref count | swap device # | disk block # | PTE |--------------------------------------------------for each frame in the system. There are also some other fields that have to to with allocating and freeing frames but we won’t go into the details. Suffice to say that the OS need to be able to sweep through the frames that are currently occupied in memory every once and a while and knowing where the frame is paged on memory is handy.In summary,  each page in a program must be loaded into a memory frame before the CPU can access it  each page in a program has a “shadow” copy on the swap device at some disk block number.  when a page is loaded into a frame          a free frame is allocated (not described yet)      the page is fetched from the disk block location      the page table entry is updated with the frame number      the valid bit is set to indicate that the in-memory copy is valid with respect to the copy on disk      the disk block address from the block descriptor table for the process is copied into the frame table entry for the frame      a pointer to the page table entry is put in the frame table entry      the reference count for the frame is incremented      Note that the frame table is shared among all processes executing at the same time. The OS uses it to identify when frames are free and, when they are not free, which page table entry corresponds to the page that occupies a Frame. For example, the following figure shows two processes that each have their first three pages mapped to frames.Each frame table entry for memory indicates which page table entry corresponds to the mapped page in the frame.Notice also that, in this example, the OS has allocated pages to frames such that there are no shared frames. It is possible, however, for processes to share memory under this scheme by having different pages mapped to the same frame. In this case, the frame table requires a list of page table entries for the page tables that map the frame (not shown in the figure).Page FaultsWe now have enough information to understand exactly what a page fault is. Every time the CPU issues a memory reference (sends an address across the address bus)  the hardware masks off the page # and indexes into the page table for the process  the valid bit in the page table entry for the page is checked (by hardware)  if the valid bit is set, the frame number for the physical frame is retrieved, multiplied by the page size, and added to the offset in the address to form the physical address. No page fault is generated.  if the valid bit is not set, the CPU throws page fault exception that the operating system must catch.  the page fault exception handler must          allocate a free frame      fetch the page from the location specified in the disk block descriptor table and load it into the frame      update the frame table entry (as described above)      update the page table entry (as described above)      restart the process so that it reissues the faulting instruction      In the previous figure, the entries marked X in the page table are not valid entries. In the page table entry itself, the valid bit would be set to zero to indicate that the entry does not contain a valid mapping.When the CPU does the address translation, and it goes to fetch the frame number from the page table entry, it check the valid bit. If the valid bit is clear, the CPU throws a page fault which traps into the OS. The OS must fine a free frame, load the frame with the data from the address space into the frame, and restart the process at the place where the fault occurred, load the frame with the data from the address space into the frame, and restart the process at the place where the fault occurred.Initiating an Address SpaceOkay – at this point, you should be able to visualize how a program address space is initiated (although we haven’t talked about all of the mechanisms in detail yet). To start a program, the OS must  allocate swap space: space on the backing store that can hold images of the pages in memory  allocate a page table for all of the pages a program might use. This table must be contiguous since the hardware indexes into it.  allocate a disk block descriptor table to hold the backing store information  copy each page of the program into the swap space noting the disk block and swap device number of the page in the appropriate disk block descriptor table entry  set all of the valid bits in the page table to be zero  run the programThe very first fetch of an instruction will cause a page fault to occur since it will be attempting to read page 0. Since the valid bit is clear, the OS will take a page fault exception immediately, go to the disk block descriptor entry for page 0, find the disk block number, get a free frame, load the page into the frame, update the frame table entry with the pointer to the page table entry, update the page table entry with the frame number, set the valid bit to 1, and restart the faulting instruction. When the programs runs onto page 1, or jump to another page, it will be faulted in accordingly.The only two pieces you are missing, before understanding exactly how this works concern how frames are allocated and deallocated, and how swap space (backing store) is managed. We won’t discuss these two issues in detail since they vary considerably from system to system. Each OS includes a swap space manager that can allocate and deallocate frame sized regions of disk space from the swap partition. Most OSs also maintain an internal “cache” of pages that have been used recently but are not allocated to a process. This page cache gets searched (efficiently) before the OS goes to disk to get a page.Dirty PagesSo far, we have discussed how frames are allocated and freed without much regard to what happens when the in-memory copy is written, making it inconsistent with the disk copy. When a program modifies a variable (either a global variable, a variable on the heap, or a variable on the stack) the page must be in a frame, and the valid bit must be set (or a page fault will occur), and when the modification occurs, the hardware sets the modified bit in the page table entry to indicate that the in-memory copy is now different (and more current) than the disk copy. It does NOT go out to disk at that time and update the disk copy. Why? Because your program is constantly modifying variables. If you had to way for a page to be flushed to disk, every time you made a simple assignment statement in your program, your program would be very very slow.The term “dirty” is sometimes used to refer to a page that has been modified in memory, and the modified bit is occasionally termed “the dirty bit.” Notice that a dirty page is always more current than the backing store copy. Thus, to “clean” a page, the copy that is in memory must be “flushed” back to backing store, updating the backing store copy to make it consistent with the memory copy.More on Freeing FramesWhat happens when a page fault occurs, but there are no free frames? The OS cannot simply steal a frame from a running program. Or can it?What would happen if the OS, when confronted with no free frames, simply chose a frame that was being used by a program, cleared the valid in the program’s page table entry and allocated the frame to the new program? If the program that originally owned the frame were using it, it would immediately take a page fault (as soon as it ran again) and the OS would steal another frame. It turns out that this condition occurs (in a slightly different form) and it is called thrashing. We’ll discuss that in a minute, but the remarkable thing to notice here is that the OS can simply steal frames that are in use from other programs and those programs will continue to run (albeit more slowly since they are page faulting a great deal).What actually happens has to do with locality of page references. It turns out that a large number of studies show that program access “sets” of pages for a good long while before they move on to other “sets.” The set of pages that a program is bashing through repeatedly at any given time is called the programs run set. Very few programs have run sets that include all of the pages in the program. As a result, a program will fault in a run set and then stay within that set for a period of time before transitioning to another run set. Many studies have exposed this phenomenon and almost all VM systems exploit it. The idea, then, is to try and get the OS to steal frames from running programs that are no longer part of a run set. Since they aren’t part of a run set the program from which the frames are stolen will not immediately fault them back in.Identifying Run Sets and Page StealingHow, then, can the OS determine a program’s run set? The common methodology uses the referenced bit the page table entry that we have not heretofore discussed and a another special thread called the page stealer. Could I make this up? I ask you.Here is the deal. First, every time a reference is made to a page (with read or write) the hardware sets the referenced bit in the page table entry. Every time.The page stealer wakes up every now and then (we’ll talk about when in a minute) and looks through all of the frames in the frame table. If the referenced bit is set, the page stealer assumes that the page has been referenced since the last time it was checked and, thus, is part of some processes run set. It clears the bit and moves on. If it comes across a page that has the referenced bit clear, the page stealer assumes that the has not been referenced recently, is not part of a run set, and is eligible to be stolen.The actual stealing algorithms are widely varied as Unix designers seem to think that the way in which pages are stolen makes a tremendous performance difference. It might, but I’ve never heard of page stealing as being a critical performance issue. Still one methodology that gets discussed a great deal is called the clock algorithm. Again – there are several variants. We’ll just talk about the basics.The Clock AlgorithmIf a particular OS uses the clock algorithm for page stealing, the page stealer wakes up when one of two events occurs:  a periodic timer has expired  the number of free pages in the system falls below a “low-water mark” indicating that the system is running short of free pages.Both the timer interval and the low-water mark can be set with the kernel is configured allowing a certain amount of “tuning.”The page stealer then maintains two “hands” – one “hand” points to the last place the page stealer looked in the frame table when it ran last. The other “hand” points to the last place it started from. When the page stealer runs, it sweeps through the frame table between where it started last and where it ended last to see if any of the referenced bits are set.  if the reference count is zero, or the valid bit is clear, the frame is already free and the page is skipped  if the referenced bit is set and the valid bit is set, the page is part of a run set. The referenced bit it is cleared and the frame is skipped.  otherwise, if the referenced bit is clear, valid bit is set, but the modified bit is clear, the page is clean and can be stolen. The page stealer sets the reference count to zero and puts the page on the free list, but it leaves the rest of the page intact. Notice that if the system is implementing page caching, and this page really is part of a run set (i.e. the page stealer was just a little too eager), a page fault will occur when the page is next referenced, and the page fault handler will most likely find the page on the free list. Thus, the cost of being “wrong” about a page being in a run set is most likely to be an extra page fault, but not an extra disk access.  otherwise if the referenced bit is clear, valid bit is set, and the modified bit is set, the page is not part of a run set, but it is dirty. The page stealer schedules the frame for cleaning by putting it on the page out queue and waking the page out thread. It does not steal the page, though, but rather waits until the next pass to see if it is unreferenced and clean before it is stolen.Here is a brief table summarizing this mess. For each frame between the starting point and the ending point (the two “hands”)v = 0 or ref cnt = 0       : page is free so skip itv = 1, r = 1               : page is busy.  clear and skipv = 1, r = 0, m = 0        : page is clean and unreferenced. stealv = 1, r = 0, m = 1        : page is dirty and unreferenced                             schedule cleaning and skipOnce the page stealer has run this algorithm for all of the pages between its start point and end point, it must move these points in the frame table. it does so by changing the new start point to be the old end point (wrapping around the end of the frame table if need be) and then it walks forward some specified number of frames (again wrapping if needed) clearing the referenced bit for each frame. These are the new start and end points (“hands”) for the next time it wakes up.It is called the clock algorithm because you can think of the frame table as being circular (due to the wrap around) and because start and end pointers work their way around the circle.Variations on this theme include “aging counters” that determine run set membership and the way in which dirty pages are handled. I’ll just briefly mention two such variations, but each Unix implementation seems to have its own.If you think about it for a minute, you can convince yourself that the clock algorithm is an attempt to implement a Least Recently Used (LRU) policy as a way of taking advantage of spatial and temporal locality. The most straight-forward way to implement LRU, though, is to use a time stamp for each reference. The cost, of course, would be in hardware since a time stamp value would need to be written in the page table entry each time a reference occurred. Some systems, however, time stamp each page’s examination by the page stealer using a counter. Every time the page stealer examines a page and find it is a “stealable” state, it bumps a counter and only steals the page after a specified number of examinations.The other variation has to do with the treatment of dirty pages. SunOS versions 2.X and 3.X (Solaris is essentially SunOS version 4.X and higher) had two low-water marks: one for stealing clean pages and a “oh oh” mode when all stealable pages would be annexed. In the first mode, when the system ran a little short of pages, it would run the clock algorithm as described. If that didn’t free enough pages, or if the free page count got really low, it would block the owners of dirty pages while they were being cleaned to try and get more usable on the free list before things got hot again. Usually, if the kernel found itself this short-handed, the system would thrash.ThrashingYou’ll notice that demand paging is really a “race” between processes that are touching pages (thereby using valuable page frames) and the page stealer that is attempting to keep the frames free. Under normal conditions, the run set of each running process is small enough to fit into memory, and the LRU approximation algorithm (e.g. the clock algorithm or one of its variants) is able to correctly identify each run set. If enough processes are running to make the run sets larger, in total, than the number of physical memory frames, then when a page is stolen, it will immediately fault back in (since it will be part of some run set). This condition is termed thrashing and it can be detected by monitoring the page in and page out rates of the system. Try running the Unix utility vmstat (after consulting the man page for details on its function). Among other valuable pieces of information, it typically includes paging rates. No Unix systems that I know of automatically throttle process creations as a result of paging activity, but the information is typically provided by a utility such as vmstat so that administrators can determine when thrashing takes place.Swapping and Paging TogetherOkay – at this point you have a fairly clear picture of all of the mechanisms that must interplay in order to make demand paging work. The concept is deceptively simple, but the implementation obviously involves considerable mechanism. One last mechanism we must talk about (and then we are done) is the process known as swapping.As mentioned about in the discussion of the clock algorithm, the kernel maintains a count of free pages with the frame table and a low-water mark to indicate when page stealing should occur. A second method that the kernel uses to try and free up frames is to send to the swap device all of the frames associated with a given process, thereby putting them on the free list. Thus, the kernel maintains a swap out thread whose job it is to evict an entire job from memory, when there is a memory shortfall.Again, your mileage may vary, but the basic idea is for the page stealer to try and do its work and, after making a complete sweep of memory, if there is still not enough free frames, for the page stealer to wake the swap out thread. The swap out thread chooses a job (based on the size of the job and how long it has run) and goes through its entire page table. It invalidates and frees any pages that have the valid bit set, but the modified bit clear, it schedules the valid and modified pages for disk write, and it sets the execution priority of the process to zero (or takes it off the run queue entirely) for a specified period of time. The idea is to pick a large, and old process (one that has received a lot of time already) and “park” it in swap space for a while. By doing so, and freeing all of its frames, the theory goes, a bunch of smaller jobs (which are probably interactive anyway) can get in and run. Also, the free frames might relieve paging pressure so that the unswapped jobs can complete, leaving more memory for the swapped job.After a suitable interval (OS dependent, of course) the swapped job is put back in the run queue and allowed to fault its pages back in. Sometimes it is given extra time slices as well on the theory that it does not good to let it fault its pages in only to be selected again by the swap out thread for swapping.And that’s it.By way of a short summary, here are the highlights for virtual memory:  there is page table entry for each process page. It contains          a frame number      valid, modified, and referenced bits        there is a block descriptor table entry for each page in a process that contains          the the swap device number (indicating which swap partition)      the block number on this device for the page      some type information indicating if the current page is in the original disk image on on the swap partition        there is a frame table entry for every physical frame in the system that contains          a reference count for the frame      the swap device number for the page contained in the frame      the block number for the page in the frame      a pointer to the PTE (or list of PTEs) for the page in the frame      pointers for the free list/free hash tables        there is an on-core swap map that indicates which blocks on the swap devices are free and which ones are busy  processes fault pages in by accessing them.          reading or writing a page causes a page fault after which the valid bit is set      any write to a page that changes its contents from the one in swap space sets the modified bit      any read or write to a page (causing a page fault or not) sets the referenced bit      all bit setting is done by hardware, typically        page faults require that frames be allocated  frames are deallocated when          the process terminates, giving up its pages      the page stealer determines that they are not part of a current run set      the swap out thread sends an entire process to swap space        pages can be reclaimed from the free list (page cache) if they are still valid          data structure like the buffer cache (with a hash table) is employed to make look up faster        when a page is dirty and heading to disk, care must be taken to prevent the process from being scheduled and attempting to access the page. Thus the page out thread and the process have a race condition that must be managed.Issues with Demand PagingThere are a few points of concern for demand paging. First, noticed that in the example we have presented, page-mapped memory halves the effective memory bandwidth. Why? Because every time the CPU translates an address it needs to go to memory twice: once for the page table entry and once for the memory access itself.The solution to this problem is to rely on locality and to add a cache of page table mappings to the CPU called a translation lookaside buffer or TLB.The TLB is usually implemented as a fast associative memory in the CPU or MMU. When the OS successfully maps a page number to a frame number it puts this mapping the TLB. The TLB is checked each time an address translation is performed and if it contains the mapping, the table look up is not performed.Thus the true performance of the memory system depends on the size of the machine’s TLB and the degree to which the workload displays spatial locality. Notice also that the TLB is per CPU. Thus when a process context switch takes place, it must typically be flushed of entries so that an new process doesn’t get access to the old process’ memory frames.Secondly, notice that in the example page table memory is not paged. Thus it must be resident and there must be one for each running process. How much memory do the pages tables take up?  page table can be huge and must be contiguous          512 byte pages =&gt; offset is 9 bits      32-bit address =&gt; page number is 23 bits =&gt; 8388608 page table entries      if page table entry is an int =&gt; 33554432 (32 MB) page tables      each process, then, need a 32MB contiguous region in the kernel =&gt; kernel is at least 32MB in size        larger pages =&gt; more internal fragmentation, but smaller pages tables  what if address space is 64 bits?SegmentationThe first, and oldest way to deal with this issue is to segment the address space so that each segment can have its own (smaller) page table. For example, as long as a process is not allowed to map its entire address space, a Linux process really only need two segments: one for the test, data, and heap and another for the stack. Then, the “top” segment (text, data, heap) have a page table that is big enough to map the largest allowable heap address (the largest address that the process will be able to allocate to the heap). Similarly the bottom segment is for the stack and mapps the lowest address that the stack pointer can take on. If the stack is allow to meet the heap, then there is no savings of page table space but if the stack and heap boundaries are fixed, then the total space can be smaller. Each segment, however, requires its own page table resister which must be set for by the OS and the hardware determines which one to use based on which segment is being accessed.For example, let’s imagine that we have a 64-bit address space so that a full page table is not possible. If we assume that the page size is still 512 bits, and page table entry is 4 bytes, and there are two segments for the program (a top and bottom segment) and that we believe that largest a stack will ever get is 16 megabytes, but the test+data+heap could be 1 GB, then the sizes look like  top segment == 2^21 bits of page space x 4 bytes per page table entry == 8 megabytes of page table      bottom (or stack) segment == 2^15 bits of page space x 4 bytes per page table entry == 128 kilobytes of page table space    Hierarchical Page Table    another page table    | 10 bits top level | 44 bits mid-level | 10 bits of offset |-------------------------------------------------------------    address    At the next level there are 44 bits of address space which might look like it is a problem, but actually is not if the OS defines a maximum size for test+data+heap and stack. That is, the top level entry points a page table for the text+data+heap that can be at most 44 bits in size, but could be restricted to be less (say 32 bits in size or 4GB). Similarly the last entry in the top level points to the stack’s page table and it could be similarly restricted. By breaking the address space up and not full mapping the regions, you can map a very large address space without having to create a single contiguous page table.    The disadvantage is that each memory reference must access page table memory multiple times. This process of following a memory reference down through a hierarchical page table is called “walking the page table” and it is a very expensive operation. The hope is that these page table walks are made infrequent by the effectiveness of the TLB.    Inverted Page Table          one physical frame table for all of memory                  entry stores pid and logical page number of the page that is mapped there          frame table is searched for (pid,page number) pair on each ref          offset into frame table gives frame number of physical address when matched (see figure 9.15, page 301)                    advantage is that the frame table need only be big enough to map physical memory of machine      disadvantage is that table must be searched on every access                  hashing          better hope the TLB is working                    UltraSPARC and PowerPC both support – need I say more?      ",
        "url"      : "/2017/06/08/Memory-Management/",
        "date"     : "2017-06-08 15:34:00 +0000"
    } ,
    
    {
        "title"    : "System Calls",
        "category" : "Operating System",
        "tags"     : "Syscall",
        "content": "  System Calls          Getting the process identifier        Files          Reading the data back      Seeking to an offset      Standard In, Standard Out and Standard Error        Fork/Exec/Wait          Fork      Exec      Night of the Living Dead        Pipes          Putting It All Together      System CallsIn the C lecture we discussed the concept of separate compilation and C “libraries.” Functions like printf(), strlen() and strncpy() are all implemented as libraries for Linux. That is, someone, some time wrote the following code (or something like it):int strlen(char *string){    int i;    i = 0;    while(string[i] != 0) {        i++;    }    return(i);}That’s pretty much all strlen() does. As mentioned, this function is included as part of a default library called libc.a that is automatically linked with any C program (by default).There are some functions that your program also gets that are not implemented as libraries. Instead, these functions (which all must manipulate data structures that are shared among all processes in the system) are implemented directly by the operating system. That is, they act like they are loaded as part of a default library, but in fact they are part of the resident OS. These kinds of functions are called system calls because they are calls to the system (the OS).Knowing whether a Linux feature is implemented as a library or system call isn’t exactly easy. As a rule, the section section of the manual (section 2) are the basic system calls. If you typeman 2 readyou will see the system call read() listed, but typeman 3 readand it won’t appear. However,man 3 fwriteshows the man page for the fwrite() library call.This isn’t a very satisfying or accurate way to determine whether a call is a default library call or a system call but it will get you started. Moreover, library calls like printf() may ultimately call system calls like write(). Finally, C was designed before dynamic linking of libraries was commonplace. As a result, before a program is fully loaded, the compiler may not be able to tell whether a call outside the program’s address space is to a library or a system call.Mostly you just need to read the developer documentation to determine what are the system calls that are available in any specific version of Linux (because they change from version to version). We’ll discuss a few system calls that are almost assured implemented in any version of Linux as system calls and not libraries.Getting the process identifierAll processes in Linux get a unique identifier of type pid_t. Printing the value of a pid_t turns out to a matter of some debate. Seriously. I’ll go fast and loose on these examples but someone may give you grief down the road for a specific Linux or Unix version.The system call to get the process identifier under Linux is getpid(). If you typeman getpidyou will see that you need to include sys/types.h to get the type specifier and unistd.h to get the right prototype for the compiler. It takes no arguments and returns a pid_t. Try it out using sys-call1.c:#include &lt; sys/types.h &gt;#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;/* * call getpid() as an example system call */int main(int argc, char **argv){    pid_t my_id;    my_id = getpid();    printf(\"my process id is %ld\\n\",(long)my_id);        return(0);}You’ll just have to trust me that getpid() is implemented as a system call. Only the OS can assign your process a process identifier so to get the identifier, your process must make a call to the OS to get it.FilesOne of the key abstractions that Linux provides is the file system. We’ll talk in depth about how the file system works later in the class. However, from a C perspective it is important that you know how to use files (in their most basic form).There are five system calls associated with files that you should understand as part of basic C and Linux:  open: opens a file specified by the first argument with the “mode” specified in the second argument, and the permissions specified in the third argument. Returns an integer &gt;= 0 when successful and negative when not.  close: closes the file which both tells the OS that your process will no longer access the file (useful for managing internal OS state) and also prevents the file from being accessed further in the process due to a programming error.  read: reads a specified number of bytes (third argument) from a file (first argument) into a buffer (second argument) at “the current offset.”  write: writes a specified number of bytes (third argument), from a buffer (second argument) into a file (first argument) “at the current offset.”  lseek: changes “the current offset.”Either this sounds straight forward or it doesn’t. In reality, like most things with Linux, it is neither.First, you are probably familiar with the notion of a file being visible in the file system when you run the Linux utility ls. Further, that file is uniquely specified by a path name – a string of directories, separated by the “/” character, indicating the path through a directory tree from the root to the file. Turns out that only open() takes a path to the file. Once the file is opened, it must be referred to in your program by its file descriptor which is an integer assigned by the OS when the file is opened. The exact usage will be come clear, but it is important to understand that when you want to do anything other than open a file, you don’t specify the path name, but instead you specify the file descriptor number that came back from the open on the file.Secondly, the OS keeps a “current offset” from the beginning of the file for you while the file is open on a specific file descriptor. When the file is first opened, the offset is zero. Subsequent calls to read() and write() on the file descriptor cause the offset to advance. To make the offset “back up” you either need to close the file, re-open it, and then advance the offset with read calls (which are non-destructive) or use the lseek() call to set the offset to a specific number.Let’s start by writing a program that takes a file name as an argument, creates the file, and writes an important and pitch message into it as a string. Consider the program code available in file-create1.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * open to create a file and write a string into it */int main(int argc, char **argv){     **int my_file_desc;**     char file_name[4096];    char write_buffer[4096];    int write_length;    char *string;    int i;    int w;    if(argc &lt; 2) {        printf(\"need to specify a file name as the first argument\\n\");        exit(1);    }    /*     * zero out the buffer for the file name     */    for(i=0; i &lt; sizeof(file_name); i++) {        file_name[i] = 0;    }    /*     * copy the argument into a local buffer     */    strncpy(file_name,argv[1],sizeof(file_name));    file_name[sizeof(file_name)-1] = 0;    /*     * try and open the file for creation     */     **my_file_desc = open(file_name,O_CREAT | O_WRONLY, 0600);**     if(my_file_desc &lt; 0) {        printf(\"failed to open %s for creation\\n\",file_name);        exit(1);    }    /*     * file is open, write a string into it     */    string =          \"This program brought to you by the council for better C programming\";    for(i=0; i &lt; sizeof(write_buffer); i++) {        write_buffer[i] = 0;    }    strncpy(write_buffer,string,sizeof(write_buffer));    write_length = strlen(write_buffer);     **w = write(my_file_desc,write_buffer,write_length);**     if(w != write_length) {        printf(\"write didn't complete, w: %d, length: %d\\n\",            w,            write_length);        close(my_file_desc);        exit(1);    }    close(my_file_desc);    return(0);}First, try running it as./file-create1 ./foo.txtThen, look in the current directory for a file called foo.txt. What does it contain? Now try:./file-create1 /foo.txtThis attempt should fail because you tried to create a file called “foo.txt” in the root directory and you don’t have write permissions for that directory (hopefully).Next, look at the line where open() is called in the code:my_file_desc = open(file_name,O_CREAT | O_WRONLY, 0600);This line says to open the file specified by the string contained in the array file_name when treating that string as a path in the file system. If you run it with foo.txt or ./foo.txt it will use the current working directory as the place where you try and create foo.txt.The second argument requires that you look at the man page for openman 2 openThe second argument to open specifies how you want the file opened. In this case, I specified that I wanted the file created if it doesn’t exist but simply opened if it did. I also specified that I wanted only to write the file. If I had tried to call read on my_file_desc at some point after the open, the read would fail.Notice that if I run the program twice./file-create1 foo.txt./file-create1 foo.txtI still only get one copy of the file with one string in it. That’s because the open() sets the current offset to zero each time. When I run this program the first time, it creates foo.txt. Each time after that it just overwrites the file with exactly the same string.The third argument says that I want to create the file with the permissions “600” – or RW for owner only. Seeman 2 chmodfor details on specifying permissions. It is possible to use constants for these instead of numbers but I think of them as numbers.Finally, notice that the open returns an integer (the type of my_file_desc is int). That value is passed to write() to indicate which file I want to write. If the code had opened two different files, each would have a different file descriptor number. For example, as in file-create2.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * open to create a couple of files and write a strings into each */int main(int argc, char **argv){    int fd1;    int fd2;    char file_name1[4096];    char file_name2[4096];    char write_buffer[4096];    int write_length;    char *string;    int i;    int w;    if(argc &lt; 3) {        printf(\"need to specify two file names as the first and second arguments\\n\");        exit(1);    }    /*     * zero out the buffers for the file names     */    for(i=0; i &lt; sizeof(file_name1); i++) {        file_name1[i] = 0;        file_name2[i] = 0;    }    /*     * copy the first argument into a local buffer     */    strncpy(file_name1,argv[1],sizeof(file_name1));    file_name1[sizeof(file_name1)-1] = 0;    /*     * and the second     */    strncpy(file_name2,argv[2],sizeof(file_name2));    file_name2[sizeof(file_name2)-1] = 0;    /*     * try and open the first file for creation     */     **fd1 = open(file_name1,O_CREAT | O_WRONLY, 0600);**     if(fd1 &lt; 0) {        printf(\"failed to open %s for creation\\n\",file_name1);        exit(1);    }    /*     * and the second     */     **fd2 = open(file_name2,O_CREAT | O_WRONLY,  0600);**     if(fd2 &lt; 0) {        printf(\"failed to open %s for creation\\n\",file_name2);        close(fd1);        exit(1);    }    /*     * both files are open, write a string into first     */    string =          \"This program brought to you by the council for better C programming\";    for(i=0; i &lt; sizeof(write_buffer); i++) {        write_buffer[i] = 0;    }    strncpy(write_buffer,string,sizeof(write_buffer));    write_length = strlen(write_buffer);     **w = write(fd1,write_buffer,write_length);**     if(w != write_length) {        printf(\"write didn't complete, w: %d, length: %d in %s\\n\",            w,            write_length,            file_name1);        close(fd1);        close(fd2);        exit(1);    }    /*     * and the second     */    string = \"C programming is both nutritious and great tasting.\";    for(i=0; i &lt; sizeof(write_buffer); i++) {        write_buffer[i] = 0;    }    strncpy(write_buffer,string,sizeof(write_buffer));    write_length = strlen(write_buffer);     **w = write(fd2,write_buffer,write_length);**     if(w != write_length) {        printf(\"write didn't complete, w: %d, length: %d in %s\\n\",            w,            write_length,            file_name2);        close(fd1);        close(fd2);        exit(1);    }    close(fd1);    close(fd2);    return(0);}You should read through this example and notice that the two files are referenced with two different file descriptors.Reading the data backNow let’s try reading the data back from a file, as in file-read1.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * open and read the contents of a file, printing them out as ascii * characters */int main(int argc, char **argv){    int my_file_desc;    char file_name[4096];    char read_buffer[4096];    int i;    int r;    if(argc &lt; 2) {        printf(\"need to specify a file name as the first argument\\n\");        exit(1);    }    /*     * zero out the buffer for the file name     */    for(i=0; i &lt; sizeof(file_name); i++) {        file_name[i] = 0;    }    /*     * copy the argument into a local buffer     */    strncpy(file_name,argv[1],sizeof(file_name));    file_name[sizeof(file_name)-1] = 0;    /*     * try and open the file for reading     */     **my_file_desc = open(file_name,O_RDONLY,0);**     if(my_file_desc &lt; 0) {        printf(\"failed to open %s for reading\\n\",file_name);        exit(1);    }    for(i=0; i &lt; sizeof(read_buffer); i++) {        read_buffer[i] = 0;    }     **r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);**     printf(\"file: %s contains the string: %s\\n\",        file_name,        read_buffer);    close(my_file_desc);    return(0);}Here the open call is a little different:    my_file_desc = open(file_name,O_RDONLY,0);indicating that the file is to be open for reading only. The permissions will be checked but no permissions need to be specified since the file will not be created if it it isn’t present already.Also, the read is a little different    r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);In the case of read() the system call will read bytes up to the number specified in the third argument. If there are fewer than that number of bytes, it will return the ones it read and leave the current offset at the end of the file. Another read at the end will return a zero indicating that the file is empty.If you wanted to read the entire contents of the file, then you will need to loop until there is no more data as in file-read2.c.#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * read the file in its entirety (like the cat utility) */int main(int argc, char **argv){    int my_file_desc;    char file_name[4096];    char read_buffer[4096];    int i;    int r;    if(argc &lt; 2) {        printf(\"need to specify a file name as the first argument\\n\");        exit(1);    }    /*     * zero out the buffer for the file name     */    for(i=0; i &lt; sizeof(file_name); i++) {        file_name[i] = 0;    }    /*     * copy the argument into a local buffer     */    strncpy(file_name,argv[1],sizeof(file_name));    file_name[sizeof(file_name)-1] = 0;    /*     * try and open the file for reading     */    my_file_desc = open(file_name,O_RDONLY,0);    if(my_file_desc &lt; 0) {        printf(\"failed to open %s for reading\\n\",file_name);        exit(1);    }    for(i=0; i &lt; sizeof(read_buffer); i++) {        read_buffer[i] = 0;    }     **r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);    /*     * read and print until EOF     */    while(r &gt; 0) {        printf(\"%s\",read_buffer);        for(i=0; i &lt; sizeof(read_buffer); i++) {            read_buffer[i] = 0;        }        r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);    }**     close(my_file_desc);    return(0);}This is essentially what the Linux utility cat does although cat probably does it in a more elegant way. Try it out on a text file that is bigger than 4K bytes to assure yourself that it works correctly.For example, try./file-read2 /cs/faculty/rich/public_html/class/cs170/notes/C/index.htmland see if you get the text from the HTML for this web page back.Seeking to an offsetThe lseek() system call moves the current offset pointer by the number of bytes specified from a starting location that is taken from its third argument. For example, consider file-seek1.c.#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * seek the file in the first argument to the location specified in the * second */int main(int argc, char **argv){    int my_file_desc;    char file_name[4096];    char read_buffer[4096];    int i;    int r;    off_t offset;    off_t where;    if(argc &lt; 3) {        printf(\"need to specify a file name and an offset\\n\");        exit(1);    }    /*     * zero out the buffer for the file name     */    for(i=0; i &lt; sizeof(file_name); i++) {        file_name[i] = 0;    }    /*     * copy the argument into a local buffer     */    strncpy(file_name,argv[1],sizeof(file_name));    file_name[sizeof(file_name)-1] = 0;    /*     * get the offset from the second argument     */     **offset = (off_t)atoi(argv[2]);**     /*     * try and open the file for reading     */    my_file_desc = open(file_name,O_RDONLY,0);    if(my_file_desc &lt; 0) {        printf(\"failed to open %s for reading\\n\",file_name);        exit(1);    }    /*     * seek to the offset specified in the second argument     */     **where = lseek(my_file_desc,offset,SEEK_SET);**     if(where &lt; 0) {        printf(\"lseek to %d in file %s failed\\n\",            (int)offset,            file_name);        close(my_file_desc);        exit(1);    }    for(i=0; i &lt; sizeof(read_buffer); i++) {        read_buffer[i] = 0;    }     **r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);    /*     * read and print until EOF     */    while(r &gt; 0) {        printf(\"%s\",read_buffer);        for(i=0; i &lt; sizeof(read_buffer); i++) {            read_buffer[i] = 0;        }        r = read(my_file_desc,read_buffer,sizeof(read_buffer)-1);    }**     close(my_file_desc);    return(0);}The linewhere = lseek(my_file_desc,offset,SEEK_SET);after the call to open() but before the first call to read() moves the current offset to the byte offset specified in the second argument. The SEEK_SET parameter says to “set” the offset to the value specified (as opposed to setting it relative to the current offset). To see other options for this parameter, check out the man page.Now try running the following:./file-create1 foo.txt./file-seek1 foo.txt 10You should see something likeam brought to you by the council for better C programmingwhere the first 10 bytes of the line are missing from the line in the file foo.txt. Does this make sense as the output?Standard In, Standard Out and Standard ErrorTake a look at the code in file-fd1.c.#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * print out some file descriptors */int main(int argc, char **argv){    int my_file_desc;    char file_name[4096];    int i;    int r;    char *string;    if(argc &lt; 2) {        printf(\"need to specify a file name\\n\");        exit(1);    }    /*     * zero out the buffer for the file name     */    for(i=0; i &lt; sizeof(file_name); i++) {        file_name[i] = 0;    }    /*     * copy the argument into a local buffer     */    strncpy(file_name,argv[1],sizeof(file_name));    file_name[sizeof(file_name)-1] = 0;    /*     * try and open the file for reading     */     **my_file_desc = open(file_name,O_RDONLY,0);**     if(my_file_desc &lt; 0) {        printf(\"failed to open %s for reading\\n\",file_name);        exit(1);    }    printf(\"my_file_desc: %d\\n\",my_file_desc);    string = \"a string written to standard out\\n\";     **write(1,string,strlen(string));**     close(my_file_desc);    return(0);}It should look a little alarming to you especially with respect to the write() call. Try running it after creating the file foo.txt:./file-create1 foo.txt./file-fd1 foo.txt my_file_desc: 3a string written to standard outWhat happened here? First, notice that the value of file descriptor returned by the open call and stored in the integer variable my_file_desc is 3. Turns out that this number is not just a random integer selected by Linux. Next, and perhaps most curiously, in the code the call to write() writes a string out to file descriptor that wasn’t opened.        string = \"a string written to standard out\\n\";        write(1,string,strlen(string));In fact, it is just writing to file descriptor 1 without any other reference to 1 as a file descriptor.Linux automatically “opens” three file descriptors – 0, 1, and 2 for you when a process is launched and connects these file descriptors to the keyboard and the terminal. This feature explains the first line of output in that the smallest file descriptor that can be returned from a call to open() is 3 since file descriptors 0, 1, and 2 are already opened. Linux could have chosen any other integer when it chose a file descriptor for the call to open() in the code. For various historical reasons it will choose the smallest unused file descriptor. When the program launches, that file descriptor number will be 3. If another call to open were made, that file descriptor would be 4. If close() is called on my_file_desc and then open is called again, 3 will be chosen since at that moment it would be the smallest unused file descriptor.The three “special” file descriptors Linux opens for you at process launch are  File descriptor 0: connected to the keyboard and is referred to as standard in.  File descriptor 1: connected to the terminal and is referred to as standard out  File descriptor 2: also connected to the terminal and is referred to as standard errorThe last two are separated so that the shell can send errors to the terminal even if you redirect the standard output to a file.Thus, the call to write tells the OS to write from the string buffer the strlen() of the string to the standard out device which (unless you closed it) will be the terminal. Notice that the call to write() doesn’t know whether the destination is standard out or a file – it just writes the data to the file that the OS has associated with the file descriptor and that “file” has cleverly been impersonated by the terminal.To see the difference between standard out and standard error try running the code in file-fd2.c#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * print out some file descriptors */int main(int argc, char **argv){    char *string;    char *string_err;    string = \"a string written to standard out\\n\";     **write(1,string,strlen(string));**     string_err = \"a string written to standard error\\n\";     **write(2,string_err,strlen(string_err));**     return(0);}First, run it from the terminal and then run it, but redirect (using the shell) the output to a file:./file-fd2a string written to standard outa string written to standard error./file-fd2 &gt; fd2.outa string written to standard errorcat fd2.outa string written to standard outIn the first execution, both standard out and standard error were sent to the terminal. In the second, standard out the shell opened a file and sent the output on standard out to the file. The &gt; operator in the shell implements this file writing function. However standard error was not redirected so the string written on file descriptor 2 was sent to the terminal.Standard in works the same way, but it needs to know when there is no more input from the keyboard. When you tun the code in file-fd3.c#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * read from stdin and write to stdout */int main(int argc, char **argv){    char buffer[4096];    memset(buffer,0,sizeof(buffer));    read(0,buffer,sizeof(buffer));    write(1,buffer,strlen(buffer));    return(0);}Notice that this codes uses the function memset() to zero out the buffer. It reads up tot he buffer’s size from standard in (the keyboard) and then writes it to the terminal. However, when you run the program, you need to type the character ctrl-D before the program will finish. Otherwise, it waits, blocked in the read call, waiting for another character.I ran it, types the characters in my dog is happy! and then ctrl-D../file-fd3my dog is happy!my dog is happy!The ctrl-D character tells the shell that the end of file has been reached which, for standard in means that no more characters will be coming from the keyboard. The program then writes the wring back out to standard out.The shell can also redirect data from a file to standard in. Using the file from the previous example, try./file-fd3 &lt; fd2.outwhich causes the contents fo the file fd2.out to be sent to standard in of file-fd3 until the end-of-file is reached. The read then terminates the data is written to standard out.Fork/Exec/WaitWhen you log into a Linux system and you get a prompt, the program that is running is called “the shell.” It is usually a BASH shell (see the man page) and its job is to create processes on your behalf. Some of those processes then run Linux utilities for you. For example, when you typels ~you are telling the shell to run a program called ls (which is located in the /usr/bin directory) and to pass it, as its first argument, the path to your home directory. The shell expands the ~ character into a path, but ls is a C program that someone wrote for your Linux system. It was compiled when your Linux was built and it is placed in /usr/bin when Linux is installed. The shell looks in a few places (defined by the PATH environment variable) for programs to run when you simply give their names, like I have in this example with ls.But how does the shell (or any other C program since the shell, itself, is just a program written in C and compfile for Linux) create a process and run another program that has been compiled?ForkThe way one program runs another in Linux is a little odd. First, it makes an exact copy of itself using the fork() system call. When fork() completes, there are two copies of the same program. Then, the second copy calls exec() which loads the binary for the second program over itself and starts it from the beginning. I know. It gets easier once you see it.Take a look at the code in fork-1.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * simple program calling fork */int main(int argc, char **argv){    pid_t child_id;    pid_t my_id;    my_id = getpid();    printf(\"pid: %d -- I am the parent about to call fork\\n\",            (int)my_id);     **child_id = fork();    if(child_id != 0) {        my_id = getpid();        printf(\"pid: %d -- I just forked a child with id %d\\n\",            (int)my_id,            (int)child_id);    } else {        my_id = getpid();        printf(\"pid: %d -- I am the child\\n\",my_id);    }**     printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}The call to fork() creates an exact copy of the process calling fork except for the process identifier which is different since it is a new process. The interesting part is that this new process (typically called the child process) begins running at the instruction immediately after the fork() call in the program. That is, after the call to fork() completes, there are two processes running and they both execute the next line which isif(child_id != 0) {which is pretty typical. The call to fork() returns the process identifier to of the newly created process to the parent, but returns zero to the child. That way the code can have the child and parent diverge and execute different code paths. In this example, the parent and child print different messages after the child has been forked. The output I get is./fork-1pid: 40285 -- I am the parent about to call forkpid: 40285 -- I just forked a child with id 40286pid: 40285 -- I am exitingpid: 40286 -- I am the childpid: 40286 -- I am exitingOn each line, the code prints the process identifier of the process calling printf(). Thus the parent or origibal process is process 40285. It creates a child process and Linux chooses 40286 for that process’ identifier which is returned from the call to fork() to the parent. The parent then prints a message indicating that it has created the child and the child prints a message with its identifier. Both then print before they exit.The parent can wait for the child to complete using the wait() system call. Take a look at fork-2.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * simple program calling fork and wait */int main(int argc, char **argv){    pid_t child_id;    pid_t my_id;    int child_status;    int i;    child_id = fork();    if(child_id != 0) {        my_id = getpid();        printf(\"pid: %d -- I just forked a child with id %d\\n\",            (int)my_id,            (int)child_id);        printf(\"pid: %d -- I am waiting for process %d to finish\\n\",            (int)my_id,            (int)child_id);         **wait(&amp;child_status);**         printf(\"pid: %d -- my child has completed with status: %d\\n\",            (int)my_id,            child_status);    } else {        my_id = getpid();        printf(\"pid: %d -- I am the child and I am count to 10\\n\",            (int)my_id);        for(i=0; i &lt; 10; i++) {            printf(\"pid: %d -- %d\\n\",my_id,i+1);        }    }    printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}Running this version yields:./fork-2pid: 74869 -- I just forked a child with id 74870pid: 74869 -- I am waiting for process 74870 to finishpid: 74870 -- I am the child and I am going to count to 10pid: 74870 -- 1pid: 74870 -- 2pid: 74870 -- 3pid: 74870 -- 4pid: 74870 -- 5pid: 74870 -- 6pid: 74870 -- 7pid: 74870 -- 8pid: 74870 -- 9pid: 74870 -- 10pid: 74870 -- I am exitingpid: 74869 -- my child has completed with status: 0pid: 74869 -- I am exitingNotice the process identifiers. The parent waits until the child exits and then continues after it has done so.Notice also that the call to wait() takes a pointer to an integer as a parameter. The wait() call uses this parameter as an out parameter to deliver an integer to the parent indicating the child’s exit status.The exit status in the child is given in the argument to exit() which, in this example, is 0. Most Linux utilities return an exit status of 0 when the complete successfully and a small positive integer (usually 1 or 2) when an error has occured. If the call to exit had beenexit(1);the parent would see the value of the integer variable child_status set to 1. You should probably adopt this convention with respect to exit status (at least, as far as returning a 0 when everything went okay) because many if not most shell scripts use the exit status to determine if an error in some utility has occured.ExecThe fork() clones the running process, but often what you need is to execute (as its own process) a program that is located in a file in the file system.While the function of fork() has remained relatively constant over the years, exec() has undergone a few changes since it was first defined. The version we will discuss is execve() since its definition is rather clear. To be complete, you should look at the man pages forman 2 execveman 2 execlThe latter call will decsribe different variants of exec() each of which has its own peculiarities. For example, when you give the filename of the program you wish to have loaded by exec() you may want the search path to be the same search path as the shell uses when it ran the program that is calling exec(). See the second man entry for details.However, we will constrain our remarks to execve() for the sakes both of simplicity and brevity.Consider the code contained in fork-3.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * run a program using fork and execve */int main(int argc, char **argv, char **envp){    pid_t child_id;    pid_t my_id;    int child_status;    int i;    char file_name[4096];    int err;    if(argc &lt; 2) {        printf(\"must specify file name as first argument\\n\");        exit(1);    }    memset(file_name,0,sizeof(file_name));    strncpy(file_name,argv[1],sizeof(file_name));    child_id = fork();    if(child_id != 0) {        my_id = getpid();        printf(\"pid: %d -- I forked pid: %d for: %s\\n\",            my_id,            child_id,            file_name);        wait(&amp;child_status);        printf(\"pid: %d -- %s has completed with status: %d\\n\",            (int)my_id,            file_name,            child_status);    } else {        my_id = getpid();        printf(\"pid: %d -- I am the child and I am going to exec %s\\n\",            (int)my_id,            file_name);         **err = execve(file_name,&amp;(argv[1]),envp);        /*         * not reached if execve is successful         */        printf(\"pid: %d -- execve of %s failed with error %d\\n\",**             (int)my_id,            file_name,            err);    }    printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}You should read this code carefully as it contains a few subtleties. First, notice that it forks a child and then the child calls execve() to run a program. The string file_name is used to tell execve() what program to run. Also, the parent program passes &amp;(argv[1]) as the second argument to execve() and the environment pointer for the main program. Finally, notice that the error message is printed immediately after the call to execve() without testing to see if the value is non-zero.Let’s take these observations one at a time.The first argument to execve() really needs to be a path to the file in the file system that contains an executable program. Thus, it will need to contain something like /bin/hostname or ./fork-1 – a complete path name to the binary.Next, execve() takes a list of arguments that it will pass (as the char *argv[] parameter) to the main() function of the program that is being run. In this example, the first argument passed to fork-3 will be the path name to the binary to execute and the remaining arguments that need to be pass to it to do its job.So, for example, to get fork-3 to run /bin/ls on the /tmp directory you would execute./fork-3 /bin/ls /tmpand /tmp will need to be passed to /bin/ls through the call to execve(). Further, like argv in the parent, the argv[0] passed to the program that is being run must be the path name of the binary. Think about this for a minute. It will make sense after a bit.Try a few examples:./fork-3 /bin/datepid: 75976 -- I forked pid: 75977 for: /bin/datepid: 75977 -- I am the child and I am going to exec /bin/dateWed Jan 14 15:01:23 PST 2015pid: 75976 -- /bin/date has completed with status: 0pid: 75976 -- I am exitingTake a look at the process identifiers. Notice that the parent forks the child and waits, the child calls execve() and when /bin/date has completed, the parent unblocks from its call to wait() getting back the exit status generated by /bin/date.Notice also that the message immediately after the call to execve() in the child is not printed. That’s because if the call to execve() is successful, the child is completely overwritten by the new code and, thus, stops executing its own code. If the call to execve() fails, however, then the child continues to execute. For example,./fork-3 datepid: 76043 -- I forked pid: 76044 for: datepid: 76044 -- I am the child and I am going to exec datepid: 76044 -- execve of date failed with error -1pid: 76044 -- I am exitingpid: 76043 -- date has completed with status: 0pid: 76043 -- I am exitingHere, the path name to /bin/date is not fully specified. The system call execve() does not use the $PATH environment variable to determine what to run. The simply specifying date causes execve() to fail because it can’t find date in the file system. Try it with /usr/date./fork-3 /usr/datepid: 76096 -- I forked pid: 76097 for: /usr/datepid: 76097 -- I am the child and I am going to exec /usr/datepid: 76097 -- execve of /usr/date failed with error -1pid: 76097 -- I am exitingpid: 76096 -- /usr/date has completed with status: 0pid: 76096 -- I am exitingand it fails the same way because there is no file called date in /usr.Now try it with /bin/ls./fork-3 /bin/ls -al fork-3.cpid: 76113 -- I forked pid: 76114 for: /bin/lspid: 76114 -- I am the child and I am going to exec /bin/ls-rw-r--r--  1 rich  staff  1118 Jan 14 14:52 fork-3.cpid: 76113 -- /bin/ls has completed with status: 0pid: 76113 -- I am exitingCompare that output to/bin/ls -al fork-3.c-rw-r--r--  1 rich  staff  1118 Jan 14 14:52 fork-3.cNifty. In fact, fork-3 is doing what /bin/bash does when you tell the shell to run /bin/ls – it is more or less the same logic.Oh-oh. Buckle up.MossPiglet% ./fork-3 /bin/bashpid: 76193 -- I forked pid: 76194 for: /bin/bashpid: 76194 -- I am the child and I am going to exec /bin/bashMossPiglet%What happened here? It started running bash and bash is blocked waiting for my input. Notice that the parent is still waiting for the child to exit. The child called execve() on /bin/bash and bash ran and is waiting for my input. If I type exit or ctrl-D that bash will exit, the parent will wake up, and fork-3 will exit.MossPiglet% ./fork-3 /bin/bashpid: 76241 -- I forked pid: 76242 for: /bin/bashpid: 76242 -- I am the child and I am going to exec /bin/bashMossPiglet% exitexitpid: 76241 -- /bin/bash has completed with status: 0pid: 76241 -- I am exitingWhy did this work? Keep in mind that the child overwrote itself with the code from the executable binary located in the file /bin/bash but it inherits everything else from the parent. In particular, it gets the same open file descriptors and they remain open. What open file descriptors does bash need? Standard in, standard out, and standard error. Thus, bash gets the same standard in, standard out, and standard error that fork-3 had and, thus, it works just fine as its own shell. Only when you call exit the fork-3 process is waiting and it wakes up to get the exit status. Try it with a different status:./fork-3 /bin/bashpid: 76401 -- I forked pid: 76402 for: /bin/bashpid: 76402 -- I am the child and I am going to exec /bin/bashMossPiglet% exit 1exitpid: 76401 -- /bin/bash has completed with status: 256pid: 76401 -- I am exitingWhoops. Why did 256 come back as a status instead of 1?Turns out _wait()_encodes the exit status in the status integer. The correct way to manage it is depcited in fork-4.c:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;#include &lt; sys/wait.h &gt;/* * run a program using fork and execve * pring the status correctly */int main(int argc, char **argv, char **envp){    pid_t child_id;    pid_t my_id;    int child_status;    int i;    char file_name[4096];    int err;    if(argc &lt; 2) {        printf(\"must specify file name as first argument\\n\");        exit(1);    }    memset(file_name,0,sizeof(file_name));    strncpy(file_name,argv[1],sizeof(file_name));    child_id = fork();    if(child_id != 0) {        my_id = getpid();        printf(\"pid: %d -- I forked pid: %d for: %s\\n\",            my_id,            child_id,            file_name);        wait(&amp;child_status);         **if(WIFEXITED(child_status)) {            printf(\"pid: %d -- %s has completed with status: %d\\n\",                (int)my_id,                file_name,                WEXITSTATUS(child_status));        }**     } else {        my_id = getpid();        printf(\"pid: %d -- I am the child and I am going to exec %s\\n\",            (int)my_id,            file_name);        err = execve(file_name,&amp;(argv[1]),envp);        /*         * not reached if execve is successful         */        printf(\"pid: %d -- execve of %s failed with error %d\\n\",            (int)my_id,            file_name,            err);    }    printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}Turns out that the parent will wake up on conditions other than the child exiting (like the child gets a signal). You can test to see why the child woke up with and if it exited gets its one byte of status using the WIFEXITED() and WEXITSTATUS() macros respectively. They com in the header file sys/wait.h. Consult the man pages for further details.Night of the Living DeadThis exit status processing can cause a bit of an issue. In fact, if the parent doesn’t call wait() and the child exits, Linux creates a zombie process which waits around for the parent only so that it can report its exit status. If the parent never calls wait(), the zombie never truly dies. It can’t do anything else, but Linux won’t clean up its process state until the parent calls wait().Or until the parent dies. When the parent of a child dies, the child is usually adopted by a special Linux process called init. Every running Linux system has an init process. After the system boots, the job of init is just to call wait() so that orphaned children can report their exit status and avoid becoming zombies, or cease to be zombies.For example, a parent that creates a child and then goes into an infinite loop before calling wait() creates a zombie when that child exits. If you kill the parent with a ctrl-C, the zombie child will immediately be adopted by init which is calling wait(). The exit status will be reported to init (which discards it) and the zombie child finally dies completely. The init process goes back to waiting for other orphaned children.Seriously.Like with many of the original Unix semantics, this behavior (which dates to early versions of Unix) can be modified in the current implementation of Linux. The default behavior is for init to adopt all parentless children. However using the prctl() system call with the PR_SET_CHILD_SUBREAPER argument you can set a process to be available as an adopter of parentless children. When called all descendents of the process will inherit this process as their “reaper” should a parent die. In the event of a parent termination, the nearest anscestor in the process tree that is a subreaper for the descendents becomes the reaper. However if no subreapers are in the process tree, then init defaults to being th reaper.Um. Yeah.PipesPipes are a way for two processes to communicate. When you create a pipe, you give the pipe() system call an array of two integers that it uses as an out parameter. It allocates two file descriptors (one for reading and the other for writing). Any data written to the write file descriptor will be available for reading on the read file descriptor. If the read and and write ends are in separate processes, the transfers the data from the writer process to the reader process.Because they are represented using file descriptors, the same read() and write() calls that work for files also work for pipes. You can’t call lseek() on them and you used the pipe() call to open then instead of calling open() but once they are set up, they behave like files from a usage perspective.In pipe-1.c, the code opens a pipe, writes a string to the “write end”, and then reads the “read end” to get what ever is in the pipe (up to the size of the read buffer).#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * simple program to create and use a pipe */int main(int argc, char **argv){    int pipe_desc[2];    int err;    char *string;    char read_buffer[4096];     **err = pipe(pipe_desc);**     if(err &lt; 0) {        printf(\"error creating pipe\\n\");        exit(1);    }    string = \"a string\";    printf(\"writing %s to pipe_desc[1] which is %d\\n\",            string,pipe_desc[1]);    write(pipe_desc[1],string,strlen(string));    memset(read_buffer,0,sizeof(read_buffer));    printf(\"attempting to read pipe_desc[0] which is %d\\n\",pipe_desc[0]);    read(pipe_desc[0], read_buffer, sizeof(read_buffer));    printf(\"read %s from pipe_desc[0]\\n\",read_buffer);    close(pipe_desc[0]);    close(pipe_desc[1]);    return(0);}The code simply opens a pipe, writes a string into it using the write() system call and then reads the string back from the pipe.There are a couple of subtlties here, though. First, while the write end “knows” how much data it will write, the read end doesn’t. In this example, I made the read buffer 4096 bytes in length. If the writer had written more than 4096 bytes, the reader would have only read the first 4096 assuming that the pipe is big enough to hold 4096 bytes. The semantics are that the pipe has some capacity (which is not known) and if the writing program exceeds that capacity it blocks under a reader has drained the pipe there by freeing some capacity.In other words, a pipe is the Linux system call implementation of the bounded buffer problem we discussed in the lecture on condition variables. Notice that in this example, the process could “deadlock” itself by writing more into the pipe than the pipe can hold.Pipes are really designed to be used between processes so having the writing process block until a reading process drains some of the data from the pipe is necessary.Similarly, the reader of read end of the pipe will block if there is no data to read in the pipe. However, if and when the write end is closed, the read end will unblock and the read will read no data. This interaction between reading a pipe, blocking, and closing a pipe will become clearer as we look at pipes between processes. The easiest way to think of it, though, is to realize that the reader of a pipe waiting for data would wait forever if the writer died without a way to inform the reader that the write end will never produce more data.Usually a pipe is established between processes that wish to communicate rather than within a single process (as in the previous example). In pipe-2.c#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;#include &lt; sys/wait.h &gt;/* * simple program creating a pipe between two processes */int main(int argc, char **argv){    pid_t child_id;    pid_t my_id;    int pipe_desc[2];    char *string;    char read_buffer[4096];    int child_status;    int err;    /*     * create the pipe     */    err = pipe(pipe_desc);    if(err &lt; 0) {        printf(\"error creating pipe\\n\");        exit(1);    }    /*     * then fork     */    child_id = fork();    if(child_id != 0) {        /*         * parent will be the writer         * doesn't need the read end         */        my_id = getpid();        close(pipe_desc[0]);        /*         * send the child a string         */        string = \"a string made by the parent\\n\";        printf(\"pid: %d -- writing %s to pipe_desc[1]\\n\",            (int)my_id,            string);         **write(pipe_desc[1],string,strlen(string));**         /*         * close the pipe to let the read end know we are         * done         */        close(pipe_desc[1]);        /*         * wait for the child to exit         */        wait(&amp;child_status);    } else {        /*         * child reads the read end         */        my_id = getpid();        /*         * doesn't need the write end         */        close(pipe_desc[1]);        memset(read_buffer,0,sizeof(read_buffer));         **read(pipe_desc[0],read_buffer,sizeof(read_buffer));**         printf(\"pid: %d -- received %s from parent\\n\",                (int)my_id,                read_buffer);        close(pipe_desc[0]);    }    printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}Notice that the code creates a pipe before it calls fork(). As a result, both the parent and child process have the pipe open on the same set of file descriptors. It is good programming practice to close the pipe descriptors in a process that are not being used. Thus, the parent closes the read end (because it writes in this example) and the child closes the write end (because it reads). The parent closes the pipe after the write and then waits for the child to exit.To see the close of write end trigger the reading end to exit, take a look at the code in pipe-3.c#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;/* * simple program creating a pipe between two processes */int main(int argc, char **argv){    pid_t child_id;    pid_t my_id;    int pipe_desc[2];    char *string;    char read_buffer[4096];    int err;    /*     * create the pipe     */    err = pipe(pipe_desc);    if(err &lt; 0) {        printf(\"error creating pipe\\n\");        exit(1);    }    /*     * then fork     */    child_id = fork();    if(child_id != 0) {        /*         * parent will be the writer         * doesn't need the read end         */        my_id = getpid();        close(pipe_desc[0]);        /*         * send the child a string         */        string = \"a string made by the parent\\n\";        printf(\"pid: %d -- writing %s to pipe_desc[1]\\n\",            (int)my_id,            string);        write(pipe_desc[1],string,strlen(string));        /*         * and another string         */        string = \"and another string\";        printf(\"pid: %d -- writing %s to pipe_desc[1]\\n\",            (int)my_id,            string);        write(pipe_desc[1],string,strlen(string));         **/*         * fall off the end to close         */**     } else {        /*         * child will read until pipe closes         * close the write end         */        my_id = getpid();        close(pipe_desc[1]);        memset(read_buffer,0,sizeof(read_buffer));         **while(read(pipe_desc[0],read_buffer,sizeof(read_buffer))) {            printf(\"pid: %d -- received %s from parent\\n\",                (int)my_id,                read_buffer);            memset(read_buffer,0,sizeof(read_buffer));        }**         printf(\"pid: %d -- child detects write end closed\\n\",            (int)my_id);        close(pipe_desc[0]);    }    printf(\"pid: %d -- I am exiting\\n\",my_id);    exit(0);}Here, the child reads the pipe in a while loop looking for the read to complete with a value of zero (indicating end-of-file). When the parent closes the write end, the read end will deliver EOF after it has delivered what ever data is in the pipe. That is, the close and exit of the parent does not prevent the data in transit from being delivered.The output./pipe-3pid: 98839 -- writing a string made by the parent to pipe_desc[1]pid: 98839 -- writing and another string to pipe_desc[1]pid: 98839 -- I am exitingpid: 98840 -- received a string made by the parentand another string from parentpid: 98840 -- child detects write end closedpid: 98840 -- I am exitingalso shows the parent writing both strings and exiting before the child runs. The child gets both strings (but in one read call – not two) then detects the close of the write end as a zero rerturned from read (causing it to fall out of the while loop).Putting It All TogetherAt this point, you have enough to understand how to write a program that creates arbitrary chains of pipes that communicate via processes that use Standard In and Standard Out.Consider the code in my-cat.c which is a program that implements functionality similar to the Linux cat utility:#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;#include &lt; string.h &gt;/* * simple program implementing cat */int main(int argc, char **argv){    pid_t my_id;    char buffer[4096];    my_id = getpid();    fprintf(stderr,\"pid: %d -- I am my-cat and I have started\\n\",my_id);    memset(buffer,0,sizeof(buffer));    while(read(0,buffer,sizeof(buffer)) &gt; 0) {        fprintf(stderr,\"pid: %d read some data\\n\",getpid());        buffer[4095] = 0; /* safety first */        write(1,buffer,strlen(buffer));        fprintf(stderr,\"pid: %d wrote some data\\n\",getpid());        memset(buffer,0,sizeof(buffer));    }    fprintf(stderr,\"pid: %d -- I am my-cat and I am exiting\\n\",my_id);    exit(0);}It simply reads data from Standard In (that it assumes is string data) and echos it to Standard Out. It also prints a message when it starts and when it ends. If you run it from the command line./my-catpid: 33672 -- I am my-cat and I have startedI love the OS class more than any other class!pid: 33672 read some dataI love the OS class more than any other class!pid: 33672 wrote some dataIt is really great to know how to program in C and to understand Linux!pid: 33672 read some dataIt is really great to know how to program in C and to understand Linux!pid: 33672 wrote some dataIt is really great to know how to program in C and to understand Linux!pid: 33672 read some dataIt is really great to know how to program in C and to understand Linux!pid: 33672 wrote some datapid: 33672 -- I am my-cat and I am exitingit will echo each line you type, one at a time, until you type ctrl-D.Wait.One line at a time? Why? That is, why doesn’t it wait until I type all three lines and a ctrl-D before it echos the buffer?The answer is that the newline character on the input line causes the call to read() to complete and return. It still what you want. Try putting the three different sentences in a .txt file and running ./my-cat &lt; my.txt. It should just print the lines out in the order they appear.Also notice that the messages my-cat prints always go to the terminal because I print them to Standard Error. I use Standard Error in this for reasons that will become clear in a moment. However, notice that my-cat prints its pid every time it reads something or writes something.To create a chain of my-cat process, each of which reads from the one before it in the chain and write to the one after it in the chain, you need to  create a pipe to serve as the communication channel between processes  in the up-stream process make make stdout write the write end of the pipe  in the down stream process make stdin read in the read end of the pipe  repeatYou also need to make sure that the first process in the chain gets Standard In from the original process (i.e the terminal) and the last process in the chain gets the Standard Out from the original process.To change file descriptors you need to use the system calls dup() or dup2(). I prefer dup2() because it is more explicit. These calls duplicate an open file descriptor. The dup() call will choose the lowest numbered file descriptor that is unopened as the target and dup2() lets you specify which file descriptor you want to use as the target.For exampleint fd = open(\"./my-file.txt\",O_RDWR);close(0);dup(fd);will set the Standard In file descriptor to be the same file descriptor as fd. Thus when your process calls read(0,…) it will not be reading from the terminal any longer but from the file. The sequenceint fd = open(\"./my-file.txt\",O_RDWR);close(0);dup2(fd,0);does the same. If you leave out the call to close() for the target, it will close it for you.Thus dup() and dup2() are ways that you can control file descriptors in general. More specifically, you can use them to change the Standard In and Standard Out of a process to be the read and write ends of a pipe.In pipe-4.c the code does, more or less what the shell does when you use the | symbol create chains of programs separated by pipes.#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; string.h &gt;#include &lt; fcntl.h &gt;#include &lt; sys/wait.h &gt;/* * simple program creating a pipe between a set of processes * that read and write stdin and stdout * first child get stdin of parent * last child gets stdout of parent */int main(int argc, char **argv, char **envp){    pid_t child_id;    int child_status;    int pipe_desc[2];    char read_buffer[4096];    int err;    char file_name[4096];    int proc_count;    int i;    int last_stdout;    if(argc &lt; 3) {        printf(\"usage: pipe-4 executable-file number-of-procs\\n\");        exit(1);    }    /*     * get the arguments     */    memset(file_name,0,sizeof(file_name));    strncpy(file_name,argv[1],sizeof(file_name));    proc_count = atoi(argv[2]);    /*     * save the parent stdout for the last child     */     **last_stdout = dup(1);**     /*     * create pipes, fork and exec the processes     */    for(i=0; i &lt; proc_count; i++) {        /*         * create the pipe         */        err = pipe(pipe_desc);        if(err &lt; 0) {            printf(\"error creating pipe\\n\");            exit(1);        }        /*         * then fork         */        child_id = fork();        if(child_id == 0) {            /*             * child closes standard out             */             **close(1);            /*             * child dups the write end of the pipe             * if it is not the last child, otherwise             * it dups the last_stdout it got from the parent             * the closed stdout will be chosen as the             * target             */            if(i &lt; (proc_count - 1)) {                dup2(pipe_desc[1],1);            } else {                dup2(last_stdout,1);                close(pipe_desc[1]);            }**             /*             * child runs the program             */            err = execve(file_name,&amp;argv[1],envp);            printf(\"parent error: %s didn't exec\\n\",                file_name);            exit(1);        } else {             **/*             * parent closes standard in             */            close(0);            /*             * parent dups the read end of the pipe for the             * next child             */            if(i &lt; (proc_count - 1)) {                dup2(pipe_desc[0],0);            }            close(pipe_desc[0]);            close(pipe_desc[1]);**         }    }    close(1);    dup2(last_stdout,1);    /*     * parent now waits for the children to exit     */    for(i=0; i &lt; proc_count; i++) {        child_id = wait(&amp;child_status);    }    exit(0);}This code is worth understanding. It takes the name of a file as its first argument and the number of processes to create as its second argument. The processes must read and write Standard In and Standard Out (like my-cat.c.Try running it:./pipe-4 ./my-cat 2pid: 34122 -- I am my-cat and I have startedpid: 34123 -- I am my-cat and I have startedI love OSpid: 34122 read some datapid: 34122 wrote some datapid: 34123 read some dataI love OSpid: 34123 wrote some datapid: 34122 -- I am my-cat and I am exitingpid: 34123 -- I am my-cat and I am exitingThe code forked two children (pid: 34122 and pid: 34123). When you type an important sentence to the terminal, 34122 wakes up, reads the message from Standard In (file descriptor 0) and writes it to Stabdard Out (file descriptor 1). The parent has been careful to leave file descriptor alone on its first fork so that the first child gets the terminal as Standard IN. Then 34123 wakes up, reads its Standard In, and writes it to its Standard Out. In this case, however, since it is the end of the chain, the parent has arranged that its Standard Out is the terminal. Thus you see the sentence a second time when 34123 writes it. Try it with three processes./pipe-4 ./my-cat 3pid: 34201 -- I am my-cat and I have startedpid: 34202 -- I am my-cat and I have startedpid: 34203 -- I am my-cat and I have startedI love OSpid: 34201 read some datapid: 34201 wrote some datapid: 34202 read some datapid: 34202 wrote some datapid: 34203 read some dataI love OSpid: 34203 wrote some datapid: 34201 -- I am my-cat and I am exitingpid: 34202 -- I am my-cat and I am exitingpid: 34203 -- I am my-cat and I am exitingand with four./pipe-4 ./my-cat 4pid: 34225 -- I am my-cat and I have startedpid: 34226 -- I am my-cat and I have startedpid: 34228 -- I am my-cat and I have startedpid: 34227 -- I am my-cat and I have startedI love OSpid: 34225 read some datapid: 34225 wrote some datapid: 34226 read some datapid: 34226 wrote some datapid: 34227 read some datapid: 34227 wrote some datapid: 34228 read some dataI love OSpid: 34228 wrote some datapid: 34225 -- I am my-cat and I am exitingpid: 34226 -- I am my-cat and I am exitingpid: 34227 -- I am my-cat and I am exitingpid: 34228 -- I am my-cat and I am exitingEach time, the first process in the set of children that has been forked reads the string from Standard In and passes it to the next process in the chain. The parent gave the first child its own Standard In which was the terminal so that first child reads the terminal. Eventually, the last process in the chain writes it to Standard Out which has been set to be the same Standard Out the parent had when it ran.The logic takes a little while to understand. In a loop, the parent creates a pipe and forks a child. The child, before it execs the program specified as the second argument, dups the write end of the pipe to Standard Out so that after the exec, the program that is running will be writing the write end of the pipe when it writes to its Standard Out.The parent, then, must dup the read end of the pipe into its own Standard In so that when it forks the next time, the child will receive the read end of the pipe the parent created on the last iteration. Thus the child before it in the loop will be writing the write end and the next child will be reading the read end.The logic must also detect when the last child in the list has been forked and, instead of duping the write end of the pipe, it must dup the original Standard Out for the parent (which is duped at the start of the parent as last_stdout).Lastly, because my-cat is also trying to print a status message, if it uses printf() Linux will try to use Standard Out. It actually works, but has a problem as the processes exit because Standard Out isn’t being closed on process exit. To keep things “straight” I’ve used Standard Error (which none of the code change with dup() so that my-cat can send messages to the screen without using Standard Out.That’s it for the basic system calls in Linux. There are many others, but these are the ones you need to do file I/O, create and descroy processes and allow processes to communicate between themselves.)}",
        "url"      : "/2017/06/08/System-Calls/",
        "date"     : "2017-06-08 15:16:00 +0000"
    } ,
    
    {
        "title"    : "Kthreads",
        "category" : "Operating System",
        "tags"     : "Thread",
        "content": "  The Kthreads Library          The Interface      Example      Understanding kt_joinall()      Implementation      Global Data                  Thread Structure          Semaphore structure                    The Scheduler                  The Kthreads functions          Semaphore functions          The scheduler revisited                    The Kthreads LibraryKthreads is a relatively simple, non-preemptive threads library that we will be using to implement our operating system later in the labs. For our purposes, it has an advantage over pthreads because it works in the debugger and because it will not interfere with the simulator (making your development life easier). It’s also simple enough to understand, and therefore makes for a good teaching tool.The kthreads library is at /cs/faculty/rich/cs170/lib/libkt.a, the source is at /cs/faculty/rich/cs170/src/libkt/, and the header is /cs/faculty/rich/cs170/include. You will also need to link to /cs/faculty/rich/cs170/lib/libfdr.a which provides some basic C functions like linked lists and red-black trees.The InterfaceUsing kthreads is pretty straightforward, so we will not go into it in much detail. You should all understand the basic thread primitives by now although you need to remember that (unlike in the case of pthreads) kthreads do not pre-empt each other. The kthreads calls are:  void *kt_fork(void *(*func)(void *), void *arg);  void kt_exit();  void kt_join(void *kt_id);  void kt_joinall();  void *kt_self();  void kt_yield();  void kt_sleep(int sec);Kthreads includes counting semaphores as its only synchronization primitive.  kt_sem make_kt_sem(int initval);  void kill_kt_sem(kt_sem ksem);  void P_kt_sem(kt_sem ksem);  void V_kt_sem(kt_sem ksem);  int kt_getval(kt_sem s);The basic kt primitives (fork, exit, join) play the same role that pthread_create, pthread_exit, and pthread_join play for pthreads. Of the other functions, kt_yield() interrupts the current thread and lets the scheduler run a new one. This primitive is nice in non-pre-emptive thread systems because it allows a kind of “polling” of the scheduler. A thread calling kt_yield() blocks itself and allows other threads that can run to go ahead. When no more runnable threads are available, the yielding thread will be resumed at the point of the yield.The call kt_sleep() sleeps the current thread for a specified time period. Again, because the thread is non-pre-emptive, the thread will be “awakened” and made runnable after the specified time, but it will not actually run until it is given the CPU.The call kt_self() returns the thread id. No confusion here.The function kt_joinall() is a useful function that causes the current thread to block until all other threads have either exited or blocked on a semaphore.You will find that this function is particularly handy in designing your OS.The semaphore primitives are exactly as we discussed. make_kt_sem() creates a semaphore with a value greater than or equal to zero, and kill_kt_sem() destroys (and frees) it. P_kt_sem() decrements the semaphore’s value by 1, and if it is negative blocks it. V_kt_sem() increments the value by one, and it if it is zero or less it unblocks one thread.There is also a call to interrogate the current value of the semaphore – kt_getval(). While not strictly part of the semaphore API, there are occasions where the ability to know how many threads are blocked on a semaphore is quite handy.ExampleHere is a hopefully familiar example of how to use this library using the Client-trader Simulation from previous classes. Here is a version of market-kthreads.c that uses the kthreads library. There are a couple things worth noting here. First, notice that it doesn’t bother protecting against any of the race conditions (the mutexes are gone) that the pthreads version does. That is, there is a distinct lack of calls to any primitive implementing mutual exclusion. This is because we know kthreads is strictly non-preemptive, and so there are no race conditions between running threads. Don’t be confused, however. In the OS you build, you can create race conditions – it just won’t be between runnable threads. We’ll discuss this at length later. for now it is enough for you to know that race conditions are possible in your labs, but not in this example. However, with that difference and small differences in the return values when threads are created and destroyed, the code is the same as the semaphore version of the client-trader simulation. As we saw before, this is a pretty elegant solution to the problem. Run it a few times, however, and you’ll see that there is no speed-up gained through the use of multiple threads. Why? Kthreads doesn’t use multiple processors – it is strictly for decomposing a problem into thread like tasks and as this example shows converting something from kthreads to pthreads and back again is pretty simple.This single-threaded implementation comes in VERY handy when you will be developing your OS. Trust me.Understanding kt_joinall()One curious primitive in the interface is kt_joinall() which bears some discussion. For reasons that will become clear when you begin working on your OS, it is sometimes convenient to have a way for a “master” thread to block and wait until there is nothing else that can run. That is, imagine you wanted to create a “watcher” thread that waits around for all of the other threads to do their work and, when they have finished or they are all blcoked waiting for something, this “watcher” thread wakes up and takes some action.Don’t ask why just yet – just imagine it to be true.You could try to pull this trick off using kt_yield() where each thread would yield right before any call to P_kt_sem() and the watcher thread were in a loop calling yield, but it would be tricky to make sure you don’t deadlock and the watcher thread is essentially spinning and burning CPU.Instead, kthreads includes the kt_joinall() command. It has the following properties  at most one thread in a kthreads program can call kt_joinall()  the thread that calls kt_joinall() will block until all other threads in the program have          exited, or      blocked on a semaphoreat which point the call to kt_joinall() unblocks.      You can also think of this as a way of setting a place in the code where you wish to continue once all other work has been finished.It may sound confusing. The code in joinall-1.c attempts to illustrate how it works. In it, the thread simply increments a shared counter (a pointer to which is passed as an argument) in a loop and does so under the control of a semaphore. The main thread (which will be the master in this example) waits until all threads have finished and then exits. It does so in a loop where it prints out the progress of the threads and calls kt_joinall()        /*         * loop showing progress         */        while(done_threads &lt; threads) {                printf(\"shared counter: %f\\n\",counter);                fflush(stdout);                kt_joinall(); /* gets here when all else is stopped */        }What should the output of this program be? Here is the execution of the program with two worker threads, each one having to make two trips (as indicated by the -C argument). With two worker threads the final count should be 4 since each thread runs the count up by two../joinall-1 -t 2 -C 2shared counter: 0.000000final count: 4.000000Is this what you expected? Try it with the -V flag:./joinall-1 -t 2 -C 2 -Vshared counter: 0.000000thread 0: incremented shared counter to 1.000000thread 0: incremented shared counter to 2.000000thread 1: incremented shared counter to 3.000000thread 1: incremented shared counter to 4.000000final count: 4.000000Notice that both threads run and do their increments. However the master thread seems not to go around the progress loop more than once.This execution sequence is correct. The master thread spawns both worker threads and calls V on the semaphore to make sure one starts (the initial semaphore value is 0). It then enters the loop because the threads aren’t done, prints the “shared counter:” message, and calls kt_joinall() at which point it blocks waiting for all threads to either exit or block.The two worker threads then run (first thread 0 and then thread 1), finish their work and exit. The kt_joinall() command then completes, the master loops around and tests, finds them all completed and exits.Now take a look at joinall-2.c. Notice that there is no call to V_kt_sem() in either thread. That is, each thread blocks in a call to P_kt_sem() but they never unblock another thread.Instead, the master thread calls V_kt_sem() in its progress loop as well as kt_joinall():        /*         * loop showing progress         */        while(done_threads &lt; threads) {                V_kt_sem(sema); /* enable a thread */                printf(\"shared counter: %f\\n\",counter);                fflush(stdout);                kt_joinall(); /* gets here when all else is stopped */        }Run this code and you’ll see./joinall-2 -t 2 -C 2shared counter: 0.000000shared counter: 1.000000shared counter: 2.000000shared counter: 3.000000final count: 4.000000and with the verbose flag set/joinall-2 -t 2 -C 2 -Vshared counter: 0.000000thread 0: incremented shared counter to 1.000000shared counter: 1.000000thread 0: incremented shared counter to 2.000000shared counter: 2.000000thread 1: incremented shared counter to 3.000000shared counter: 3.000000thread 1: incremented shared counter to 4.000000final count: 4.000000What happens in this case is that the master thread calls V to enable some thread blocked on the semaphore, prints its progress message, and blcoks in kt_joinall(). Which ever thread was awakened (thread 0 in this case) runs, increments, and then blocks again in its call to P and which point there are no other runnable threads. Take a moment to understand that last sentence. After thread 0 in this example runs for the first time, it calls P again. No other thread has called V and the master thread is blocked so all threads are blocked. Because the master thread is blocked in kt_joinall() the call unblocks at this moment, the master runs, tests to see if all threads have completed, calls V to enable another thread, and calls kt_joinall() again.Thus, kt_joinall() specifies a place to continue when all other threads have blocked or have exited which will come in handy at some point in your future.ImplementationWe will be discussing the logic that the kthreads library uses to make all of its calls, and also discuss some of the more important concepts. I regret that I will not have the time to go into the nitty gritty details on how this library works. It is realtively simple and relatively short, and I urge you all to do this on your own. It should not be difficult to relate the source code to the ideas I will discuss. Also, Dr. Plank at UT has a very detailed lecture on kthreads implementation here. To understand this explanation in deapth, you will need to understand the Linux calls setjmp() and longjmp(). If you don’t understand the man pages now, at some point before this class ends read them and you will certainly be able to see how they work.Global DataKthreads uses a few pieces of global data to keep track of the current process. Let’s go over those first:  KThread *ktRunning; - currently running thread.  KThread *ktJoinall; - current thread doing a joinall.  Dllist ktRunnable; - fifo list of all runable threads.  JRB ktSleeping; - sorted list of sleeping threads.  JRB ktBlocked; - sorted list of blocked threads.  JRB ktActive; - searchable list of all threads.Thread StructureAlso, there are some variables associated with each thread.  void (*func)(void *); - thread function.  void *arg; - thread function argument.  int id; - unique thread id.  int state; - state of the thread: BLOCKED, RUNNING, RUNABLE, SLEEPING  void *stack; - thread’s stack.  jmp_buf jmpbuf; - thread’s jump buffer.Semaphore structure  int value; - the semaphores value.  int id; - the semaphores unique value.The SchedulerThe first thing to understand is that the scheduler is really managing a set of queues, each one of which contains jobs in a certain state. The implementation does not necessarily use a linked list for each of these queues, but logically, you can think of them as queues. They are  The Run Queue contains a list of threads that have been made runnable. That is, can run as soon as their turn to use the CPU comes up.  The Blocked Queue contains a list of threads that are blocked waiting to be awakened by other threads.  The Sleep Queue contains a queue of jobs that are sleeping for a specified time period.There is also a global pointer to the currently running thread.The scheduler, called KtSched() is the core of the threads system. It is called whenever a thread is abdicating the CPU and its job is to manage these queues, and set the next runnable thread. Since this system is non-premptive, threads will only abdicate the cpu on their own initative. The calls that do this are kt_join(), kt_exit(), kt_sleep(), kt_yield(), kt_joinall(), and P_kt_sem(). When a thread abdicates the cpu, KtSched() takes the thread that is at the head of the Run Queue and makes it the running thread. It sets the global pointer and switches from the current thread (which is the one that is abdicating) to the new “running” thread. The scheduler does not return untill there is a thread that can be run or when there are no more threads in the system. We’ll discuss this behavior in greater detail as we discuss the other primitives. The important thing to know about the scheduler, however, is that its job is to switch from the currently running thread to the next runnable thread, and to manage the internal queues.The Kthreads functionsThe first function that we discuss is kt_yield() since it is now easy to understand. When a thread calls kt_yield() is simply adds itself to the end of the Run Queue and calls ktSched(). Here is the source code for kt_yield().void kt_yield(){        InitKThreadSystem();        ktRunning-&gt;state = RUNNABLE;        dll_append(ktRunnable,new_jval_v(ktRunning));        KtSched();        return;}Simple, huh? The call to InitKThreadSystem() is there just to handle the case when kt_yield() is the first thread call made by a thread. This idempotent call prevents us from having to make an explicit initialization call in a KThread code. A caller of kt_yield() sets its state to RUNNABLE (it will no longer be RUNNING), appends itself to the end of the Run Queue, and calls the scheduler. If there are other runnable threads, they will each be run in turn and then, when this thread’s turn comes up it will be run again.The next function we will cover is kt_fork(). It is going to create a stack and a state for the thread, which we will discuss later. It will then set the func and arg fields to their approprate values, choose a unique id for the thread, set its state to RUNNABLE, and add it to the end of ktRunning.The easiest function is kt_self(). It simply returns its unique thread id of ktRunning typecast to a (void *). The only reason it returns a (void *) is to hide some of the innerworkings of the library from the user.kt_sleep(int sec) is pretty easy too. For this, you set the state to SLEEPING, calculate the wakup time (time(NULL) + sec), and insert it into ktSleeping keyed on its wakeup time.kt_join(void *ktid) is simmilar. First off, check the thread at ktid exists. If it doesn’t, then we figure that it has exited and we simply return. Technically, this allows a caller to join with a thread id that has never existed, but in order to keep track, we’d have to have a list of all valid thread ids ever used. We’ll leave this as a subtle point.Next we need to see if there is a thread in ktBlocked keyed on the joinee’s id. Each thread can only have one other thread waiting to join with it. Think about that for a minute. Thread A tries to join with Thread B and later Thread C tries to join with Thread B. What do you want to have happen? Your options are  Thread A continues to wait and Thread C is ignored, returning an error.  Thread C waits and Thread A wakes up with an error  Global programming error.Kthreads takes this latter approach and exits your program.If we make it this far, then we set ktid’s joining field to point to ourself, set our state to BLOCKED, and add ourself to the blocked tree keyed on ktid’s id.With this in mind, kt_joinall() is pretty simple. We do the same as kt_join(), but we treat it is if we were joining with a thread with id 0 which we will never assign internally. The scheduler takes one last look at the Blocked Queue before it decides to exit, and if it sees a thread trying to join with 0, it wakes that thread as the joinall thread. Again, at most one thread can call kt_joinall(). Multiple calls casue the program to exit.Last, kt_exit() is going to free up all of the data for the thread and simply run the scheduler again without putting the current thread back in the list. I am glossing over the details of this because it is easier said than done. The only thing that it needs to do is check to see if it has a joiner. It check ktBlocked to see if anyone is blocked on its id. If there is, we remove it, set the state to RUNNABLE, and append it to ktRunnable.Semaphore functionsOur semaphores are going to work simmilar to the kt_join() and kt_joinall() code. When they are created with the make_kt_sem() call, our semaphores are going to get their own unique ids. These are not going to overlap with the kthread ids because we will block exactly the same way as above. We will also set the semaphore up with an initial value when we create it.The function P_kt_sem() will decrement the value of the semaphore, and if this value is less than zero, it will block the thread. To do this, it will set its state to BLOCKED and insert it into ktBlocked keyed on the id of the semaphore. Here is the code:    void P_kt_sem(kt_sem iks){        Ksem ks = (Ksem)iks;        K_t me = ktRunning;        InitKThreadSystem();        ks-&gt;val--;        if(ks-&gt;val &lt; 0)        {                /*                 * use the semaphore tid as the blocking key                 */                ktRunning-&gt;ks = ks;                BlockKThread(ktRunning,ks-&gt;sid);                KtSched();                ktRunning-&gt;ks = NULL;                return;        }        return;}Again – the code is fairly simple once you understand that KtSched() is doing all of the hard work associated with switching between threads.V_kt_sem() increments the counter on the semaphore and checks to see if the value is less than or equal to zero. If it is, it searches ktBlocked for a thread keyed on its id, sets its state to RUNNABLE, and appends it to the end of ktRunnable. Notice that there is no garuntee that the threads are going to be ublocked in FIFO order. It would be hard to do, but we couldn’t keep up the nice, generic system we have. So it goes.Here is the code:void V_kt_sem(kt_sem iks){        Ksem ks = (Ksem)iks;        K_t wake_kt;        InitKThreadSystem();        ks-&gt;val++;        if(ks-&gt;val &lt;= 0)        {                wake_kt = jval_v(jrb_val(jrb_find_int(ktBlocked,ks-&gt;sid)));                WakeKThread(wake_kt);        }        return;}Notice that it picks some thread off the list of threads blocked on the semaphore and wakes it up. The wake code is here:void WakeKThread(K_t kt){        /*         * look through the various blocked lists and try to wake the         * specified thread         */        if (kt-&gt;state == RUNNING || kt-&gt;state == RUNNABLE                                 || kt-&gt;state == DEAD) return;        jrb_delete_node(kt-&gt;blocked_list_ptr);        kt-&gt;state = RUNNABLE;        kt-&gt;blocked_list = NULL;        kt-&gt;blocked_list_ptr = NULL;        dll_append(ktRunnable,new_jval_v(kt));        return;}Last is kill_kt_sem(). There is not much to say about this except that it checks to see if there are any threads blocked on the semaphore, and if there are flags and error and exits.The scheduler revisitedSo now that we know how the functions work I think we are in a better position to discuss the scheduler. It basically goes through a set of steps before it takes the next thread and runs is. Here is what it does:  Check ktSleeping to see if there are any threads that are ready to wake up. If there are, take them out of ktSleeping, set their state to RUNNABLE, and add them to the end of ktRunnable  If there are threads in ktRunnable, take the first one off the list and run it. At this point, the scheduler returns (actually it stops and never returns).  If there are threads in ktSleeping, take the next thread to wakeup and sleep until it is time for it to run again. Wake up and run the thread as step 2  If there is a joinall thread, make it runable and run it as in step 2So thats the scheduler, and now we understand how how kthreads works in general. Its surprisingly simple, isn’t it? The only “hard” part we haven’t discussed is what a thread actually is made of, and the scheduler switches between them. You need to understand the Unix calls setjmp() and longjmp() very clearly before we can address these questions. Unfortunately, we won’t do so explicitly, but by the time you implemented processes in your OS, you’ll know enough to be able to work through the man pages and the innerworking of ktSched().",
        "url"      : "/2017/06/08/Kthreads/",
        "date"     : "2017-06-08 15:06:00 +0000"
    } ,
    
    {
        "title"    : "Semaphore",
        "category" : "Operating System",
        "tags"     : "Thread",
        "content": "  Synchronization  Implementation  Implementing Semaphores Using pthreads  Types of Synchronization Problems  The Client-Trader Example Revisited  Implementing Mutual Exclusion  Atomic Update of a Counter  Sending a Signal  Speed of the Solution  SummarizingSynchronizationSo far we have discussed mutexes and condition variables as the tools of synchronization and of managing critical sections of code. These are not the only tools that can be used for the job, and you are going to find yourselves very soon doing a lab where mutexes and condition variables are not available to you, but semaphores are. So we need to consider semaphores.The concept of semaphores as used in computer synchronization is due to the Dutch computer scientist Edsgar Dijkstra. They have the advantages of being very simple, but sufficient to construct just about any other synchronization function you would care to have; we will cover a few of them here. There are several versions of the semaphore idea in common use, and you may run into variants from time to time. The end of these notes briefly describe two of the most common, binary semaphores and the SYSV IPC semaphores.A semaphore is an integer with a difference. Well, actually a few differences.  You set the value of the integer when you create it, but can never access the value directly after that; you must use one of the semaphore functions to adjust it, and you cannot ask for the current value.  There are semaphore functions to increment or decrement the value of the integer by one.  Decrementing is a (possibly) blocking function. If the resulting semaphore value is negative, the calling thread or process is blocked, and cannot continue until some other thread or process increments it.  Incrementing the semaphore when it is negative causes one (and only one) of the threads blocked by this semaphore to become unblocked and runnable.  All semaphore operations are atomic.There are various ways that these operations are named and described, more or less interchangeably. This can be confusing, but such things happen in computer science when we try to use metaphors, especially multiple metaphors, to describe what a program is doing. Here are some:**Increment**Dijkstra called this function **V()**; it is also called signal, unlock, leave or release.**Decrement**Dijkstra called this function **P()**; it is also called wait, lock, enter, or get.ImplementationThe easiest way for me to think of semaphores is, of course, with code. Here is a little pseudo-code that may help.typedef struct sem {  int value;  other_stuff} *Sem;There are two actions defined on semaphores (we’ll go with the classic terminology): P(Sem s) and V(Sem s). P and V are the first letters of two Dutch words proberen (to test) and verhogen (to increment) which, on balance, makes about as much (or as little) sense as any other set of monikers. The inventor of semaphores was Edsger Dijkstra who was very Dutch.  P(Sem s) decrements s-&gt;value, and if this is less than zero, the thread is blocked, and will remain so until another thread unblocks it. This is all done atomically.  V(Sem s) increments s-&gt;value, and if this is less than or equal to zero, then there is at least one other thread that is blocked because of s. Exactly one of these threads is chosen and unblocked. The definition of V() does not specify how the thread to unblock is chosen, although most uniprocessor thread packages use a FIFO algorithm.initialize(i){    s-&gt;value = i    return}P(Sem s){    s-&gt;value--;    if(s-&gt;value &lt; 0)    block on semaphore    return}V(s){    s-&gt;value++;    if(s-&gt;value &lt;= 0)    unblock one process or thread that is blocked on semaphore    return}You should understand these examples to be protected somehow from preemption, so that no other process could execute between the decrementing and testing of the semaphore value in the P() call, for instance.If you consider semaphores carefully, you might decide that they are like mutexes, but they don’t “lose” extra signals. This is a good way to look at them, but not the only way.Implementing Semaphores Using pthreadsUp to this point, we have discussed semaphores in general terms. That’s because you will need to use them in this class in a couple of different contexts where the internal implementation will be different (and possibly hidden). It is critical that you understand the concept of the counting semaphore for this reason. Indeed, on many hardware platforms, there are primitive instructions to make implementation easy and efficient.The power of semaphores, though, is that they can be implemented relatively simply and (as we’ll see) they can be used to solve a wide variety of syncronization problems in a way that many would characterize as elegant.In pthreads, the implementation of sempahores is pretty simple as long as your pthreads code adheres to basic thread principles. That is, the code uses threads and thread synchronization in a way that conforms to the fork/join model. We’ll discuss the full pthreads picture with respect to semaphores as well, but to begin, consider the following code:typedef struct{        pthread_mutex_t lock;        pthread_cond_t wait;        int value;} sema;void pthread_sema_init(sema *s, int count){        s-&gt;value = count;        pthread_cond_init(&amp;(s-&gt;wait),NULL);        pthread_mutex_init(&amp;(s-&gt;lock),NULL);        return;}void pthread_sema_P(sema *s){        pthread_mutex_lock(&amp;(s-&gt;lock));        s-&gt;value--;        if(s-&gt;value &lt; 0) {                pthread_cond_wait(&amp;(s-&gt;wait),&amp;(s-&gt;lock));        }        pthread_mutex_unlock(&amp;(s-&gt;lock));        return;}void pthread_sema_V(sema *s){        pthread_mutex_lock(&amp;(s-&gt;lock));        s-&gt;value++;        if(s-&gt;value &lt;= 0) {                pthread_cond_signal(&amp;(s-&gt;wait));        }        pthread_mutex_unlock(&amp;(s-&gt;lock));}And that’s it. You could add error checking (e.g. the initial value should never be negative) but that’s the basic implementation.This basic implementation makes several assumptions about the structure of the code. In particular, it assumes  no thread is ever cancelled via pthread_cancel()  the primitive pthread_cond_signal() wakes up exactly one thread among the threads that are blocked on the condition variable.  the threaded program is not using Linux signals for interprocess communicationOf these requirements, the latter is the most troubling since Linux signals can be useful in a threaded program. For example, setting a timer signal as a way of implementing timeouts when the program is using sockets is often convenient.The problem here is that the POSIX specification has changed over the years to make pthread less compatible with the original fork/join model it was intended to implement. The issue with pthread_cancel() is that it adds a new abstraction (cancellation state) to the API that the programmer must consider. In particular, condition variable synchronization primitives are cancellation points where, depending on the thread cancellation state, a thread might be cancelled. In general, cancelling a thread that is blocked in a synchronization primitive is a bad idea. That thread is almost assuredly part of some state update protocol in the program. When the thread is canceled, all of the state in the state-update protocol must be removed and determining this state precisely can be difficult.For example, in the semaphore code shown above, a thread block in pthread_cond_wait() that is cancelled has decermented the semaphore counter. Cancel is like the thread never existed so the counter value should be incremented. However the increment must be done in a critical section. The cancelled thread will not run. Instead a cancellation handler runs and that handler must run in the critical section which means it must acquire the lock. Is the lock automatically acquired? Is it dropped when the handler completes? You can look up these details, but the short answer is that you should not use cancel unless you are prepared to make your pthread code substantially more complex.The second issue is that the current pthread specification says that a call to pthread_cond_signal() will wake at least one thread (but possibly more). The exact reason the specification is written this way is not clear, but there is a reasonable explanation.The primitive pthread_cond_signal() as described in the lecture on condition variables was really designed to implement operating system monitors. In a monitor, after a thread is awakened, it must re-enter the monitor by acquiring the lock that the monitor uses to assure mutual exclusion. For pthreads, the easiest way to do this is to have threads that wake up as a result of a pthread_cond_signal() then call (inside the pthread_cond_wait() code) pthread_mutex_lock() to reacquire the lock they were holding when then blocked. However, in a monitor, this lock is usually the same lock that is used to implement mutual exclusion. As such, there is no guarantee that the thread coming out of pthread_cond_wait() will get the lock when it calls pthread_mutex_lock() to re-enter the monitor. Instead, it may be that a new thread trying to enter the monitor (not coming out of a wait) is given the lock and allowed to enter the mutual exclusion region. If this thread changes the test predicate state and then leaves the mutual exclusion region and then the thread that was awakened re-enters the monitor, the state will not be the state that was present when the thread was signalled.Thus when calling pthread_cond_wait() in a monitor, the thread must re-test the state of the predicate it used to decide to call pthread_cond_wait(). If that state remains unchanged (i.e. the thread should not proceed) it should call pthread_cond_wait() again (say in a while loop).The problem is that there are ways to use condition variables in the context of a fork/join model that do not suffer from this possibilty. Again, if pthread_cond_signal() wakes exactly one thread (as most threaded programs assume) then the semaphore code shown above is incorrect if the P() primitive uses a while loop. The reason is that the semaphore counter is counting the number of blocked threads and the use of mutual exclusion assures that the counter state and the number of sleeping threads is synchronized if pthread_cond_signal() wakes exactly one thread.It seems that because the intent was to implement monitors, however, the specification designers took the opportunity to allow pthread_cond_signal() and pthread_cond_wait() to include the possibility that  threads can wake up randomly form pthread_cond_wait()  a call to pthread_cond_signal() will wake one or more threadsThis decision making is unfortunate because it introduces considerable complexity into what were relatively simple synchronization primitives. Further, it is unlikely that any truly sane implementation of pthreads will intentinally take advantage of this part of the specification. That is, these strange behaviors will occur when non-thread like events occur (cancel, fork, Linux signals, etc.) Indeed, if pthread_cond_signal() were always to wake 2 threads (which is allowed in the specification) it is almost certainly the case that many programs using condition varaibles would need to be rewritten.Be that as it may, it is possible to write the semaphore code in a slightly more complex way to work with these unfortunate semantics in the pthreads specification. Consider the following code:#include &lt; stdlib.h &gt;#include &lt; unistd.h &gt;#include &lt; stdio.h &gt;#include &lt; pthread.h &gt;typedef struct{        pthread_mutex_t lock;        pthread_cond_t wait;        int value;    int waiters;} sema;sema *InitSem(int count){               sema *s;        s = (sema *)malloc(sizeof(sema));        if(s == NULL) {                return(NULL);        }        s-&gt;value = count;        s-&gt;waiters = 0;        pthread_cond_init(&amp;(s-&gt;wait),NULL);        pthread_mutex_init(&amp;(s-&gt;lock),NULL);        return(s);}void P(sema *s){        pthread_mutex_lock(&amp;(s-&gt;lock));        s-&gt;value--;        while(s-&gt;value &lt; 0) {                /*                 * maintain semaphore invariant                 */                if(s-&gt;waiters &lt; (-1 * s-&gt;value)) {                        s-&gt;waiters++;                        pthread_cond_wait(&amp;(s-&gt;wait),&amp;(s-&gt;lock));                        s-&gt;waiters--;                } else {                        break;                }        }        pthread_mutex_unlock(&amp;(s-&gt;lock));        return;}void V(sema *s){        pthread_mutex_lock(&amp;(s-&gt;lock));        s-&gt;value++;        if(s-&gt;value &lt;= 0)        {                pthread_cond_signal(&amp;(s-&gt;wait));        }        pthread_mutex_unlock(&amp;(s-&gt;lock));}This code relies on the invariant that the absolute value of the semaphore’s value must be equal to the number of waiting threads when the value itself is less than zero. The code for V() does not change (it still calls pthread_cond_signal(). However, the code for P() now uses a counter (updated inside the critical section) to record how many threads are waiting on the condition variable. If a thread wakes up, it re-tests both the semaphore value and the invariant to determine whether it should proceed or it has been awakened spuriously and need to block again.To test that this works, try changing the call to pthread_cond_signal() to pthread_cond_broadcast() which wakes up all threads. The semaphore primitive still work correctly (although they are much slower when there are many threads calling P().Types of Synchronization ProblemsBy now, you’ve see three types of synchronization mechanisms:  locks (implemented at pthread_mutex_* under pthreads)  condition variables (implemented as pthread_cond_* under pthreads)  semaphoresIt turns out that that these mechanisms are essentially equivalent in terms of their “power.” The power of a a language primitive is usually measured by the number of different programming challenges that a particular primitive can address. In the case of these synchronization mechanisms, each one can be used to implement the others (with some assumptions about how variables are shared and the atomicity of memory read and write instructions).Much of the research that went into the design of these primitives centered on how “elegantly” they solved different synchronization problems that appear to be common to many asynchronous systems. In addition to the bounded buffer problem, there are a few others.The Client-Trader Example RevisitedTake a look at the code for the Client-Trader simulation written for semaphores. Study it for a minute. You should notice two features when comparing it to the Client-Trader code written for condition variables discussed in the lecture on Condition Variables.  the client thread and trader thread are MUCH simpler  the initialization of the semaphores in the constructor routines is REALLY importantLet’s look at these features a little more closely. Here is the client and trader thread code with all of the comments removed to show how compact it is:void *ClientThread(void *arg){    struct client_arg *ca = (struct client_arg *)arg;    int i;    int next;    struct order *order;    int stock_id;    int quantity;    int action;    double now;    for(i=0; i &lt; ca-&gt;order_count; i++) {        stock_id = (int)(RAND() * ca-&gt;max_stock_id);        quantity = (int)(RAND() * ca-&gt;max_quantity);        if(RAND() &gt; 0.5) {            action = 0; /* 0 =&gt; buy */        } else {            action = 1; /* 1 =&gt; sell */        }        order = InitOrder(stock_id,quantity,action);        if(order == NULL) {            fprintf(stderr,\"no space for order\\n\");            exit(1);        }         **P(ca-&gt;order_que-&gt;full);        P(ca-&gt;order_que-&gt;mutex);**         next = (ca-&gt;order_que-&gt;head + 1) % ca-&gt;order_que-&gt;size;        if(ca-&gt;verbose == 1) {            now = CTimer();            printf(\"%10.0f client %d: \",now,ca-&gt;id);            printf(\"queued stock %d, for %d, %s\\n\",                order-&gt;stock_id,                order-&gt;quantity,                (order-&gt;action ? \"SELL\" : \"BUY\"));         }        ca-&gt;order_que-&gt;orders[next] = order;        ca-&gt;order_que-&gt;head = next;         **V(ca-&gt;order_que-&gt;empty);        V(ca-&gt;order_que-&gt;mutex);        P(order-&gt;fulfilled);**         FreeOrder(order);    }    return(NULL);}void *TraderThread(void *arg){    struct trader_arg *ta = (struct trader_arg *)arg;    struct order *order;    int tail;    double now;    int next;    struct stock *stock;    while(1) {         **P(ta-&gt;order_que-&gt;empty);**         if(*(ta-&gt;done) == 1) {             **V(ta-&gt;order_que-&gt;empty);**             return;        }         **P(ta-&gt;order_que-&gt;mutex);**         next = (ta-&gt;order_que-&gt;tail + 1) % ta-&gt;order_que-&gt;size;        order = ta-&gt;order_que-&gt;orders[next];        ta-&gt;order_que-&gt;tail = next;         **V(ta-&gt;order_que-&gt;full);        V(ta-&gt;order_que-&gt;mutex);**         stock = &amp;(ta-&gt;market-&gt;stocks[order-&gt;stock_id]);         **P(stock-&gt;mutex);**         if(order-&gt;action == 1) { /* BUY */            stock-&gt;quantity -= order-&gt;quantity;            if(stock-&gt;quantity &lt; 0) {                stock-&gt;quantity = 0;            }        } else {            stock-&gt;quantity += order-&gt;quantity;        }         **V(stock-&gt;mutex);**         if(ta-&gt;verbose == 1) {            now = CTimer();            printf(\"%10.0f trader: %d \",now,ta-&gt;id);            printf(\"fulfilled stock %d for %d\\n\",                order-&gt;stock_id,                order-&gt;quantity);        }         **V(order-&gt;fulfilled);**     }    return(NULL);}Simpler, no? Notice, also, that there are really three different uses of a sempahore in this code:  to implement a critical section around the shared queue head and tail pointers and the individual stock records  to count the number of full and empty slots and syncronize the threads based on this count  to send a “wake up” signal to a client from a trader when its order has been fulfilledBefore we discuss these three uses (and it is important that you undertand all three), take a moment to marvel at the magic of the counting semaphore. Look at how much that one primitive replaces in the corresponding pthreads implementation of this code. Really – I think the pthreads implementation is nice, but it is just stunning that so much of the mutex this, wait that, loop here and there just melts away with semaphores. So much so that one wonders why the pthreads specification doesn’t include them as a first class primitive? Perhaps it is because they are easy to implement, but it is still a bit of a mystery.Okay – having enjoyed that reverie for a moment, it is important to understand that the semaphore primitive in this program is doing three different syncronization jobs entirely alone. You should notice that there are no other suncronization calls in the code (no calls to pthread_mutex_lock() or pthread_cond_wait()) – it is only done with the semaphore.Implementing Mutual ExclusionOne use is to implement mutual exclusion. In the code, the head and tail pointers in the order queue must be updated atomically within a critical section. To implement a “lock” with a counting semaphore (so that code segments can be implemented atomically) on sets the initial value of the semaphore to 1. Notice that doing so will allow the first thread that calls P() on the semaphore to proceed but all other calls to P() to block until V() is called. When V() is eventually called, one thread is selected and allowed to proceed.Notice also that the count in the semaphore records (as a negative number) the number of threads that are blocked. By setting the initial value to 1, calling P() to get into a critical section and V() to get out, the value only ever goes back to 1 when a thread leaves the critical section and there are no threads waiting. That’s exactly the functionality you’d like for mutual exclusion.Notice in the code that mutual exclusion is needed both to implement the order queue correctly and also to ensure that updates to the stock totals in the market are atomic. In the constructor functions, then, for the order queue and the market, you’d expect to see a semaphore initialized with a value of 1:struct order_que *InitOrderQue(int size){    struct order_que *oq;    oq = (struct order_que *)malloc(sizeof(struct order_que));    if(oq == NULL) {        return(NULL);    }    memset(oq,0,sizeof(struct order_que));    oq-&gt;size = size+1; /* empty condition burns a slot */    oq-&gt;orders = (struct order **)malloc(size*sizeof(struct order *));    if(oq-&gt;orders == NULL) {        free(oq);        return(NULL);    }    memset(oq-&gt;orders,0,size*sizeof(struct order *));     **oq-&gt;mutex = InitSem(1);**     oq-&gt;full = InitSem(size);    oq-&gt;empty = InitSem(0);    return(oq);}struct market *InitMarket(int stock_count, int init_quantity){    struct market *m;    int i;    m = (struct market *)malloc(sizeof(struct market));    if(m == NULL) {        return(NULL);    }    m-&gt;count = stock_count;    m-&gt;stocks = (struct stock *)malloc(stock_count*sizeof(struct stock));    if(m-&gt;stocks == NULL) {        free(m);        return(NULL);    }    for(i=0; i &lt; stock_count; i++) {        m-&gt;stocks[i].quantity = init_quantity;         **m-&gt;stocks[i].mutex = InitSem(1);**     }    return(m);}Also you should check that the threads always call P() when the try to enter a critical section and V() when they leave. It may see a little confusing becasue P() and V() are also being called for other syncronization reasons. Be sure you can identify which calls are there for mutual exclusion reasons.Atomic Update of a CounterThe second use of semaphores in the code is to keep track of how many full and empty slots there are in the queue. In the code based on condition variables the client and trader threads test (under a lock) whether the queue is full or empty as needed. Notice that in the semaphore example, neither the client thread nor the trader thread test the head and tail pointers.Instead, the code uses the ability of the semaphore to implement a simple integer counter atomically.Client threads use this capability to make progress when the number of full slots is not equal to the size of the buffer. That is, if the count of full slots in the buffer is ever the same as the buffer’s capacity, the client thread should block. Thus, the initial value of the sema *full semaphore in the order queue should be set to the number of available slots. Each time a client thread calls P() on this semaphore, the counter will decremented atomically. If the counter gets decremented to zero, the client thread is put to sleep because there isn’t a slot available: the queue is full. To make this work, then, a trader thread must call V() on the full semaphore every time it takes an order from the queue to indicate to the clients that a new slot is available.Similarly, a trader thread must block until there is work in the queue. Initially, there is no work so the initial value of the sema *empty semaphore must be zero. When a client puts work in the queue, it must call V() on this semaphore to indicate that new work is present. If one or more trader threads are blocked becaus ethey have called P() one will be selected and unblocked so that it can proceed.Of equal importance, though, is the notion that exactly one thread is released from the semaphore when a V() call is made and that the semaphore keeps track of P() and V() calls in its counter.For example, when ever a client thread enqueues an order it calls V() on the empty semaphore once, per order. Each call will release exactly one thread from being blocked so there is a one-to-one correspondence between orders and trader threads: each order will get a trader thread.Notice also that it doesn’t matter whether the trader threads are blocked or not. If all trader threads are busy and a new order arrives, the semaphore will go positive so the next P() call by a trader will immediately release it without blocking.Contrast this ability to “remember” that a V() call has happened so that the next P() call can proceed with pthread_mutex_lock() and pthread_mutex_unlock(). An unlock does not “store” the notion that an immediate wake up is needed if a lock happens afterwards. Put another way, lock/unlock depends on having a lock happen before an unlock (as in a critical section). Semaphores, however, can be used in cases where it is not possible to guarantee that a P() will always happen before its subsequent V().In this case, we don’t know when the threads will run. Imagine, for example, that there are the same number of client threads as there are slots in the buffer and 1 trader thread. It might be that all client threads run and fill the buffer slots before the trader runs. Because the semaphore value of the empty semaphore will be equal to the number of client threads which is also equal to to the number of buffer queue slots, the trader thread will immediately proceed and start fulfilling orders when it runs and calls P() on the empty semaphore.Sending a SignalThe final use of semaphores in this code is to signal a waiting client thread that the order has been fulfilled. Here, the sema *fulfilled semaphore is initialized to zero indicating that the client thread should not proceed until a trader thread has completed the order. Thus, then the client calls P() on the fulfilled semaphore it knows that the order has been fulfilled when the P() call completes as long as the trader thread has called V() on the semaphore after its completion.Again, you should convince yourself that the thread execution order between client threads and trader threads does not affect the correctness of the result. Specifically, if a client gets to the P() call on the fulfilled semaphore before the order is fulfilled it will wait until the trader thread’s V() call indicates that it should proceed. Alternatively, if the trader is faster and gets the order filled, calling V() before the client gets to the P() call the client will proceed immediately.Speed of the SolutionHere are the same set of experiments as we discussed in the lecture on condition variables.MossPiglet% ./market-semaphore -c 1 -t 1 -q 1 -s 1 -o 100000140849.225016 transactions / secMossPiglet% ./market-semaphore -c 1 -t 1 -q 10000 -s 1 -o 100000143667.854678 transactions / secMossPiglet% ./market-semaphore -c 1 -t 2 -q 1 -s 1 -o 100000142093.914085 transactions / secMossPiglet% ./market-semaphore -c 1 -t 2 -q 10000 -s 1 -o 100000133561.460408 transactions / secMossPiglet% ./market-semaphore -c 2 -t 2 -q 1 -s 1 -o 100000118935.758078 transactions / secMossPiglet% ./market-semaphore -c 2 -t 2 -q 10000 -s 1 -o 100000101221.276261 transactions / secMossPiglet% ./market-semaphore -c 100 -t 100 -q 100 -s 1 -o 1055894.243070 transactions / secCuriously, the performance is a little slower than that for the market3 solution. That’s weird since it is basically doing the same kind of computation and synchronization and there are no cases where the code loops back to retest. However, the code is MUCH simpler to understand which makes it easier to maintain. The loss of performance (which would need to be verified over many runs) might be okay in exchange for the simplicity of the solution.SummarizingSo far, we have studied three types of syncronization problems, all of which are present in the client-trader example: mutual exclusion, atomic counters, and signalling. We’ve also looked through examples of how these problems may be addressed in pthreads using locks, condition variables, and semaphores. If, at this point, you are unclear on these concepts you should go back and review because they are the basis for most (but not all) of what you may encounter.",
        "url"      : "/2017/06/08/Semaphore/",
        "date"     : "2017-06-08 14:54:00 +0000"
    } ,
    
    {
        "title"    : "Condition Variables",
        "category" : "Operating System",
        "tags"     : "Thread",
        "content": "  Introduction – The Bounded Buffer Problem  The Client/Trader Example  Example Solutions          Clients      Traders      Exectuting the Examples      Understanding the Examples        Solution 1: Synchronization using Mutexes Only          Speed of Solution 1      The Effects of Polling        Solution 2: Synchronizing the Order Queue using Condition Variables          Test under Lock      Summarizing pthread_cond_wait() and pthread_cond_signal()      Speed of Solution 2        Solution 3: Using a condition variable to signal order fulfillment          Speed of Solution 3        For your Personal Enjoyment  A brief word about monitorsIntroduction – The Bounded Buffer ProblemIt is possible to use locks (pthread_mutex_lock in pthreads) to implement different forms of synchronization, but often it is cumbersome to do so. In particular, there are cases when it is necessary to have one thread wait for something to be computed by another and then only to make progress when that computation has been completed. In the examples we have studied so far, the thread that logically waits is created by the thread that is logically producing the computation upon which that thread is waiting.That is, if thread 1 produces a variable value that will be used by thread 2 (as in the average example in the IntroThreads lecture), thread 1 performs the computation, stores the value, and spawns the threads that then use this computation.This form of parallelism is often called “producer-consumer” parallelism and it occurs in operating systems frequently. In this example, thread 1 “produces” a value that thread 2 consumes (presumably to compute something else).Perhaps more germane to the class at hand, this problem is also called the bounded buffer problem since the buffer used to speed-match the producers and consumers has a fixed length. In an operating systems context, these types of synchronization problems occur frequently. There are cases where threads and/or devices need to communicate, their speeds differ or vary, and the OS must allocate a fixed memory footprint to enable efficient producer-consumer parallelism.While it is logically possible to spawn a new thread that runs every time a new value is produced and to have that thread terminate after it has consumed the value, the overhead of all of the create and join calls would be large. Instead, pthreads includes a way for one thread to wait for a signal from another before proceeding. It is called a condition variable and it is used to implement producer-consumer style parallelism without the constant need to spawn and join threads.Condition variables are a feature of a syncronization primitive called a monitor which is similar to the way in which operating systems kernels work. We won’t discuss them formally at this juncture but their use will become clear as the class progresses. For pthreads, however, condition variables are used to  implement test under lock  implement wait and signalwhich as we will see are more or less equivalent usages.The Client/Trader ExampleTo make the concepts clearer, consider the following hypothetical example. Imagine that you are to write a simple trading program that allows “clients” to send stock orders to “traders” who are responsible for executing them in a market. Orders should be presented to the traders in first-come, first-served order, but the traders then execute the orders as fast as possible. Further, once a trader has executed an order, it needs to send a signal back to the client that initiated the order indicating that the order has been fulfilled.Thus clients  create an order  place it on a queue shared by all traders and clients  wait for the order to be fulfilled      repeat    and traders    take an order from the queue  execute it (buy or sell)  tell the client who initiated the that the order is fulfilled  repeatSimple enough?First, let’s start with a picture. Often the easiest way to begin to understand these types of syncronization problems is with a picture.This will be a simplified example that we’ll implement using pthreads. In it, clients use the queue to send their orders to traders, the traders interact via a single shared “market” and then send responses directly back to clients.Example SolutionsWe’ll go through several solutions that are based on the same code base. They differ only in the way that they syncronize the clients and traders. To understand the effects of these difference it is best to first understand the code that is common to all of them.There four main data structures:  order: specifying a stock, a quantity, an action (BUY/SELL), and a flag indicating when the order has been fulfilled  order_que: a one-dimensional array of pointers to orders that is managed as a FIFO using a head and tail index  stock: a counter indicating the quantity of a stock that is currently available for sale  market: a one-dimensional array of stocks indexed by stock IDHere are the C structure definitions for these data structures:struct order{    int stock_id;    int quantity;    int action; /* buy or sell */    int fulfilled;  };struct order_que{    struct order **orders;    int size;    int head;    int tail;    pthread_mutex_t lock;};struct market{    pthread_mutex_t lock;    int *stocks;    int count;};Notice that the struct market structure just uses an array of integers to represent the array of possible stocks. Each stock is named by an integer that is passed from client to trader in the int stock_id field of the order.The struct order_que has a pointer to an array of pointers to orders, a head and tail pointer to use for the FIFO, and a size indicating how many elements there are in the array of pointers to orders. Thus each element in the queue points to an order that has been dynamically allocated. The size of that queue of pointers is also used to dynamically allocate the array itself. To see how this works, it is helpful to study the constructor function for a struct order_questruct order_que *InitOrderQue(int size){        struct order_que *oq;        oq = (struct order_que *)malloc(sizeof(struct order_que));        if(oq == NULL) {                return(NULL);        }        memset(oq,0,sizeof(struct order_que));        oq-&gt;size = size+1; /* empty condition burns a slot */        oq-&gt;orders = (struct order **)malloc(oq-&gt;size*sizeof(struct order *));        if(oq-&gt;orders == NULL) {                free(oq);                return(NULL);        }        memset(oq-&gt;orders,0,size*sizeof(struct order *));        pthread_mutex_init(&amp;oq-&gt;lock,NULL);        return(oq);}Notice that the size parameter is used to malloc() an array of pointers to struct order data types. If you don’t understand this use of malloc() and pointers, stop here and review. It will be important since dynamic memory allocation, arrays, structures, and pointers are essential to success in this class.ClientsEach solution is a simulation of the client-trader interaction. In it, clients, create orders for stocks that consist of  a randomly selected stock among all possible stocks  a random quantity for the transaction for that stock  a BUY or SELL action, again selected randomlyNotice that price doesn’t factor into the simulation. It is possible to introduce a notion of price but it requires a third type of actor (the market maker) to adjust prices based on supply and demand. To keep it simple, we’ll assume that each client “knows” that she wants to trade at what ever the current price is (which isn’t represented in the simulation).Clients create a struct order data type, fill it it with their randomly generated order information, and queue it on an order queue (of which there will only be one in these examples). Then the client waits for the int fulfilled flag to indicate that the order has been fulfilled by a trader (which will set the flag to 1).TradersTraders each dequeue and order from the order queue and execute the trade with a market (of which there will only be one in these examples). To do so, they use the int stock_id in the struct order as an index into the struct market and then either decrement the value in the int *stocks array if the action is a BUY, or they increment it if the action is a SELL. For simplicity, the value in an element of the int *stocks array is never allowed to go negative. Then, after an order has been fulfilled, the trader sets the int fulfilled flag in the order to 1.The client has to retain a pointer to the order so that it ca “see” when the order has been fulfilled, deallocate the order, and loop around to create a new one.Exectuting the ExamplesEach example uses the same argument list  -c client_count: specifies the number of client threads  -t trader_count: specifies the number of trader thread  -o order_count: specifies the total number of orders each client will issue before finishing  -s stock_count: specifies the number of stocks in the market  -q queue_size: specifies the size of the queue to use between clients and traders  -V: sets the verbose flag on so that the simulation prints out internal information (slows the execution down)Thus, for example, the first solution discussed as./market1 -c 10 -t 10 -s 100 -q 5 -o 1000says to runs the simulation with 10 clients, 10 traders, 100 stocks, a queue size of 5, and each client will issues and wait for 1000 orders to complete.Without the -V each simulation prints out the transaction rate. That is, the number of orders/second the entire simulation was able to achieve using the parameters specified.Understanding the ExamplesIn these lecture notes, we’ll focus on specific aspects of the code that have to do with the way in which the clients and traders (each represented by a thread) must synchronize. We won’t cover possible useful details like the way in which the arguments are parsed. That information, however, is available to you in the source code itself and you are encouraged to study it. It is also helpful if you make copies of these examples, build them, and run them as you may find yourself wanting to use parts of these programs for your own assignments.All of the examples and a makefile are available from http://www.cs.ucsb.edu/~rich/class/cs170/notes/CondVar/exampleSolution 1: Synchronization using Mutexes OnlyIn the first attempted solution we’ll look at how the syncronization works if you used only the pthread mutex data type. The market1.First look at the arguments passed to the client thread:struct client_arg{        int id;        int order_count;        struct order_que *order_que;        int max_stock_id;        int max_quantity;        int verbose;};The clients need to know the address of the order queue (struct order_que *)), how many orders to place (int order count), the maximum ID to use when choosing a stock (int max_stock_id), the maximum quantity to use when choosing a quantity (max_quantity) an id for printing and the verbose flag.Note that the pointer the order queue will be the same for all clients so that they share the same queue.Next look at the body of the client thread codevoid *ClientThread(void *arg){    struct client_arg *ca = (struct client_arg *)arg;    int i;    int next;    struct order *order;    int stock_id;    int quantity;    int action;    int queued;    double now;    for(i=0; i &lt; ca-&gt;order_count; i++) {        /*         * create an order for a random stock         */        stock_id = (int)(RAND() * ca-&gt;max_stock_id);        quantity = (int)(RAND() * ca-&gt;max_quantity);        if(RAND() &gt; 0.5) {            action = 0; /* 0 =&gt; buy */        } else {            action = 1; /* 1 =&gt; sell */        }        order = InitOrder(stock_id,quantity,action);        if(order == NULL) {            fprintf(stderr,\"no space for order\\n\");            exit(1);        }         /*           queue it for the traders         */        queued = 0;        while(queued == 0) {            pthread_mutex_lock(&amp;(ca-&gt;order_que-&gt;lock));            next = (ca-&gt;order_que-&gt;head + 1) % ca-&gt;order_que-&gt;size;            /*             * is the queue full?             */            if(next == ca-&gt;order_que-&gt;tail) {                pthread_mutex_unlock(&amp;(ca-&gt;order_que-&gt;lock));                continue;            }**             /*             * there is space in the queue, add the order and bump             * the head             */            if(ca-&gt;verbose == 1) {                now = CTimer();                printf(\"%10.0f client %d: \",now,ca-&gt;id);                printf(\"queued stock %d, for %d, %s\\n\",                    order-&gt;stock_id,                    order-&gt;quantity,                    (order-&gt;action ? \"SELL\" : \"BUY\"));             }             **ca-&gt;order_que-&gt;orders[next] = order;            ca-&gt;order_que-&gt;head = next;            queued = 1;            pthread_mutex_unlock(&amp;(ca-&gt;order_que-&gt;lock));            /*             * spin waiting until the order is fulfilled             */            while(order-&gt;fulfilled == 0);**             /*             * done, free the order and repeat             */            FreeOrder(order);        }    }    return(NULL);}In this code segment the lines where the client is synchronizing are in bold face. Notice that the head and tail pointers for the share order queue must be managed as a critical section. Each client must add an order and update the head and tail pointers atomically.To do this using pthread_mutex_t is a little tricky. Why? Because when the client must test to make sure the queue isn’t full before it updates the head and tail pointers (or an order will be lost). If the queue is full, the client must wait, but it can’t stop and wait holding the lock or a deadlock will occur. Thus, when the queue is full, the client must drop its lock (so that a trader can get into the critical section to dequeue an order thereby opening a slot) and loop back around to try again.This type of syncronization is called “polling” since the client “polls” the full condition and loops while the condition polls as true.Notice also that the client comes out of the polling loop holding the lock so that it adds the order to the queue and moves the head pointer atomically. Only after it has successfully added the order to the queue does the client drop its lock.The other tricky business here is in the code where the client waits for the trader to fulfill the order// spin waiting until the order is fulfilledwhile(order-&gt;fulfilled == 0);Why is there no lock? Notice that the lock for the order has been dropped. Should there be a lock here?Well, if there is, it can’t be the same lock as the one for the order queue or the trader (which will also use this lock to test the empty condition) won’t be able to get into the critical section to dequeue an order. It is possible to have added a pthread_mutex_t lock to the order structure but the syncronization of the fulfillment of a single order is between a single client and a single trader. That is, the client and trader are not racing. Rather, the trader needs to send a signal to the client that the trade is done and the client needs to wait for that signal. Since there is only going to be one client waiting and one trader sending the signal, the client can simply “spin” polling the value of the int fulfilled flag until the trader sets it to 1. This method of syncronization works only if the memory system of the processor guarantees that memory reads and writes are atomic (which the x86 architecture does).Thus the while loop shown above simply spins until the value in the order structure gets set to 1, thereby “holding” the client until the trade completes.The trader thread code is as follows:void *TraderThread(void *arg){    struct trader_arg *ta = (struct trader_arg *)arg;    int dequeued;    struct order *order;    int tail;    double now;    int next;    while(1) {         **dequeued = 0;        while(dequeued == 0) {            pthread_mutex_lock(&amp;(ta-&gt;order_que-&gt;lock));            /*             * is the queue empty?             */            if(ta-&gt;order_que-&gt;head == ta-&gt;order_que-&gt;tail) {                pthread_mutex_unlock(&amp;(ta-&gt;order_que-&gt;lock));                /*                 * if the queue is empty, are we done?                 */                if(*(ta-&gt;done) == 1) {                    pthread_exit(NULL);                }                continue;            }**             /*             * get the next order             */            next = (ta-&gt;order_que-&gt;tail + 1) % ta-&gt;order_que-&gt;size;            order = ta-&gt;order_que-&gt;orders[next];            ta-&gt;order_que-&gt;tail = next;             **pthread_mutex_unlock(&amp;(ta-&gt;order_que-&gt;lock));**             dequeued = 1;        }        /*         * have an order to process         */         **pthread_mutex_lock(&amp;(ta-&gt;market-&gt;lock));        if(order-&gt;action == 1) { /* BUY */            ta-&gt;market-&gt;stocks[order-&gt;stock_id] -= order-&gt;quantity;            if(ta-&gt;market-&gt;stocks[order-&gt;stock_id] &lt; 0) {                ta-&gt;market-&gt;stocks[order-&gt;stock_id] = 0;            }        } else {            ta-&gt;market-&gt;stocks[order-&gt;stock_id] += order-&gt;quantity;        }        pthread_mutex_unlock(&amp;(ta-&gt;market-&gt;lock));**         if(ta-&gt;verbose == 1) {            now = CTimer();            printf(\"%10.0f trader: %d \",now,ta-&gt;id);            printf(\"fulfilled stock %d for %d\\n\",                order-&gt;stock_id,                order-&gt;quantity);        }        /*         * tell the client the order is done         */         **order-&gt;fulfilled = 1;**     }    return(NULL);}In the trader thread there are three synchronization events (which I have tried to boldface).  synchronizing on the empty condition and head/tail pointers of the order queue  synchronizing with other traders on the stock record in the market  setting the fulfilled flag in the order structure to tell the client the order is completedLike with the client, the trader takes a lock, then tests the condition of the queue only this time it is looking to make sure the queue is not empty (while the client is worried about the queue being full). If the queue is empty, the trader must drop the lock and loop around until there is work. It then comes out of the loop holding the lock so it can dequeue the order and move the head and tail pointer atomically.The trader includes an additional wrinkle for shutting down the entire simulation. In the case of the client, the main loop runs for as many orders as there are going to be for each client. The traders, however, don’t know when the clients are done. To tell the traders, the trader argument structurestruct trader_arg{        int id;        struct order_que *order_que;        struct market *market;        int *done;        int verbose;};includes a pointer to an integer (int *done) which will be set to 1 by the main thread once it has joined with all of the clients. That is, the main thread will try and join will all clients (which will each finish once their set of orders is processed), set the done flag, and then join with all traders. Take a look at the main() function in the source code to see the details. However, in the spin loop where the trader is waiting for the queue not to be empty, it must also test to see if the main thread has signaled that the simulation is done.Note also that the traders must synchronize when accessing the market since each BUY or SELL order must be processed atomically. That is, there is a race condition with respect to updating the stock balance that the traders must avoid by accessing the market atomically.Finally, once an order has been successfully executed, the trader sets the int fulfilled flag to tell the client that it can continue.Speed of Solution 1On the machine I used (my laptop which is a Mac with a 2.8Hz i7 having 2 cores and hyperthreading), I get the following outputs:MossPiglet% ./market1 -c 1 -t 1 -q 1 -s 1 -o 100000160194.266347 transactions / secMossPiglet% ./market1 -c 1 -t 1 -q 10000 -s 1 -o 100000178848.307204 transactions / secMossPiglet% ./market1 -c 1 -t 2 -q 1 -s 1 -o 10000069281.818970 transactions / secMossPiglet% ./market1 -c 1 -t 2 -q 10000 -s 1 -o 10000067015.364164 transactions / secMossPiglet% ./market1 -c 2 -t 2 -q 1 -s 1 -o 100000116164.790273 transactions / secMossPiglet% ./market1 -c 2 -t 2 -q 10000 -s 1 -o 100000116390.389026 transactions / secMossPiglet% ./market1 -c 100 -t 100 -q 100 -s 1 -o 10425.016132 transactions / secStare at these results for a minute. Hilarious, no? What do they say, and then (more importantly) what do they mean?First, on my Mac, the best performance is when there is 1 client, 1 trader, and a good sized queue between them to speed match. With 2 physical cores and only two threads you’d expect pretty good performance (especially if the i7 and OSX keeps the threads pinned to cores).You can also see that hyperthreading isn’t doing you much good. Intel says it gives you almost two additional “virtual cores” but when we run this code with 2 clients and 2 traders, the performance goes down by about a third. So much for virtual cores (at least under OSX – try this on Linux and see if it is any better).The size of the queue seems to matter sometimes, but not others (that’s curious). And the performance is abysmal when there are 100 clients and 100 traders.The Effects of PollingClearly there are interesting and (perhaps) bizarre interactions taking place between the implementation of pthreads that is available for OSX and the Intel i7. However one thing is certain from the code: polling wastes CPU time slices. That is, when a thread is looping it is entitled to do so for its entire time slice. For the duration of that time slice (technically speaking – will discuss the exception in a minute) the thread is guaranteed not to be descheduled for another thread. Time slices are in the 50ms to 100ms range these days.However, to see this effect, think of an extreme example where the time slice is 1 second. That is, when a thread is given the CPU, it will use it for 1 second unless it tries to do an I/O (print to the screen, etc.). Imagine that there is one processor (no hyperthreading), 1 client thread, and 1 trader thread. What is the maximum transaction rate? It should be about 0.5 transactions / second since in every second, there is a 50% chance that a client thread or trader thread is assigned the processor and during that second it simply spins waiting. Thus in half of the second, no work gets done while the CPU allows the spinning thread to run out its time slice.It turns out that OSX (and Linux) is likely smarter than I’ve indicated in this simple thought experiment. Both of these systems “know” that the client and trader threads are calling pthread_mutex_lock() in their spin loops. The lock call generates a system call and there are really tricky locking features that can be employed. However in the client while loop where it waits for the int fulfilled flag and there is no lock, the OS has no choice but to let the thread spin out its time slice.Solution 2: Synchronizing the Order Queue using Condition VariablesHere is the source code for a solution that uses condition variables to synchronize the order queue. Notice a change to the order queue structure:struct order_que{        struct order **orders;        int size;        int head;        int tail;        pthread_mutex_t lock;        pthread_cond_t full;        pthread_cond_t empty;** };in which two condition variables (for the full and empty conditions) have been added. In the client thread, the queue synchronization becomeswhile(queued == 0) {     **pthread_mutex_lock(&amp;(ca-&gt;order_que-&gt;lock));**     next = (ca-&gt;order_que-&gt;head + 1) % ca-&gt;order_que-&gt;size;    /*     * is the queue full?     */    while(next == ca-&gt;order_que-&gt;tail) {         **pthread_cond_wait(&amp;(ca-&gt;order_que-&gt;full),                  &amp;(ca-&gt;order_que-&gt;lock));**         next = (ca-&gt;order_que-&gt;head + 1) % ca-&gt;order_que-&gt;size;    }Notice how this works. Like before, the client must take a mutex lock. However, the pthread_cond_wait() primitive allows the client to “sleep” until it has been signaled and to automatically drop the lock just before going to sleep. The client will sleep (not spinning on the CPU) until some other thread calls pthread_cond_signal() on the same condition variable that was specified as the first argument to pthread_cond_wait(). To be able to drop the lock on behalf of the client, the call to pthread_cond_wait() must also take (as its second argument) the address of the lock to drop.The pthread_cond_wait() primitive also has another important property. When the client is awoken by some call to pthread_cond_signal() it will be allowed to try and acquire the lock before the call to pthread_cond_wait() completes. Thus, when the wait is over, the client “wakes up” holding the lock.Test under LockThis features is called test-under-lock because it allows the caller of “wait” to hold a lock, conduct a condition test, and sleep all as a single atomic operation. It also, then, gives the caller the lock back once the wait completes. Note that you can’t easily implement test-under-lock with mutexes and sleep. For example, if you were to writepthread_mutex_lock(&amp;lock);while(queue-&gt;full == 1) {    pthread_mutex_unlock(&amp;lock);    /*     * whoops -- what if it goes to not full here?     */    sleep();}There is a moment in time right between the call to pthread_mutex_unlock() and sleep() where the queue status could change from full to not full. However, the thread will have already decided to go to sleep and, thus, will never wake up.The pthread_cond_wait() call is specially coded to avoid this race condition.Now take a look at the trader thread just after the thread has determined that there is work to do:    //get the next order    next = (ta-&gt;order_que-&gt;tail + 1) % ta-&gt;order_que-&gt;size;    order = ta-&gt;order_que-&gt;orders[next];    ta-&gt;order_que-&gt;tail = next;     **pthread_cond_signal(&amp;(ta-&gt;order_que-&gt;full));    pthread_mutex_unlock(&amp;(ta-&gt;order_que-&gt;lock));**     dequeued = 1;The trader thread must call pthread_cond_signal() on the same condition variable that the client threads are using to wait for the queue to no longer be full.Similarly, the trader thread (before it processes an order) must ensure that there is work in the queue. That is, it cannot proceed until the queue is no longer empty. Its code isdequeued = 0;while(dequeued == 0) {    pthread_mutex_lock(&amp;(ta-&gt;order_que-&gt;lock));    /*     * is the queue empty?     */    while(ta-&gt;order_que-&gt;head == ta-&gt;order_que-&gt;tail) {        /*         * if the queue is empty, are we done?         */        if(*(ta-&gt;done) == 1) {            pthread_cond_signal(&amp;(ta-&gt;order_que-&gt;empty));            pthread_mutex_unlock(&amp;(ta-&gt;order_que-&gt;lock));            pthread_exit(NULL);        }         **pthread_cond_wait(&amp;(ta-&gt;order_que-&gt;empty),                  &amp;(ta-&gt;order_que-&gt;lock));**     }Here the trader thread is waiting while the queue is empty. Thus, the client thread must signal a waiting trader (if there is one) once it has successfully queued an order. In the client thread    ca-&gt;order_que-&gt;orders[next] = order;    ca-&gt;order_que-&gt;head = next;    queued = 1;     **pthread_cond_signal(&amp;(ca-&gt;order_que-&gt;empty));**     pthread_mutex_unlock(&amp;(ca-&gt;order_que-&gt;lock));Summarizing pthread_cond_wait() and pthread_cond_signal()In this example, there are two condition variables in struct order_que:  empty: which is signaled by a client when the queue is not empty  full: which is signaled by a trader when the queue is no longer fullClients must call pthread_cond_wait() on the full variable when the detect that the queue is full. Traders must call pthread_cond_wait() on the empty variable when they detect that the queue is empty.The API for condition variables is  int pthread_cond_init(pthread_cond_t *variable, pthread_cond_attr_t *attr): initializes a condition variable (second argument NULL says to use pthreads defaults)  int pthread_cond_wait(pthread_cond_t *variable, pthread_mutex_t *lock): sleep until a signal is sent to the condition variable passed in the first argument and will reacquire the lock (passed as the second variable) before the wait completes.  int pthread_cond_signal(pthread_cond_t *variable): signals at least one thread blocked on the condition variable passed as its first argument.There are several interesting caveats to understand about the pthread specification with respect to condition variables. First, a signal sent to a variable where no thread is waiting is just a noop (i.e. it does nothing). Notice that in Solution 2, the clients and traders always call pthread_cond_signal() regardless of whether there is a thread waiting or not. Signals that come in when no threads are waiting are just lost.Thus, in this example, the calls to pthread_cond_signal() occur under the same lock that is being used to control the wait. That way, the signaler “knows” it is in the critical section and hence no other thread is in its critical section so there is no race condition that could cause a lost signal.Another, more subtle point, is that a thread that has been signaled is not guaranteed to acquire the lock immediately. Notice that in the example, each thread retests the condition in a loop when it calls pthread_cond_wait(). That’s because between the time it is signaled and the time it reacquires the lock, another thread might have “snuck in” grabbed the lock, changed the condition, and released the lock.Most implementations try and give priority to a thread that has just awoken from a call to pthread_cond_wait() but that is for performance and not correctness reasons. To be completely safe, the threads must retest after a wait since the immediate lock acquisition is not strictly guaranteed by the specification.Lastly, the specification does not say which thread (among more than one that are waiting on a condition variable) should be awakened. Again, good implementations try and choose threads to wake up in FIFO order as a default, but that isn’t guaranteed (and may not be desirable in all situations). You should never count on the implementation of wait/signal in pthreads being “fair” to the threads that are waiting.Speed of Solution 2Here are the performance results for the same set of tests as for Solution 1:MossPiglet% ./market2 -c 1 -t 1 -q 1 -s 1 -o 100000291456.104778 transactions / secMossPiglet% ./market2 -c 1 -t 1 -q 10000 -s 1 -o 100000283434.731280 transactions / secMossPiglet% ./market2 -c 1 -t 2 -q 1 -s 1 -o 100000294681.020728 transactions / secMossPiglet% ./market2 -c 1 -t 2 -q 10000 -s 1 -o 100000293809.091170 transactions / secMossPiglet% ./market2 -c 2 -t 2 -q 1 -s 1 -o 100000105729.484982 transactions / secMossPiglet% ./market2 -c 2 -t 2 -q 10000 -s 1 -o 100000102448.573029 transactions / secMossPiglet% ./market2 -c 100 -t 100 -q 100 -s 1 -o 10369.965854 transactions / secYee haw! Not bad, eh? The best Solution 1 could do was about 178K transactions per second and Solution 2 goes like 295K transactions per second. Hyperthreading does even worse relative to non-hyperthreading in Solution 2, but it is still 40% faster than its Solution 1 counterpart. However, the test with a bunch of threads still really performs badly. Let’s fix that.Solution 3: Using a condition variable to signal order fulfillmentThe problem here with the last test is that all of the clever scheduling tricks that the pthreads implementation is using are failing for the spin loop in the client where it waits for the order to be fulfilled. The pthreads code doesn’t “know” a spin is taking place so it can’t tell the OS to deprioritize the clients while they wait. When there are a lot of clients there is a lot of useless spinning. However we now know how to fix this problem: use a condition variable to implement a wait and signal. Here is the complete source code for Solution 3.First, we add a condition variable to struct orderstruct order{        int stock_id;        int quantity;        int action;     /* buy or sell */        int fulfilled; **pthread_mutex_t lock;        pthread_cond_t finish;** };Notice that we had to add a lock as well. Condition variables implement “test under lock” which means we need to “test” the fulfilled condition under a lock or there will be a race condition.Next, in the client thread, we lock the order, test its condition (in case the trader thread has already fulfilled the order) and wait if it hasn’t:    //wait using condition variable until    //order is fulfilled         pthread_mutex_lock(&amp;order-&gt;lock);    while(order-&gt;fulfilled == 0) {        pthread_cond_wait(&amp;order-&gt;finish,&amp;order-&gt;lock);    }    pthread_mutex_unlock(&amp;order-&gt;lock);Then, in the trader thread, instead of just setting the fulfilled flag    //tell the client the order is done    pthread_mutex_lock(&amp;order-&gt;lock);    order-&gt;fulfilled = 1;    pthread_cond_signal(&amp;order-&gt;finish);    pthread_mutex_unlock(&amp;order-&gt;lock);Notice that the flag must be set inside a critical section observed by both the client and the trader threads. Otherwise, bad timing might allow the trader thread to sneak in, set the value, and send a signal that is lost in between the time the client tests the flag and calls pthread_cond_wait(). Think about this possibility for a minute. What happens if we remove the call to pthread_mutext_lock() and pthread_mutex_unlock() in the trader thread. Can you guarantee that the code won’t deadlock?Speed of Solution 3Here are the same runs as before:MossPiglet% ./market3 -c 1 -t 1 -q 1 -s 1 -o 100000156951.367687 transactions / secMossPiglet% ./market3 -c 1 -t 1 -q 10000 -s 1 -o 100000151438.489162 transactions / secMossPiglet% ./market3 -c 1 -t 2 -q 1 -s 1 -o 100000150724.983497 transactions / secMossPiglet% ./market3 -c 1 -t 2 -q 10000 -s 1 -o 100000146666.960399 transactions / secMossPiglet% ./market3 -c 2 -t 2 -q 1 -s 1 -o 10000091147.153955 transactions / secMossPiglet% ./market3 -c 2 -t 2 -q 10000 -s 1 -o 10000090862.384876 transactions / secMossPiglet% ./market3 -c 100 -t 100 -q 100 -s 1 -o 1075545.821326 transactions / secUm – yeah. For all but the last case with 200 total threads, the speed is slower. However, look at the last run. It is MUCH faster than the same runs for Solution 1 and Solution 2.This exercise illustrates an important point in performance debugging. What might be fast at a small scale is really slow at a larger scale. Similarly, optimizations for the large scale may make things worse at a small scale. You have been warned.In this case, however, the likely situation is that you have many more clients than traders. Let’s see how they compare in that case.MossPiglet% ./market3 -c 1000 -t 10 -q 10000 -s 1 -o 1068965.526313 transactions / sec./market2 -c 1000 -t 10 -q 10000 -s 1 -o 10That’s right, market2 doesn’t finish. In fact, it pretty much kills my laptop. I’m afraid to try market1. Seriously.For your Personal EnjoymentSo far, in these examples, we haven’t focuses on the number of stocks in the market. That is, in each case, I’ve tested the system with only one stock that clients and traders manipulate (the -s parameter is 1). It could be that there is additional performance to be gained by using a lock for each stock instead of a global lock on the whole market. Here is the full source code for a solution that uses a separate lock per stock.We’ll leave the analysis of this code to you as an exercise that might prove both helpful and informative (especially as you prepare for the midterm and final in this class). Does it make the code even faster?A brief word about monitorsCondition variables were originally proposed as part of an operating system concept called a monitor. The concept is a little like mutual exclusion, but with some important differences. Specifically:  at most one thread can be in the monitor at a time (like a critical section)  while in the monitor it is possible for a thread to determine that it must wait for some condition to occur, in which case it leaves the monitor and goes on a queue waiting to get back in  when a thread in the monitor satisfies a condition upon which other threads may be waiting, it signals one of those threads to unblock and attempt to re-enter the monitor  when a thread leaves the monitor (either because it has finished its work in the monitor or because it is waiting), if there are threads waiting to enter (either from the beginning or because they have been signaled) the monitor selects one and admits it.This idea is an old one and there have been many variations (more than one thread allowed in the monitor, threads signaled are given priority over those trying to get in, the signaler leaves and waits to come back in, etc.) but these don’t change its essential functionality.To see why condition variables in threads correspond to monitors, consider the way in which the client and trader threads syncronize in the examples.A client thread, for example, tries to enter “the monitor” when it attempts to take the mutex lock. Either it is blocked (because some other client or trader thread is in the monitor) or it is allowed to proceed. If the queue is full, the client thread leaves the monitor but waits (on some kind of hidden queue maintained by pthreads) for the full condition to clear.When a trader thread clears the full condition, it must be in the monitor (it has the queue locked) and it signals the condition variable. The pthreads implementation selects a thread from the hidden queue attached to that variable and allows it to try and re-enter the monitor by re-locking the lock.Now, hopefully, it becomes clearer as to why the codes must retest once they have been awoken from a wait. Pthreads lets a thread that has been signaled try and retake the mutex lock that is passed to the wake call. For a monitor, this lock controls entry into the monitor and there may be other threads waiting on this lock to enter for the first time. If the lock protocol is FIFO, then one of these other threads may enter before the thread that has been awoken (it remains blocked). Eventually, though, it will be allowed to take the lock (it will get to the head of the lock’s FIFO) but by that time, the condition may have changed.This subtlety trips up many a pthreads programmer. Once you understand it in terms of a monitor with one shared mutex and several conditions that use the mutex, it makes more sense (at least to me).",
        "url"      : "/2017/06/08/Condition-Variables/",
        "date"     : "2017-06-08 13:31:00 +0000"
    } ,
    
    {
        "title"    : "Race Conditions",
        "category" : "Operating System",
        "tags"     : "Thread",
        "content": "  Race conditions and mutexes          Pre-emption and Context Switching      An Example      Critical Sections, Atomic Execution, and Mutual Exclusion                  In Through the Out Door                    A Code Example                  Formal Definition of Race Conditions          Terse advice on mutexes          Race Condition Thought Question                    Race conditions and mutexesOkay – we are ready for our second operating systems concept. Operating systems must be able to protect shared state from race conditions. Rather than giving you a formal definition for a race condition (which I will provide later), we will start with a very simple example.The first thing to understand is that it is possible to run more than one thread on each CPU. Try running the avg-manythread.c on 50 threads or so. The machines you can access have at most 8 processors. How could they run 50 threads?Pre-emption and Context SwitchingThe answer is that the OS and threading system to arrange to multiplex the threads on the CPUs. Each thread is given a turn on some CPU. When it goes to do I/O, or a fixed amount of time has expired, the OS pauses the thread (saving off all of its machine state), and selects another thread to run. It then loads the saved machine state from the new thread onto the CPU and starts that thread at the spot where it last left off (or at the beginning if it was just created).The process of pausing one thread to run another is called pre-emption and the second thread is said to pre-empt the first. The process of switching to a new thread (as a result of a pre-emption event) is called context switching and the saved machine state that is necessary to get a thread running again after a pause is often called a context.An ExampleConsider the use of an ATM at a bank. Somewhere, in bowels of your bank’s computer system, is a variable called “account balance” that stores your current balance. When you withdraw $200, there is a piece of assembly language code that runs on some machine that does the following calculation:ld  r1,@richs_balancesub r1,$200st  r1,@richs_balancewhich says (in a fictitious assembly language) “load the contents of rich’s account_balance” into register r1, subtract 200 from it and leave the result in r1, and store the contents of r1 back to the variable rich’s account_balance.” The code is executed sequentially, in the order shown.So far, so good.Your bank is a busy place, though, and there are potentially millions of ATM transactions all at the same time, but each to a different account variable. So when Bob withdraws money, the machine executesld  r2,@bobs_balancesub r2,$200st  r2,@bobs_balanceand Fred’s transactions look likeld      r3,@freds_balancesub     r3,$200st      r3,@freds_balanceIn each case, the register and the variable are different.Now, let’s assume that the bank wants to use threads as a programming convenience, and that the programmer has chosen preemptive threads as we have been discussing. Each set of instructions goes in its own threadthread_0                thread_1                thread_2-------                 --------                --------ld r1,@richs_balance    ld r2,@bobs_balance ld  r3,@freds_balancesub r1,$200             sub r2,$200             sub r3,$200st r1,@richs_balance    st r2,@bobs_balance     st r3,@freds_balanceThe thing about preemptive threads is you don't know when pre-emption will take place. For example, thread_0 could start, execute two instructions, and suddenly be preempted for thread_1 which could be preempted for thread_2, and so on    ld      r1,@richs_balance ;; thread_0    sub     r1,$200           ;; thread_0    **** pre-empt! ****    ld      r2,@bobs_balance  ;; thread_1    sub     r2,$200           ;; thread_1    **** pre-empt! ****    ld      r3,@freds_balance ;; thread_2    sub     r3,$200           ;; thread_2    **** pre-empt! ****    st      r1,@richs_balance ;; thread_0    **** pre-empt! ****    st      r2,@bobs_balance  ;; thread_1    **** pre-empt! ****    st      r3,@freds_balance ;; thread_2In fact (and this is the part to get)any interleaving of instructions that preserves the sequential order of each individual thread is legal and may occur.The system cannot choose to rearrange the instructions within a thread but because threads can be preempted at any time all interleavings of the instructions are possible.Again, in this example, there is no real problem (yet). It doesn’t matter where you put the preempts or whether you leave them out – the ATM system will function properly.Now let’s say you’ve thought about this for a good long while and you come up with a scheme. You get a good friend, you give them your ATM PIN number and a GPS synchronized watch, and you say “at exactly 12:00, withdraw $$200.” 12:00 rolls around and you and your friend both go to separate ATMs and simultaneously withdraw $200. Let’s say, further, that you are lucky, your account contains $1000 to begin with, and that the bank’s computers makes two threads:thread_0                thread_1-------                 --------ld r1,@richs_balance    ld r2,@richs_balancesub r1,$200             sub r2,$200st r1,@richs_balance    st r2,@richs_balanceBecause you are lucky and you’ve gotten the bank to launch both threads at the same time, the following interleaving takes placeld r1,@richs_balance ;; thread_0*** pre-empt ***ld r2,@richs_balance ;; thread_1*** pre-empt ***sub r1,$200          ;; thread_0*** pre-empt ***sub r2,$200          ;; thread_1*** pre-empt ***st r1,@richs_balance ;; thread_0*** pre-empt ***st r2,@richs_balance ;; thread_1What is the contents of richs_balance when both threads finish?It should be $$600, right? Both you and your friend withdrew $200 each from your $1000 balance. If this were the way things worked at your bank, however, richs_balance would be $800.Why?Look at what happens step by step. The first ld loads 1000 into r1. thread_0 gets preempted and thread_1 starts. It loads 1000 into r2. Then it gets preempted and thread_0 runs again. r1 (which contains 1000) is decremented by 200 so it now contains 800. Then thread_1 pre-empts thread_0 again, and r2 (which contains 1000 from the last load of r2) gets decremented by 200 leaving 800. Then thread_0 runs again and stores 800 into richs_balance. Then thread_1 runs again and stores 800 into richs_balance and the final value is 800.This problem is called a race condition. It occurs when there is a legal ordering of instructions within threads that can make the desired outcome incorrect. Notice that there are lots of ways thread_0 and thread_1 could have interleaved in which the final value of richs_balance would have been $$600 (the correct value). It is just that you are lucky (or you tried this trick enough so that the law of averages eventually worked out for you) to cause one of the $200 withdrawals to disappear.Note also that the problem is worse if the bank has a machine with at least 2 CPUs. In this case, thread_0 and thread_1 run at the same time which means that they both execute the fist subtraction at the same time. How does a race condition occur in this case? Turns out that the memory system for multi-processors implements memory write operations one-at-a-time (multiple simultaneous reads are possible from cache). Thus when they both go to store the balance of $$800, one will write its value first and the other will over write that value with the same $800. The outcome is the same as the profitable interleaving shown above with preemption.Critical Sections, Atomic Execution, and Mutual ExclusionIt may be obvious, but the way to ensure that the bank balance is always computed correctly is to ensure that at most one thread is allowed to execution the load-substract-store sequence at a time. As long as those three instructions are executed, in order, without any other threads interleaving themselves, the balance will be computed correctly.A segment of code that must be executed sequentially without the potential interleaving of other threads is called a Critical Section. We will sometime refer to the notion that all instructions within a critical section will be executed without being interleaved as Atomic Execution as in the sentenceThe instructions within a critical section are executed atomically.meaning that they will not be interleaved with instructions executed by other threads executing in the same section of code.The process of ensuring that at most one thread can be executing in a critical section is often termed mutual exclusion to distinguish it from other forms of synchronization. Those other forms will become clear as the course progress. For now, the important concept to grasp is that we need a way to make sure, no matter what the circumstances, at most one thread can be executing is specific code segments where a race condition due to interleaving could produce an unwanted computation.As the following example attempts to illustrate, mutual exclusion is typically implemented with some form of “lock.” A lock has the following semantics:  a lock is initially in the state “unlocked”  there is a primitive that allows a thread to attempt to “lock” the lock  there is a primitive that allows a thread to “unlock” a lock that is locked  any attempt to lock a lock that is already locked, blocks the thread and puts it on a list of threads waiting for the lock to be unlocked  when a lock is unlocked by the thread that is holding the lock, if there are threads waiting (because they tried to lock before), one is selected by the system, given the lock, and allowed to proceed.This sounds a bit like Dr. Seuss, but is is pretty simple. Think of it as a lock on a door to a room. When one person enters and locks the door, any other attempts to entry will be blocked. In the cases we’ll study, the threads that try to get into the room (the critical section) while someone (another thread) is in it will wait patiently just outside the door. Then, when a thread that is in the room leaves, it will pick one of the waiting threads (and only one) and allow it to enter the room and lock the door behind it.In Through the Out DoorMutual exclusion has some interesting properties. First, it is important that any thread that enters a critical section by locking it, leave it by unlocking it. In the room example, if a person enters the room, locks the door, and then climbs out a window without unlocking the door, no one else will ever be able to get in. In a program, leaving a critical section without calling the unlock primitive is like climbing out the window of the room. Worse, (and don’t think of this as being morbid) if the person in the room dies (or your thread exits due to a fault or because you have returned) the door never gets unlocked and threads waiting will wait forever.The other thing to understand is that even when your threads correctly enter and leave critical sections, the size of the section influences the amount of concurrency your program will have. For example, if every ATM in the US had to lock the entire bank to implement a transaction, ATM response time would probably be pretty slow.Thus, you typically try and keep the length of each critical section as small as possible, both to maximize the amount of concurrency and also to minimize the possibility of having a bug cause a thread to die to exit the section accidentally without calling unlock.A Code ExampleLook at race1.c. Its usage israce1 nthreads stringsize iterationsThis is a pretty simple program. The command line arguments call for the user to specify the number of threads, a string size and a number of iterations. Then the program does the following. It allocates an array of stringsize+1 characters (the +1 accounts for the null terminator). Then it forks off nthreads threads, passing each thread its id, the number of iterations, and the character array. Here is the output if we call it with the arguments 4, 4, 1../race1 4 40 1Thread 3: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 2: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCThread 1: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBThread 0: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALooks fine, doesn’t it? Try again with more threads and iterations:./race1 10 40 2Thread 1: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 1: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBThread 2: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBThread 2: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCThread 3: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCThread 3: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 0: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 0: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAThread 4: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAThread 4: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEThread 6: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEThread 6: GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGThread 7: GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGThread 7: HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHThread 8: HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHThread 8: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIThread 9: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIThread 9: JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJThread 5: JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJThread 5: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFDoes this look right? Not exactly. In the main loop of each thread for (j = 0; j &lt; t-&gt;size-1; j++) {    t-&gt;s[j] = 'A'+t-&gt;id; }the thread should put its own letter (defined as the thread’s ID + ‘A’) in the buffer. Thus, for example, Thread 1 should always print ‘B’ and not any other character.Notice that all 4 threads share the same buffer s in the program. Consider the output from race2.c./race2 4 40 2Thread 2: AAAAAAAAAAAAAAAAABBBCCCCCCCCCCCCCCCCCCCCThread 3: CCCCCCCCCCCCCCCCCAAAAAAAAAAAAAAABBBBBDDDThread 1: DDDDDDDDDDDDDDDDDDDDAAAAAAAAAAAABBBBBDBBThread 0: BBBBBBBBBBBBBBBBBBDDAAAAAAAAAAAAAAAAAAAAThread 3: AAAAAAAAAAAAABBBBCCCDDDDDDDDDDDDDDDDDDDDThread 2: AAAAAAAAAAAAAABBBCBBBBBBBBBBBBCCCCCCCCCCThread 1: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCThread 0: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAThe code is exactly the same as race1.c but with a delay loop scheduled in the main write loop to allow a greater chance for preemption. The reason you don’t see this problem in the first execution is because the machine is too fast and Linux is too smart. That is, without the delay loop sometimes the speed of the machine and the Linux thread schedule “get it right” and you get the answer you are expecting. However, if you ran this program over and over, eventually you’d see an output that you didn’t expect. Don’t care? Well you should because things like airplanes which rely on many such calculations per second need to get the right answer every time. Race conditions are difficult to debug because they often manifest only rarely. Thus be forewarned:Just because your program runs and doesn’t appear to have a race condition doesn’t mean it is free from race conditions.In this example, we can fix the race condition by enforcing the condition that no thread can be interrupted by another thread when it is modifying and printing s. This can be done with a mutex (which is short for mutual exclusion), sometimes called a “lock.” There are three procedures for dealing with mutexes in pthreads:pthread_mutex_init(pthread_mutex_t *mutex, NULL);pthread_mutex_lock(pthread_mutex_t *mutex);pthread_mutex_unlock(pthread_mutex_t *mutex);You create a mutex with pthread_mutex_init(). Then any thread may lock or unlock the mutex. When a thread locks the mutex, no other thread may lock it. If they call pthread_mutex_lock() while the thread is locked, then they will block until the thread is unlocked. Only one thread may lock the mutex at a time.So, we fix the race program with race3.c. You’ll notice that a thread locks the mutex just before modifying s and it unlocks the mutex just after printing s. This fixes the program so that the output makes sense:./race3 10 40 2Thread 0: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAThread 0: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAThread 2: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCThread 2: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCThread 6: GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGThread 6: GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGThread 4: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEThread 4: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEThread 8: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIThread 8: IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIThread 1: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBThread 1: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBThread 7: HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHThread 7: HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHThread 9: JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJThread 9: JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJThread 3: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 3: DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDThread 5: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFThread 5: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFAre these outputs correct? Yes. Each thread prints a full buffer full if its specific letter. Notice that the order in which the threads is not controlled. That is, Linux is still free to schedule Thread 2 before Thread 1 even though the code called pthread_create() for Thread 1 before Thread 2. However, the lock ensures that each thread completely fills the buffer and prints it without being preempted by another thread.Formal Definition of Race ConditionsNow we’ll try for a formal definition.Race condition: the possibility in a program consisting of concurrent threads that all legal instruction orderings do not result in exactly the same output.Notice that under this definition of race condition the program race3.c still has a race condition – just a different one than the one we fixed with a mutex lock. Technically, because the threads can run in any order, different outputs from the same execution are possible. The question of whether a race condition is a bug or not has to do with what the programmer intended. If any thread ordering is fine but we want each thread to fill and print its buffer, than this version of the code is correct even though it has a race condition.Terse advice on mutexesRace conditions exist, and mutexes and condition variables (see next lecture) are needed when ever preemptive threads update a shared data structure. There is no problem is the threads only read what is there. If updates take place, however, thread access must be synchronized. That is the key. In all of these examples, some shared variable is being updates. If you are using pre-emption, and you see updates to shared state, think “race condition.”Race Condition Thought QuestionConsider the code in race_ABC.c What does it do? Does it contain a race condition?",
        "url"      : "/2017/06/08/Race-Conditions/",
        "date"     : "2017-06-08 12:40:00 +0000"
    } ,
    
    {
        "title"    : "Introduction to Threads",
        "category" : "Operating System",
        "tags"     : "Thread",
        "content": "  Introduction          Threads      So what is a Thread?        Threads versus Processes  Why threads?  A Simple Example          C Code, No threads      Computing the Average using One Thread      Reading through the Code      Creating a thread      When does the thread run?      Waiting for the result      Computing the sum in parallel        Synchronization  How much faster is it?  Experimenting with these examples          Building the examples      IntroductionWe’ll spend quite a bit of time in this class discussing the concept of “concurrency” – the notion that independent sets of operations can be occurring at the same time inside the machine when it is executing a program. For example, when a program does I/O (say to a disk), the CPU may be switched that it is able to work on some other task while the I/O is taking place. Operating systems both allow the user to manage concurrency and also (in many cases) exploit concurrency on behalf of the user to improve performance. As a result, concurrency and concurrency management/control will be themes that recur throughout this course.ThreadsThreads are a programming abstraction that is designed to allow a programmer to control concurrency and asynchrony within a program. In some programming languages, like Java, threads are “first class citizens” in that they are part of the language definition itself. For others, like C and C++, threads are implemented as a library that can be called from a program but otherwise are not considered part of the language specification.Some of the differences between having threads “in the language” and threads “as a library” are often subtle. For example, a C compiler need not take into account thread control while a Java compiler must. However one obvious difference is that in the library case, it is possible to use different thread libraries with the same language. In this class, we’ll be programming in C, and we’ll use both POSIX Threads and a thread library specifically designed for the OS project called Kthreads. Kthreads and POSIX threads are similar in that they are both thread abstractions and they are both implemented as libraries that can be called from a C program. They are different in that POSIX threads requires operating system support to work properly and, thus, can’t be used directly to implement the operating system. In contrast, Kthreads can be implemented without the OS using only the C language compiler and a little bit of the C runtime. For this reason, we can use Kthreads as an abstraction with which to build an operating system (i.e. there is no circular dependence).We’ll study both before the end of the class but we’ll start with POSIX threads since they a standard.So what is a Thread?There are many different kinds of thread abstractions. In this class, we’ll use the typical “OS thread” abstraction that underpins POSIX threads, but particularly for language-defined threads, different definitions are possible.Abstractly, for our purposes, a thread is three things:  a sequential list of instructions that will be executed  a set of local variables that “belong” to the thread (thread private)  a set of shared global variables that all threads can read and writeIt is no accident that this definition corresponds roughly to the C language sequential execution model and variable scoping rules. Operating systems are still, for the most part, written in C and thus thread libraries for C are easiest to understand and implement when they conform to C language semantics.Threads versus ProcessesRecall from your C programming experiences, that your compiled program becomes a “process” when you run it. We’ll discuss processes and what they really are at length, but at this stage, it is enough to know that a C program runs as a process when it is executed on, say, a Linux system.A C program also defines a sequential list of instructions, local variables, and global variables so you might be asking “What is the difference between a thread and a process?”The answer is “not much” as long as there is only one thread. However, as discussed previously, threads are an abstraction designed to manage concurrency which means it is possible to have multiple threads “running” at the same time. Put another way,A standard C program when executing is a process with one thread.However, it is possible (using a thread library) to write a C program that defines multiple threads. That isA threaded C program, when executing, is a process that contains one or more threads.Furthermore, these threads are logically independent and thus may be executed concurrently. They don’t have to be (it depends on the implementation) but the abstraction says that the threads are independent.Why threads?There are many reasons to program with threads. In the context of this class, there are two important ones:  They allow you to deal with asynchronous events synchronously and efficiently.  They allow you to get parallel performance on a shared-memory multiprocessor.You’ll find threads to be a big help in writing an operating system.A Simple ExampleBefore we dive into an anatomical and physiological exploration of POSIX threads, which heretofore will be referred to as pthreads, it is probably helpful to walk through a simple example. Pthreads are widely used and their full interface is somewhat complicated. We’ll eventually discuss much of it, but “the basics” as most easily understood through an example.C Code, No threadsTo begin with, consider a simple program that computes the average over a set of random numbers. In the following examples we’ll use a Linux-internal random number generator rather than numbers from a file to keep the code easier to read. You might also think that you know what the answer will be ahead of time. For example, if the random number generator generates numbers on the interval (0,1) then you’d expect the average to be 0.5. How true is that statement? Is it affected by the number of numbers in the set? This example can also be used to investigate these kinds of questions but mostly it is designed to introduce the way in which pthreads and C interact.The basic program generates an array that it fills with random numbers from the interval (0,1). It then sums the values in the array and divides by the number of values (which is passed as an argument from the command line).Here is the C code. I’ve put a commented version of the code in avg-nothread.c that also includes argument sanity checks. To improve readability in these notes, however, the in-lined code will remove parts that are good practice but don’t shed light on the use of threads. We’ll also describe how to build and run the examples in this lecture in its last section.Here is the code#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;/* * program to find the average value of a set of random numbers * * usage: avg-nothread count * * where count is the number of random values to generate */char *Usage = \"usage: avg-nothread count\";#define RAND() (drand48()) /* basic Linux random number generator */int main(int argc, char **argv){    int i;    int n;    double *data;    double sum;    int count;    count = atoi(argv[1]);  /* count is first argument */     /*     * make an array large enough to hold #count# doubles     */    data = (double *)malloc(count * sizeof(double));    /*     * pick a bunch of random numbers     */    for(i=0; i &lt; count; i++) {        data[i] = RAND();    }    sum = 0;    for(i=0; i &lt; count; i++) {        sum += data[i];    }    printf(\"the average over %d random numbers on (0,1) is %f\\n\",            count, sum/(double)count);    return(0);}There are a few C language features to note in this simple program. First, it uses the utility function malloc() to allocate dynamically a one-dimensional array to hold the list of random numbers. It also casts the variable count to a double in the print statement since the variable sum is a double.Now is a good time to take a moment to make sure that you understand each line of the program shown above – each line. If there is something you don’t recognize or understand in this code you will want to speak with the instructor or the TAs about brushing up on your C programming skills. This code is about as simple as any C program will be that you will encounter in this class. If it isn’t completely clear to you it will be important to try and brush up because the assignments will depend a working knowledge of C.Computing the Average using One ThreadThe next program performs the same computation, but does so using a single thread rather than in the main body as in the previous program. The full version of this program is available from avg-1thread.c#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; pthread.h &gt;char *Usage = \"usage: avg-1thread count\";#define RAND() (drand48()) /* basic Linux random number generator */struct arg_struct{    int size;    double *data;};struct result_struct{    double sum;};void *SumThread(void *arg){    int i;    double my_sum;    struct arg_struct *my_args;    int my_size;    double *my_data;    struct result_struct *result;    printf(\"sum thread running\\n\");    fflush(stdout);    my_args = (struct arg_struct *)arg;    result = (struct result_struct *)malloc(sizeof(struct result_struct));    my_size = my_args-&gt;size;    my_data = my_args-&gt;data;    free(my_args);    my_sum = 0.0;    for(i=0; i &lt; my_size; i++) {        my_sum += my_data[i];    }    result-&gt;sum = my_sum;    printf(\"sum thread done, returning\\n\");    fflush(stdout);    return((void *)result);}int main(int argc, char **argv){    int i;    int n;    double *data;    int count;    struct arg_struct *args;    pthread_t thread_id;    struct result_struct *result;    int err;    count = atoi(argv[1]);  /* count is first argument */     data = (double *)malloc(count * sizeof(double));    for(i=0; i &lt; count; i++) {        data[i] = RAND();    }    args = (struct arg_struct *)malloc(sizeof(struct arg_struct));    args-&gt;size = count;    args-&gt;data = data;    printf(\"main thread forking sum thread\\n\");    fflush(stdout);    err = pthread_create(&amp;thread_id, NULL, SumThread, (void *)args);    printf(\"main thread running after sum thread created, about to call join\\n\");    fflush(stdout);    err = pthread_join(thread_id,(void **)&amp;result);    printf(\"main thread joined with sum thread\\n\");    fflush(stdout);    printf(\"the average over %d random numbers on (0,1) is %f\\n\",            count, result-&gt;sum / (double)count);    free(result);    free(data);    return(0);}In pseudocode form, the logic is as follows. The main() function does  allocate memory for thread arguments  fill in thread arguments (marshal the arguments)  spawn the thread  wait for the thread to complete and get the result (the sum in this example) computed by the thread  print out the averageand the thread executes  unmarshal the arguments  compute and marshal the sum so it can be returned  return the sum and exitReading through the CodeThe first thing to notice is that the code that computes the sum is performed in a separate C function called SumThread(). The pthreads standard specifies that threads begin on function boundaries. That is, each thread starts with some function call which, in this example, is SumThread(). This “first” function can call other functions, but it defines the “body” of the thread. We’ll call this first function the “entry point” for the thread.The second thing to notice are the types in the prototype for the thread entry point:void *SumThread(void *arg)The standard as implemented for Linux specifies that  the entry point funtion take one argument that is of type (void *)  the entry point function return a single argument that if of type (void *)This typing specification can cause some confusion if you are not entirely comfortable with C pointers so it is important to try and understand why it is defined this way. In C, a (void *) pointer is that can legally point to any data type. The key is that your program can make the decision about what it points to at run time. That is, a a (void *) tells the compiler that your program will determine the type at run time under program control.You can use a (void *) pointer to point to any type that is supported by C (e.g. int,double,char, etc.) but it is most useful when it is used to point to a structure.A structure is a way for you to define your own composite data type. In a pthreads program, the assumption that the API designers make is that you will define your own data type for the input parameters to a thread and also one for the return values. This way you can pass what ever arguments you like to your threads and have them return arbitrary values.In this example, we define two structures // data type definition for arguments passed to threadstruct arg_struct{        int size;        double *data;};// data type definition for results passed back from threadsstruct result_struct{        double sum;};The argument structure allows the code that spawns the thread to pass it two arguments: the size of the array of values and a pointer to the array. The thread passes back a single value: the sum.Notice that the thread entry point function converts its one argument to a pointer to the argument data structure. The data type for my_args isstruct arg_struct *my_args;and the body of the thread assigns the arg pointer passed as an argument to my_args via a C language cast.my_args = (struct arg_struct *)arg;You can think of the thread as receiving a message with its initial arguments in it as its only parameter. This message comes in a generic “package” with the type (void *) and it is the thread’s job to unpack the message into a structure that it understands. This process is called “unmarshaling” which refers to the process of translating a set of data types from a generic tranport form to one that can be processed locally. Thus the line shown above in which the (void *) is cast to a (struct arg_struct *) is the thread “unmarshaling” its arguments.Similarly, when the thread has finished computing the sum, it needs a data structure to pass back to a thread that is waiting for the result. The code calls malloc() to allocate the memory necessary to transmit the results once the thread has completed:result = (struct result_struct *)malloc(sizeof(struct result_struct));and when the sum is computed, the thread loads the sum into the result structure:result-&gt;sum = my_sum;The marshaling into a (void *) of the (struct result_struct *) takes place directly in the return callreturn((void *)result);From the my_args variable, the thread can then access the size value and the data pointer that points to the array of numbers. Notice that the very next thing the thread does is to call free() on the arg pointer. In a C program it is essential that you keep track of how memory is allocated and freed. Good practive is to free memory as soon as you know the memory is no longer needed. In this example, the code that creates this thread in the main() function calls malloc to allocate the memory that is needed to hold the argument structure.Notice that the thread has called malloc() to create a result variable to pass back the sum. It must be the case that the main() thread calls free() on the result structure it gets back from the thread. Look for the free() call in the main() routine to see where this takes place.Creating a threadThe main() function creates an argument structure, spawns the thread, waits for it to complete, and uses the result that the thread passes back to print the average.Creating and marshaling the arguments for the thread:args = (struct arg_struct *)malloc(sizeof(struct arg_struct));args-&gt;size = count;args-&gt;data = data;The thread that computes the sum is created by the pthread_create() call in the main() function.err = pthread_create(&amp;thread_id, NULL, SumThread, (void *)args);The pthread_create() call takes four arguments and returns a single result. The arguments are  a pointer to a variable of type pthread_t (as an out parameter)  a pointer to a structure indicating how to schedule the thread (NULL means use the default scheduler)  the name of the entry point function  a single pointer to the arguments as a (void *)The return value is an error code, with zero indicating success. If the return value is zero, the variable pointed to by the first argument will contain the thread identifier necessary to interact with the thread (see pthread_join() below).When does the thread run?Logically, the thread begins executing as soon as the call to pthread_create() completes. However it is up to the implementation as to when the thread is actually scheduled. For example, some implementations will allow the spawning thread to continue executing “for a while” before the spawned threads begin running. However, from a logical perspective, the newly created thread and the thread that created it are running “in parallel.”Notice also that the main() function is acting like a thread even though it wasn’t spawned via pthread_create(). Under Linux, the program that begins executing before any threads are spawned is, itself, a thread. The logical abstraction is that Linux “spawned” this first thread for you. That is, when using pthreads, the function main() behaves as if it has been spawned by Linux. It is a little different since it takes two arguments, but for thread scheduling purposes, it behaves like a thread otherwise. We’ll call this thread “the main thread” from now on to indicate that it is the “first” thread that gets created when the program begins to run.Thus the main thread in this example spawns a single thread to compute the sum and waits for this thread to complete before proceeding.Waiting for the resultAfter the main thread spawns the thread to compute the sum, it immediately callserr = pthread_join(thread_id,(void **)&amp;result);The first argument to this call is the identifier (filled in by the call to pthread_create() when the thread was created. The second argument is an out parameter of type (void **). That is, pthread_join() takes a pointer to a (void *) so that it can return the (void *) pointer passed back from the thread on exit.This “pointer to a pointer” parameter passing method often confuses those new to pthreads. The function pthread_join() needs a way to pass back a (void *) pointer and it can’t use the return value. In C, the way that a function passes back a pointer through an out parameter is to take a pointer to that kind of pointer as a parameter. Notice that the type of result is (struct result_struct *). My using the &amp; operator, the parameter passed is the address of result (which is a pointer) and that “pointer to a pointer” is cast as a (void **).Like with pthread_create(), pthread_join() returns an integer which is zero on success and non zero when an error occurs.Here is the output./avg-1thread 100000main thread forking sum threadmain thread running after sum thread created, about to call joinsum thread runningsum thread done, returningmain thread joined with sum threadthe average over 100000 random numbers on (0,1) is 0.499644Notice that the main thread continues to run after the Sum thread is spawned. Then it blocks in pthread_join() waiting for the su thread to finish. Then the sum thread runs and finishes. When it exits, the call to pthread_join() unblocks and the main thread completes.Computing the sum in parallelThe previous example is a little contrived in that there is no real advantage (and probably a very small performance penalty) in spawning a single thread to compute the sum. That is, the first non-threaded example does exaxtly what the single threaded example does only without the extra work for marshaling and unmarshaling the arguments and spawninga and joining. You might ask, then, “why use threads at all?”The answers is that it is possible to compute some things in parallel using threads. In this example, we can modify the sum thread so that it works on a subregion of the array. The main thread can spawn multiple subregions (which are computed in parallel) and then sum the sums that come back to get the full sum. The following example code does this parallel computation of the sums.#include &lt; unistd.h &gt;#include &lt; stdlib.h &gt;#include &lt; stdio.h &gt;#include &lt; pthread.h &gt;char *Usage = \"usage: avg-manythread count threads\";#define RAND() (drand48()) /* basic Linux random number generator */struct arg_struct{    int id;    int size;    double *data;    int starting_i;};struct result_struct{    double sum;};void *SumThread(void *arg){    int i;    double my_sum;    struct arg_struct *my_args;    int my_size;    double *my_data;    struct result_struct *result;    int my_start;    int my_end;    int my_id;    my_args = (struct arg_struct *)arg;    result = (struct result_struct *)malloc(sizeof(struct result_struct));    printf(\"sum thread %d running, starting at %d for %d\\n\",        my_args-&gt;id,        my_args-&gt;starting_i,        my_args-&gt;size);    fflush(stdout);    my_id = my_args-&gt;id;    my_size = my_args-&gt;size;    my_data = my_args-&gt;data;    my_start = my_args-&gt;starting_i;    free(my_args);    my_end = my_start + my_size;    my_sum = 0.0;    for(i=my_start; i &lt; my_end; i++) {        my_sum += my_data[i];    }    result-&gt;sum = my_sum;    printf(\"sum thread %d returning\\n\",        my_id);    fflush(stdout);    return((void *)result);}int main(int argc, char **argv){    int i;    int t;    double sum;    double *data;    int count;    int threads;    struct arg_struct *args;    struct result_struct *result;    int err;    pthread_t *thread_ids;    int range_size;    int index;    count = atoi(argv[1]);  /* count is first argument */     threads = atoi(argv[2]); /* thread count is second arg */    data = (double *)malloc(count * sizeof(double));    for(i=0; i &lt; count; i++) {        data[i] = RAND();    }    thread_ids = (pthread_t *)malloc(sizeof(pthread_t)*threads);    range_size = (count / threads) + 1;    if(((range_size-1) * threads) == count) {        range_size -= 1;    }    printf(\"main thread about to create %d sum threads\\n\",            threads);    fflush(stdout);    index = 0;    for(t=0; t &lt; threads; t++) {        args = (struct arg_struct *)malloc(sizeof(struct arg_struct));        args-&gt;id = (t+1);        args-&gt;size = range_size;        args-&gt;data = data;        args-&gt;starting_i = index;        if((args-&gt;starting_i + args-&gt;size) &gt; count) {            args-&gt;size = count - args-&gt;starting_i;        }        printf(\"main thread creating sum thread %d\\n\",            t+1);        fflush(stdout);        err = pthread_create(&amp;(thread_ids[t]), NULL, SumThread, (void *)args);        printf(\"main thread has created sum thread %d\\n\",            t+1);        index += range_size;    }    sum = 0;    for(t=0; t &lt; threads; t++) {        printf(\"main thread about to join with sum thread %d\\n\",t+1);        fflush(stdout);        err = pthread_join(thread_ids[t],(void **)&amp;result);        printf(\"main thread joined with sum thread %d\\n\",t+1);        fflush(stdout);        sum += result-&gt;sum;        free(result);    }    printf(\"the average over %d random numbers on (0,1) is %f\\n\",            count, sum / (double)count);    free(thread_ids);    free(data);    return(0);}In this example, each thread is given a starting index into the array and a range it needs to cover. It sums the values in that range and returns the partial sum in the result result structure.The main thread spawns each thread one at a time in a loop, giving each its own argument structure. Notice that the argument structure is filled in with the starting index where that thread is supposed to start and the range of values it needs to cover.Aftre the main thread spawns all of the sum threads, it goes into another loop and joins with them, one-at-a-time, in the roder that they were spawned.Here is a a sample output from this multi-threaded program:MossPiglet% ./avg-manythread 100000 5main thread about to create 5 sum threadsmain thread creating sum thread 1main thread has created sum thread 1main thread creating sum thread 2main thread has created sum thread 2main thread creating sum thread 3main thread has created sum thread 3main thread creating sum thread 4sum thread 1 running, starting at 0 for 20000main thread has created sum thread 4sum thread 2 running, starting at 20000 for 20000sum thread 3 running, starting at 40000 for 20000main thread creating sum thread 5sum thread 4 running, starting at 60000 for 20000sum thread 1 returningmain thread has created sum thread 5main thread about to join with sum thread 1sum thread 5 running, starting at 80000 for 20000sum thread 2 returningsum thread 3 returningsum thread 4 returningmain thread joined with sum thread 1main thread about to join with sum thread 2main thread joined with sum thread 2main thread about to join with sum thread 3sum thread 5 returningmain thread joined with sum thread 3main thread about to join with sum thread 4main thread joined with sum thread 4main thread about to join with sum thread 5main thread joined with sum thread 5the average over 100000 random numbers on (0,1) is 0.499644This output is worth studying for a moment. Notice that the main thread starts 5 threads. The it completes the creation of 3 threads. It calls the create for the 4th thread, but before it prints the “created” message sum thread 1 starts running. Then the message saying that 4 was created prints. Then sum thread 2 and 3 start, and then the main thread creates thread 5.This order of execution is not guaranteed. In fact, there are many leagl orderings of thread execution that are possible. All that matters is that the main thread not try and access the result from a thread until after it has joined with that thread.This point is important All of the threads are independent and they can run in any order once they are created. They interleave their execution with each other and the main thread (or exencute in paralle if multiple cores are available). However a call to pthread_join() ensures that the calling thread will not proceed until the thread being joined with completes.It is in this way that the main thread “knows” when each thread has completed computing its partial sum and successfully returned it.You might wonder “What happens if a sum thread completes before the main thread calls pthread_join()?” In fact, that occurs in this sample execution. Thread 1 returns before the main thread calls pthread_join() on thread 1. The semantics of pthread_join() are that it will immediately unblock if the thread being joined with has already exited. Thus pthread_join()  blocks is the thread being joined with hasn’t yet exited  unblocks immediately if the thread being joined with has already exitedEither way, the thread calling pthread_join() is guaranteed that the thread it is joining with has exited and, thus, any work that thread was doing must have been completed.SynchronizationThe functionality of pthread_join() illustrates an important operating systems concept: synchronization.The term synchronization literally means “at the same time.” However, in a computer science context it means\"the state of a concurrent program is consistent across two or more concurrent events.\"In this example, the main thread “synchronizes” with each of the sum threads using the pthread_join() call. After each call to pthread_join() you, the programmer, know the state of two threads:  the main thread, which has received the partial sum from the thread whose id is contained in the variable thread_ids[t]  the sum thread whose id is contained in the variable thread_ids[t] and this state is that the thread has exited after sucessfully computing its partial sum.Thus the main thread and one of the sum threads “synchronize” before the main thread tries to use the partial sum computed by the sum thread.Synchronization is an important concept when concurrent and/or asynchronous events must be managed. We’ll discuss syncronization a great deal through this class as it is a critical function of an operating system.How much faster is it?You can experiment with these last two example codes to see how much of an improvement threading and parallelism make in terms of the performance of the code. On my laptop. Running these codes with the Linux time command:time ./avg-1thread 100000000main thread forking sum threadmain thread running after sum thread created, about to call joinsum thread runningsum thread done, returningmain thread joined with sum threadthe average over 100000000 random numbers on (0,1) is 0.500023real    0m1.620suser    0m1.419ssys 0m0.195sandtime ./avg-manythread 100000000 10main thread about to create 10 sum threadsmain thread creating sum thread 1main thread has created sum thread 1main thread creating sum thread 2sum thread 1 running, starting at 0 for 10000000main thread has created sum thread 2sum thread 2 running, starting at 10000000 for 10000000main thread creating sum thread 3main thread has created sum thread 3main thread creating sum thread 4main thread has created sum thread 4main thread creating sum thread 5main thread has created sum thread 5main thread creating sum thread 6sum thread 4 running, starting at 30000000 for 10000000sum thread 3 running, starting at 20000000 for 10000000main thread has created sum thread 6sum thread 5 running, starting at 40000000 for 10000000sum thread 6 running, starting at 50000000 for 10000000main thread creating sum thread 7main thread has created sum thread 7sum thread 7 running, starting at 60000000 for 10000000main thread creating sum thread 8main thread has created sum thread 8main thread creating sum thread 9main thread has created sum thread 9main thread creating sum thread 10main thread has created sum thread 10main thread about to join with sum thread 1sum thread 8 running, starting at 70000000 for 10000000sum thread 9 running, starting at 80000000 for 10000000sum thread 10 running, starting at 90000000 for 10000000sum thread 1 returningmain thread joined with sum thread 1main thread about to join with sum thread 2sum thread 5 returningsum thread 3 returningsum thread 4 returningsum thread 6 returningsum thread 8 returningsum thread 7 returningsum thread 2 returningmain thread joined with sum thread 2main thread about to join with sum thread 3main thread joined with sum thread 3main thread about to join with sum thread 4main thread joined with sum thread 4main thread about to join with sum thread 5main thread joined with sum thread 5main thread about to join with sum thread 6main thread joined with sum thread 6main thread about to join with sum thread 7main thread joined with sum thread 7main thread about to join with sum thread 8main thread joined with sum thread 8main thread about to join with sum thread 9sum thread 9 returningmain thread joined with sum thread 9main thread about to join with sum thread 10sum thread 10 returningmain thread joined with sum thread 10the average over 100000000 random numbers on (0,1) is 0.500023real    0m1.437suser    0m1.501ssys 0m0.207sThat’s right – using 10 threads only speeds it up with 0.2 seconds. Can you figure out why?Experimenting with these examplesMany of the lectures in this class (like this one) include coding examples that are intended to illustrate some of the concepts that the lecture hopes to get across. In this case, the concepts are  threads as a programming abstraction for dealing with concurrency and parallelism,  basic thread creation and synchronization in pthreads  concurrency in the thread runtimeIf you don’t understand these three concepts yet, that’s okay – there are a couple of ways to proceed.First, you should go back an reread the lecture notes from beginning to end. The notes that I provide are intended to be read sequentially and not skimmed when you are trying to learn the concepts. This type of writing differs from other forms with which you might be familiar. It is, however, important to understand that it is by design. Each section of the notes leads to the next thus simply dropping into the middle or (more probably) looking at the bullet points and the examples won’t likely yield a satisfactory explanation.Secondly, the code exmaples are intended to serve as a vehicle for your own personal experimentation. That is, you can build, modify, and run these codes as a way of fiamilarizing yourself with some of the details that you might not have understood from the lecture notes.For example, you might suspect that the reason the speed up using 10 threads over 1 thread in the previous examples is due to the printf() statements. Both of the last two codes include print statements to show how the threads interleave their execution (the third concept the lecture covers). You might suspect that the small difference in time is because the programs spend time printing messages and the message printing is much slower than the computation time.To test this theory, you can make copies of these programs, comment out or remove the print statements, and rerun the timing experiments. Does it make the program faster? By how much?Building the examplesIn the notes, I include code fragments that won’t necessarily execute properly if you cut and paste them directly from the test. That’s because the working code is often too long to display properly in a classroom lecture format. Instead, I’ve provided commented, working versions of the code athttp://www.cs.ucsb.edu/~rich/class/cs170/notes/IntroThreads/exampleIn this directory you will find  one or more C program files  possibly one or more C head header files  a makefile  a README.md fileYou can copy the files from this location.I’ve also made the examples available on github. If you haven’t used github before, it is a public code repository that promotes code sharing using the git source code control system. Git has many interesting features that are designed to allow distributed sets of developers to collaborate. One such feature allows you to “clone” a repository so that you can work with it on your own. To get a copy fo the examples from this class, log into a CSIL machine and typegit clone https://github.com/richwolski/cs170-lecture-examples.gitThis command will create a subdirectory called cs170-lecture-examples. In it you’ll see several subdirectories. For this lecture, the code is in the “IntroThreads” subdirectory.To build the programs typecd cs170-lecture-examples/IntroThreadsmakeIf all has gone well, you’ll build the three programs this lecture discusses. You should look at the code in these progras as well. In the lecture notes, I’ve removed many of the error checks that good C and Linux programs should have.You can make modifications to these programs. To rebuild them simply run the “make” command again and it will invoke the C compiler for you. The use of the make command is trivial for these specific examples (you can run gcc manually with little trouble). When we get to some of the assignments, however, it will be important to use make since the build environment is substantially more complex.",
        "url"      : "/2017/06/08/Introduction-to-Threads/",
        "date"     : "2017-06-08 12:06:00 +0000"
    } ,
    
    {
        "title"    : "First post",
        "category" : "Random",
        "tags"     : "",
        "content": "First post  haha  nima",
        "url"      : "/2017/06/08/first-post/",
        "date"     : "2017-06-08 10:45:00 +0000"
    } 
    
]
